{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "681fee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Importing all the relevant library\n",
    "%matplotlib inline\n",
    "import h5py, os\n",
    "#from functions import transforms as T\n",
    "#from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import glob\n",
    "#from functions import transforms as T \n",
    "#from functions.subsample import MaskFunc\n",
    "from PIL import Image\n",
    "import random\n",
    "from numpy.fft import fftshift, ifftshift, fftn, ifftn\n",
    "import cmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c5f4d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bbd1420",
   "metadata": {},
   "source": [
    "AWS has torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57966d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class MaskFunc:\n",
    "    \"\"\"\n",
    "    MaskFunc creates a sub-sampling mask of a given shape.\n",
    "\n",
    "    The mask selects a subset of columns from the input k-space data. If the k-space data has N\n",
    "    columns, the mask picks out:\n",
    "        1. N_low_freqs = (N * center_fraction) columns in the center corresponding to\n",
    "           low-frequencies\n",
    "        2. The other columns are selected uniformly at random with a probability equal to:\n",
    "           prob = (N / acceleration - N_low_freqs) / (N - N_low_freqs).\n",
    "    This ensures that the expected number of columns selected is equal to (N / acceleration)\n",
    "\n",
    "    It is possible to use multiple center_fractions and accelerations, in which case one possible\n",
    "    (center_fraction, acceleration) is chosen uniformly at random each time the MaskFunc object is\n",
    "    called.\n",
    "\n",
    "    For example, if accelerations = [4, 8] and center_fractions = [0.08, 0.04], then there\n",
    "    is a 50% probability that 4-fold acceleration with 8% center fraction is selected and a 50%\n",
    "    probability that 8-fold acceleration with 4% center fraction is selected.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, center_fractions, accelerations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            center_fractions (List[float]): Fraction of low-frequency columns to be retained.\n",
    "                If multiple values are provided, then one of these numbers is chosen uniformly\n",
    "                each time.\n",
    "\n",
    "            accelerations (List[int]): Amount of under-sampling. This should have the same length\n",
    "                as center_fractions. If multiple values are provided, then one of these is chosen\n",
    "                uniformly each time. An acceleration of 4 retains 25% of the columns, but they may\n",
    "                not be spaced evenly.\n",
    "        \"\"\"\n",
    "        if len(center_fractions) != len(accelerations):\n",
    "            raise ValueError('Number of center fractions should match number of accelerations')\n",
    "\n",
    "        self.center_fractions = center_fractions\n",
    "        self.accelerations = accelerations\n",
    "        self.rng = np.random.RandomState()\n",
    "\n",
    "    def __call__(self, shape, seed=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            shape (iterable[int]): The shape of the mask to be created. The shape should have\n",
    "                at least 3 dimensions. Samples are drawn along the second last dimension.\n",
    "            seed (int, optional): Seed for the random number generator. Setting the seed\n",
    "                ensures the same mask is generated each time for the same shape.\n",
    "        Returns:\n",
    "            torch.Tensor: A mask of the specified shape.\n",
    "        \"\"\"\n",
    "        if len(shape) < 3:\n",
    "            raise ValueError('Shape should have 3 or more dimensions')\n",
    "\n",
    "        self.rng.seed(seed)\n",
    "        num_cols = shape[-2]\n",
    "\n",
    "        choice = self.rng.randint(0, len(self.accelerations))\n",
    "        center_fraction = self.center_fractions[choice]\n",
    "        acceleration = self.accelerations[choice]\n",
    "\n",
    "        # Create the mask\n",
    "        num_low_freqs = int(round(num_cols * center_fraction))\n",
    "        prob = (num_cols / acceleration - num_low_freqs) / (num_cols - num_low_freqs)\n",
    "        mask = self.rng.uniform(size=num_cols) < prob\n",
    "        pad = (num_cols - num_low_freqs + 1) // 2\n",
    "        mask[pad:pad + num_low_freqs] = True\n",
    "\n",
    "        # Reshape the mask\n",
    "        mask_shape = [1 for _ in shape]\n",
    "        mask_shape[-2] = num_cols\n",
    "        mask = torch.from_numpy(mask.reshape(*mask_shape).astype(np.float32))\n",
    "\n",
    "        return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0dce273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.fft import fftshift, ifftshift, fftn, ifftn\n",
    "\n",
    "def transform_kspace_to_image(k, dim=None, img_shape=None):\n",
    "    \"\"\" Computes the Fourier transform from k-space to image space\n",
    "    along a given or all dimensions\n",
    "    :param k: k-space data\n",
    "    :param dim: vector of dimensions to transform\n",
    "    :param img_shape: desired shape of output image\n",
    "    :returns: data in image space (along transformed dimensions)\n",
    "    \"\"\"\n",
    "    if not dim:\n",
    "        dim = range(k.ndim)\n",
    "\n",
    "    img = fftshift(ifftn(ifftshift(k, axes=dim), s=img_shape, axes=dim), axes=dim)\n",
    "    #img = fftshift(ifft2(ifftshift(k, dim=dim)), dim=dim)\n",
    "    img *= np.sqrt(np.prod(np.take(img.shape, dim)))\n",
    "    return img\n",
    "\n",
    "\n",
    "def transform_image_to_kspace(img, dim=None, k_shape=None):\n",
    "    \"\"\" Computes the Fourier transform from image space to k-space space\n",
    "    along a given or all dimensions\n",
    "    :param img: image space data\n",
    "    :param dim: vector of dimensions to transform\n",
    "    :param k_shape: desired shape of output k-space data\n",
    "    :returns: data in k-space (along transformed dimensions)\n",
    "    \"\"\"\n",
    "    if not dim:\n",
    "        dim = range(img.ndim)\n",
    "\n",
    "    k = fftshift(fftn(ifftshift(img, axes=dim), s=k_shape, axes=dim), axes=dim)\n",
    "    #k = fftshift(fft2(ifftshift(img, dim=dim)), dim=dim)\n",
    "    k /= np.sqrt(np.prod(np.take(img.shape, dim)))\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1579782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def add_noise(array: np.ndarray, dropout_rate: float = 0.10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function adds noise\n",
    "    :param array:\n",
    "    :param dropout_rate: percent of pixels to be dropped\n",
    "    :return:\n",
    "    \"\"\"\n",
    "   # assert len(array.shape) == 4\n",
    "   # assert array.shape[1] == 3\n",
    "\n",
    "    channels = array.shape[1]\n",
    "    height = array.shape[2]\n",
    "    width = array.shape[3]\n",
    "\n",
    "    total_pixels = height * width\n",
    "    queued_pixels = int(total_pixels * dropout_rate)\n",
    "\n",
    "    filled_pixels = 0\n",
    "    while filled_pixels < queued_pixels:\n",
    "        d_h = random.randint(1, 3)\n",
    "        d_w = random.randint(1, 3)\n",
    "        filled_pixels += d_h * d_w\n",
    "        h = random.randint(0, height - d_h)\n",
    "        w = random.randint(0, width - d_w)\n",
    "\n",
    "        # now overwrite selected pixels with random dark color\n",
    "        array[:, :, h:h+d_h, w:w+d_w] = random.randint(1, 25) / 255.0\n",
    "\n",
    "    return array\n",
    "\n",
    "\n",
    "def array_to_image(array: np.ndarray):\n",
    "    \"\"\"\n",
    "    This function converts NumPy array to image\n",
    "    :param array:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(array.shape) == 4\n",
    "    assert array.shape[1] == 1 or array.shape[1] == 3\n",
    "\n",
    "    scaled = (array * 255.0).astype(np.uint8)\n",
    "    if array.shape[1] == 1:\n",
    "        reshaped = scaled.reshape((array.shape[2], array.shape[3]))\n",
    "        return Image.fromarray(reshaped,\"L\")\n",
    "    else:\n",
    "        reshaped = scaled.reshape((3, array.shape[2], array.shape[3])).transpose([1, 2, 0])\n",
    "        return Image.fromarray(reshaped,\"RGB\")\n",
    "\n",
    "\n",
    "def image_to_array(image) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function converts image to NumPy array\n",
    "    :param image:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    image1 = np.array(image)\n",
    "    return np.expand_dims(np.asarray(image1), 0).astype(np.float) / 255.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac2c258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "327d8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "\n",
    "        return get_epoch_batch(subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8209b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from numpy.fft import fftshift, ifftshift, fftn, ifftn\n",
    "import cmath\n",
    "def noise_and_kspace(image):\n",
    "    #change to k-space\n",
    "    img_fft = fftshift(fftn(image))\n",
    "    size_img = img_fft.shape\n",
    "     #np.random.uniform, np.random.normal\n",
    "    std = np.random.normal(0.000, 0.005) * np.amax(img_fft)\n",
    "    noise = fftshift(std * np.random.standard_normal(size_img) + std * 1j * np.random.standard_normal(size_img));     #This generates a complex noise signal.\n",
    "    img_fft_noise = img_fft + noise # k-space\n",
    "    img_noise = ifftn(ifftshift(img_fft_noise))# revert k-space back to noise\n",
    "    return img_fft, img_fft_noise, img_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "56178edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_name = subject_id  \n",
    "    \n",
    "#    with h5py.File(rawdata_name, 'r') as data:\n",
    "#        rawdata = data['kspace'][slice]\n",
    "   \n",
    "    im_frame = Image.open(rawdata_name)\n",
    "    im_fft, im_fft_noise, noise_im_frame = noise_and_kspace(im_frame)\n",
    "\n",
    "    ############################\n",
    "    #img_und = to_tensor(np.array(noise_im_frame)).unsqueeze(0) # noise image tensor form    \n",
    "    preprocess = T.Compose([\n",
    "                       # T.Grayscale(num_output_channels=1),\n",
    "                           T.Resize(64),    #128 as maximum #64\n",
    "                           T.CenterCrop(64),\n",
    "                           T.ToTensor() #,\n",
    "                            ])\n",
    "    img_gt = preprocess(Image.fromarray(np.uint8(im_fft)).convert('L'))\n",
    "    img_und = preprocess(Image.fromarray(np.uint8(im_fft_noise)).convert('L'))\n",
    "    \n",
    "    n1 = (img_und**2).sum(dim=-1).sqrt()\n",
    "    norm = n1.max() \n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "    img_gt, img_und = img_gt/norm , img_und/norm\n",
    "\n",
    "    return img_gt.squeeze(0), img_und.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "575b71b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path, val_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)):\n",
    "\n",
    "        data_list[train_and_val[i]] = []\n",
    "        \n",
    "        which_data_path = data_path[i]\n",
    "        tr = 0\n",
    "        te = 0\n",
    "        alfa = 0\n",
    "    \n",
    "        for fname in sorted(os.listdir(which_data_path + '/images')):\n",
    "            if fname == '.DS_Store': continue\n",
    "            \n",
    "            subject_data_path = os.path.join(which_data_path + '/images', fname)\n",
    "                     \n",
    "            if not os.path.isfile(subject_data_path): continue \n",
    "            \n",
    "     \n",
    "            #get information from text file\n",
    "            # this will return a tuple of root and extension\n",
    "            split_tup = os.path.splitext(fname)\n",
    "\n",
    "  \n",
    "            # extract the file name and extension\n",
    "            file_name = split_tup[0]\n",
    "  \n",
    "  \n",
    "            data_list[train_and_val[i]].append((fname, subject_data_path))\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0eeec3b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "img = imread('IMage Path'); %This is where I read the image matrix\n",
    "img_fft = fftshift(fft2(img));  %change the image to K-Space\n",
    "size_img = size(img_fft); Extract the size matrix for image to create a noise for the same size.\n",
    "\n",
    "%standard deviation decided for the noise. I am using a uniform distribution but you can choose what works best for you.\n",
    "%This noise is with 0 mean and std standard deviation\n",
    "\n",
    "std  = unifrnd(0.000,0.001)*max(img_fft(:,:),[],'all');\n",
    "\n",
    "noise = fftshift(std.*randn(size(img_fft)) + std1.*1i*randn(size(img_fft)));     %This generates a complex noise signal.\n",
    "\n",
    "img_ftt_noise = img_fft + noise;  %Here we add noise to the original image K-Space\n",
    "\n",
    "img_new = ifft2(ifftshift(img_fft));  %This gives you the noise back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0961b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb098708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74c35f60",
   "metadata": {},
   "source": [
    "# The initial model, based on the AlexNet architecture"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfb03fcb-f57e-4c4c-b858-26bfa87a57cf",
   "metadata": {},
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1), #320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64 ,64, kernel_size=3, padding=1),  # 320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # 320/320\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, padding=1),  # 320/320\n",
    "            \n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b87ba",
   "metadata": {},
   "source": [
    "# RestNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957014f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "560bd999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseBlock(torch.nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self,input_planes,planes,stride=1,dim_change=None):\n",
    "        super(baseBlock,self).__init__()\n",
    "        #declare convolutional layers with batch norms\n",
    "        self.conv1 = torch.nn.Conv2d(input_planes,planes,stride=stride,kernel_size=3,padding=1)\n",
    "        self.bn1   = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv2 = torch.nn.Conv2d(planes,planes,stride=1,kernel_size=3,padding=1)\n",
    "        self.bn2   = torch.nn.BatchNorm2d(planes)\n",
    "        self.dim_change = dim_change\n",
    "    def forward(self,x):\n",
    "        #Save the residue\n",
    "        res = x\n",
    "        output = F.relu(self.bn1(self.conv1(x)))\n",
    "        output = self.bn2(self.conv2(output))\n",
    "        if self.dim_change is not None:\n",
    "            res = self.dim_change(res)\n",
    "        \n",
    "        output += res\n",
    "        output = F.relu(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self,block,num_layers,classes=10):\n",
    "        super(ResNet,self).__init__()\n",
    "        #according to research paper:\n",
    "        self.input_planes = 64 #256\n",
    "        self.conv1 = torch.nn.Conv2d(1,64,kernel_size=3,stride=1,padding=1)\n",
    "        self.layer1 = self._layer(block,64,num_layers[0],stride=1)\n",
    "        self.layer2 = self._layer(block,64,num_layers[1],stride=1)\n",
    "        self.layer3 = self._layer(block,64,num_layers[2],stride=1)\n",
    "        self.layer4 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer5 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer6 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer7 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer8 = self._layer(block,64,num_layers[2],stride=1)\n",
    "        self.layer9 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer10 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer11 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer12 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer13 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer14 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer15 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer16 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer17 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer18 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.conv2 = torch.nn.Conv2d(64,1,kernel_size=3,stride=1, padding=1)\n",
    "        \n",
    "    \n",
    "    def _layer(self,block,planes,num_layers,stride=1):\n",
    "        dim_change = None\n",
    "        if stride!=1 or planes != self.input_planes*block.expansion:\n",
    "            dim_change = torch.nn.Sequential(torch.nn.Conv2d(self.input_planes,planes*block.expansion,kernel_size=1,stride=stride),\n",
    "                                             torch.nn.BatchNorm2d(planes*block.expansion))\n",
    "        netLayers =[]\n",
    "        netLayers.append(block(self.input_planes,planes,stride=stride,dim_change=dim_change))\n",
    "        self.input_planes = planes * block.expansion\n",
    "        for i in range(1,num_layers):\n",
    "            netLayers.append(block(self.input_planes,planes))\n",
    "            self.input_planes = planes * block.expansion\n",
    "        \n",
    "        return torch.nn.Sequential(*netLayers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.layer8(x)\n",
    "        x = self.layer9(x)\n",
    "        x = self.layer10(x)\n",
    "        x = self.layer11(x)\n",
    "        x = self.layer12(x)\n",
    "        x = self.layer13(x)\n",
    "        x = self.layer14(x)\n",
    "        x = self.layer15(x)\n",
    "        x = self.layer16(x)\n",
    "        x = self.layer17(x)\n",
    "        x = self.layer18(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8195fdba",
   "metadata": {},
   "source": [
    "Changed in version 0.16: This function was renamed from skimage.measure.compare_ssim to skimage.metrics.structural_similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6b06b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as cmp_ssim \n",
    "from skimage.metrics import mean_squared_error\n",
    "from skimage.metrics import normalized_root_mse\n",
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return cmp_ssim(\n",
    "         gt, pred, multichannel=False, data_range=gt.max()\n",
    "    )\n",
    "#def ssim(gt, pred):\n",
    "#    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "#    return cmp_ssim(\n",
    " #       gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    " #   )\n",
    "def mse(gt, pred):\n",
    "    \"\"\" Compute mean squared error. \"\"\"\n",
    "    return mean_squared_error(gt, pred)\n",
    "\n",
    "def nrmse(gt, pred):\n",
    "    \"\"\" Compute normalized root mse. \"\"\"\n",
    "    return normalized_root_mse(gt, pred)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "755c3b23",
   "metadata": {},
   "source": [
    "All the code from here on is almost the same as before, only for the 8fold data. The main difference is the learning rate, which is 3 times higher for the 8fold data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44d5dbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish data loading- now train\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "data_path_train = 'data/train'\n",
    "data_path_val = 'data/val'\n",
    "data_list = load_data_path(data_path_train, data_path_val)\n",
    "    \n",
    "\n",
    "num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "#mae_loss = nn.L1Loss().to('cuda:0')\n",
    "mae_loss = nn.L1Loss()\n",
    "lr = 3e-3\n",
    "    #acc =8 , network_8fold\n",
    "network_8fold = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "#network_8fold.to('cuda:0') #move the model on the GPU\n",
    "\n",
    "    \n",
    "optimizer2 = optim.Adam(network_8fold.parameters(), lr=lr)\n",
    "\n",
    " \n",
    "train_dataset = MRIDataset(data_list['train'])\n",
    "val_dataset = MRIDataset(data_list['val'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=64, num_workers=num_workers) \n",
    "print(\"finish data loading- now train\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "061c8b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (layer1): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer6): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer7): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer8): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer9): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer10): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer11): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer12): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer13): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer14): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer15): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer16): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer17): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer18): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (conv2): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_8fold"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69044fe6",
   "metadata": {},
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "then = time.time() #Time before the operations start\n",
    "\n",
    "#DO YOUR OPERATIONS HERE\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    for iteration, sample in enumerate(train_loader):\n",
    "        img_nr += 1\n",
    "        img_gt, img_und = sample\n",
    "        #print(img_und.shape)\n",
    "\n",
    "now = time.time() #Time after it finished\n",
    "\n",
    "print(\"It took: \", now-then, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8672c18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/multiprocessing/queues.py\", line 242, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yr/qwqgmggj2_12p38pnv1hf6380000gp/T/ipykernel_955/3691090664.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0moptimizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m#set current gradients to 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#backpropagate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0moptimizer2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m     \u001b[0;31m#update the weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mmean_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "then = time.time() #Time before the operations start\n",
    "losses2=[]\n",
    "mean_loss_list = []\n",
    "img_nr = 0\n",
    "epoch_nums = 100 #64 # 5\n",
    "for epoch in range(epoch_nums):\n",
    "    for iteration, sample in enumerate(train_loader):\n",
    "        img_nr += 1\n",
    "        img_gt, img_und = sample\n",
    "        \n",
    "        img_gt = img_gt.unsqueeze(1)#.to('cuda:0') # img_gt = img_gt.unsqueeze(1).to('cuda:0')\n",
    "        img_und = img_und.unsqueeze(1)#.to('cuda:0') #img_und = img_und.unsqueeze(1).to('cuda:0')\n",
    "       # img_gt = img_gt.to('cuda:0')\n",
    "       # img_und = img_und.to('cuda:0')\n",
    "            \n",
    "        output = network_8fold(img_und)      #feedforward\n",
    "        #print(output.shape) #// debug\n",
    "        loss = mae_loss(output, img_gt)\n",
    "\n",
    "        optimizer2.zero_grad()       #set current gradients to 0\n",
    "        loss.backward()      #backpropagate\n",
    "        optimizer2.step()     #update the weights\n",
    "        mean_loss_list.append(loss.item())\n",
    "       # print(\"Loss value: \", loss.item())\n",
    "            #compute and print the mean L1 lossscore for the last 20 training images.\n",
    "        if img_nr%20 == 0:\n",
    "            print(\"L1 Loss score: \", np.round(np.mean(mean_loss_list), decimals = 5), \"  Image number: \", img_nr, \"  Epoch: \", epoch+1)\n",
    "            mean_loss_list = []\n",
    "        losses2.append(loss.item() * img_gt.size(0))\n",
    "        \n",
    "now = time.time() #Time after it finished\n",
    "\n",
    "print(\"It took: \", now-then, \" seconds\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f0d027e",
   "metadata": {},
   "source": [
    "From the Pytorch documentation on convolutional layers, Conv2d layers expect input with the shape\n",
    "\n",
    "(n_samples, channels, height, width) # e.g., (1000, 1, 224, 224)\n",
    "Passing grayscale images in their usual format (224, 224) won't work.\n",
    "\n",
    "To get the right shape, you will need to add a channel dimension. You can do it as follows:\n",
    "\n",
    "x = np.expand_dims(x, 1)      # if numpy array\n",
    "tensor = tensor.unsqueeze(1)  # if torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da32f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f59efc7b-fde5-47a8-a566-f5ca13563301",
   "metadata": {},
   "source": [
    "np.moveaxis(arr, 1, -3)\n",
    "https://github.com/scikit-image/scikit-image/issues/4636\n",
    "https://stackoverflow.com/questions/68079012/valueerror-win-size-exceeds-image-extent-if-the-input-is-a-multichannel-color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7795a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the SSIM score for every image after a feedforward propagation through \n",
    "#the network.\n",
    "#Subtract the image SSIM score before the feedforward prop to obtain the net improvement for every image.\n",
    "#Print the average improvement and the average SSIM score after the reconstruction.\n",
    "SSIM_improvement = []\n",
    "SSIM_score = []\n",
    "MSE_improvement = []\n",
    "MSE_score = []\n",
    "NRMSE_improvement = []\n",
    "NRMSE_score = []\n",
    "MIE_improvement = []\n",
    "MIE_score = []\n",
    "for i in range(0,len(val_dataset)):\n",
    "    gt, image = val_dataset[i]\n",
    "    #image = image.unsqueeze(0).to('cuda:0')\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.unsqueeze(0)\n",
    "    gt = gt.unsqueeze(0).numpy()\n",
    "    output = network_8fold(image)\n",
    "  #  output = output.squeeze(1).cpu().detach().numpy()\n",
    "    output = output.squeeze(1).detach().numpy()\n",
    "    image = image.squeeze(1).numpy()\n",
    "    gt =  np.squeeze(gt)\n",
    "    output =  np.squeeze(output)\n",
    "    image =  np.squeeze(image)\n",
    "\n",
    "\n",
    "    output_loss1 = torch.tensor(ssim(gt, output))\n",
    "    output_loss2 = torch.tensor(mse(gt, output))\n",
    "    output_loss3 = torch.tensor(nrmse(gt, output))\n",
    "    output_loss4 = np.mean(np.abs(gt - output))\n",
    "  #  image_loss = torch.tensor(ssim(gt, image.squeeze(1).cpu().numpy()))\n",
    "    image_loss1 = torch.tensor(ssim(gt, image))\n",
    "    image_loss2 = torch.tensor(mse(gt, image))\n",
    "    image_loss3 = torch.tensor(nrmse(gt, image))\n",
    "    image_loss4 = np.mean(np.abs(gt - image))\n",
    "    SSIM_improvement.append(output_loss1.item()-image_loss1.item())\n",
    "    SSIM_score.append(output_loss1.item())\n",
    "    MSE_improvement.append(output_loss2.item()-image_loss2.item())\n",
    "    MSE_score.append(output_loss2.item())\n",
    "    NRMSE_improvement.append(output_loss3.item()-image_loss3.item())\n",
    "    NRMSE_score.append(output_loss3.item())\n",
    "    MIE_improvement.append(output_loss4 -image_loss4)\n",
    "    MIE_score.append(output_loss4)\n",
    "\n",
    "print(np.nanmean(SSIM_improvement))\n",
    "print(np.nanmean(SSIM_score))\n",
    "print(np.nanmean(MSE_improvement))\n",
    "print(np.nanmean(MSE_score))\n",
    "print(np.nanmean(NRMSE_improvement))\n",
    "print(np.nanmean(NRMSE_score))\n",
    "print(np.nanmean(MIE_improvement))\n",
    "print(np.nanmean(MIE_score))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ba92d80-ff2a-4cc7-a0d1-ba55184f0938",
   "metadata": {},
   "source": [
    "#compute the SSIM score for every image after a feedforward propagation through \n",
    "#the network.\n",
    "#Subtract the image SSIM score before the feedforward prop to obtain the net improvement for every image.\n",
    "#Print the average improvement and the average SSIM score after the reconstruction.\n",
    "SSIM_improvement = []\n",
    "SSIM_score = []\n",
    "MSE_improvement = []\n",
    "MSE_score = []\n",
    "NRMSE_improvement = []\n",
    "NRMSE_score = []\n",
    "for i in range(0,len(val_dataset)):\n",
    "    gt, image = val_dataset[i]\n",
    "    #image = image.unsqueeze(0).to('cuda:0')\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.unsqueeze(0)\n",
    "    gt = gt.unsqueeze(0).numpy()\n",
    "    output = network_8fold(image)\n",
    "  #  output = output.squeeze(1).cpu().detach().numpy()\n",
    "    output = output.squeeze(1).detach().numpy()\n",
    "    image = image.squeeze(1).numpy()\n",
    "    gt =  np.squeeze(gt)\n",
    "    output =  np.squeeze(output)\n",
    "    image =  np.squeeze(image)\n",
    "\n",
    "\n",
    "    output_loss1 = torch.tensor(ssim(gt, output))\n",
    "    output_loss2 = torch.tensor(mse(gt, output))\n",
    "    output_loss3 = torch.tensor(nrmse(gt, output))\n",
    "  #  image_loss = torch.tensor(ssim(gt, image.squeeze(1).cpu().numpy()))\n",
    "    image_loss1 = torch.tensor(ssim(gt, image))\n",
    "    image_loss2 = torch.tensor(mse(gt, image))\n",
    "    image_loss3 = torch.tensor(nrmse(gt, image))\n",
    "    SSIM_improvement.append(output_loss1.item()-image_loss1.item())\n",
    "    SSIM_score.append(output_loss1.item())\n",
    "    MSE_improvement.append(output_loss2.item()-image_loss2.item())\n",
    "    MSE_score.append(output_loss2.item())\n",
    "    NRMSE_improvement.append(output_loss3.item()-image_loss3.item())\n",
    "    NRMSE_score.append(output_loss3.item())\n",
    "\n",
    "print(np.nanmean(SSIM_improvement))\n",
    "print(np.nanmean(SSIM_score))\n",
    "print(np.nanmean(MSE_improvement))\n",
    "print(np.nanmean(MSE_score))\n",
    "print(np.nanmean(NRMSE_improvement))\n",
    "print(np.nanmean(NRMSE_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_improvement.sort()\n",
    "plt.plot(SSIM_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dbcb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_improvement.sort()\n",
    "plt.plot(MSE_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NRMSE_improvement.sort()\n",
    "plt.plot(NRMSE_improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23428529",
   "metadata": {},
   "source": [
    "## save Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb1c0afb",
   "metadata": {},
   "source": [
    "https://learn-pytorch.oneoffcoder.com/model-persistence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afffbf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_dir = f\"s3://savemodels/network_8fold/restnet-model{index}.pt\"\n",
    "output_dir = f\"./network_8fold/restnet-model{index}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fcce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model to S3 bucket or data\n",
    "torch.save(network_8fold.state_dict(), output_dir)\n",
    "#torch.save(network_8fold.state_dict(), './models/resnet18-model.pt')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de009c4f",
   "metadata": {},
   "source": [
    "#save the whole model\n",
    "torch.save(network_8fold, output_dir)\n",
    "#torch.save(network_8fold, './models/resnet18-model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc56219",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Model from saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9718f388-6792-42eb-9ede-ad730317cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b955f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_dir = f\"s3://savemodels/network_8fold/restnet-model{index}.pt\"\n",
    "output_dir = f\"./network_8fold/restnet-model{index}.pt\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9d7ffc6-c8a7-430e-a4ad-313287b60833",
   "metadata": {},
   "source": [
    "#load model on GPU\n",
    "device = torch.device(\"cuda\")\n",
    "model = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "\n",
    "#model = model.to('cuda:0')\n",
    "model.load_state_dict(torch.load(output_dir, map_location='cuda:0'))\n",
    "#model.load_state_dict(torch.load('./models/resnet18-model.pt', map_location='cuda:0'))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5448b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67996389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model on CPU: laptop\n",
    "device = torch.device('cpu')\n",
    "#model = TheModelClass(*args, **kwargs)\n",
    "model = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "#model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "model.load_state_dict(torch.load(output_dir, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0fb806c",
   "metadata": {},
   "source": [
    "model = torch.load(PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c0fb0",
   "metadata": {},
   "source": [
    "## Predict a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74ad0dc8-645b-4c17-b2b1-4029244808a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aa3f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_dir = f\"s3://savemodels/network_8fold/restnet-model{index}.pt\"\n",
    "model_dir = f\"./network_8fold/restnet-model{index}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba5162f4-b5e3-4af1-99ff-e5747ab90220",
   "metadata": {},
   "outputs": [],
   "source": [
    "#simply copy from previous cell, using for testing the prediction\n",
    "class baseBlock(torch.nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self,input_planes,planes,stride=1,dim_change=None):\n",
    "        super(baseBlock,self).__init__()\n",
    "        #declare convolutional layers with batch norms\n",
    "        self.conv1 = torch.nn.Conv2d(input_planes,planes,stride=stride,kernel_size=3,padding=1)\n",
    "        self.bn1   = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv2 = torch.nn.Conv2d(planes,planes,stride=1,kernel_size=3,padding=1)\n",
    "        self.bn2   = torch.nn.BatchNorm2d(planes)\n",
    "        self.dim_change = dim_change\n",
    "    def forward(self,x):\n",
    "        #Save the residue\n",
    "        res = x\n",
    "        output = F.relu(self.bn1(self.conv1(x)))\n",
    "        output = self.bn2(self.conv2(output))\n",
    "        if self.dim_change is not None:\n",
    "            res = self.dim_change(res)\n",
    "        \n",
    "        output += res\n",
    "        output = F.relu(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self,block,num_layers,classes=10):\n",
    "        super(ResNet,self).__init__()\n",
    "        #according to research paper:\n",
    "        self.input_planes = 64 #256\n",
    "        self.conv1 = torch.nn.Conv2d(1,64,kernel_size=3,stride=1,padding=1)\n",
    "        self.layer1 = self._layer(block,64,num_layers[0],stride=1)\n",
    "        self.layer2 = self._layer(block,64,num_layers[1],stride=1)\n",
    "        self.layer3 = self._layer(block,64,num_layers[2],stride=1)\n",
    "        self.layer4 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer5 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer6 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer7 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer8 = self._layer(block,64,num_layers[2],stride=1)\n",
    "        self.layer9 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer10 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer11 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer12 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer13 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer14 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer15 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer16 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer17 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer18 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.conv2 = torch.nn.Conv2d(64,1,kernel_size=3,stride=1, padding=1)\n",
    "        \n",
    "    \n",
    "    def _layer(self,block,planes,num_layers,stride=1):\n",
    "        dim_change = None\n",
    "        if stride!=1 or planes != self.input_planes*block.expansion:\n",
    "            dim_change = torch.nn.Sequential(torch.nn.Conv2d(self.input_planes,planes*block.expansion,kernel_size=1,stride=stride),\n",
    "                                             torch.nn.BatchNorm2d(planes*block.expansion))\n",
    "        netLayers =[]\n",
    "        netLayers.append(block(self.input_planes,planes,stride=stride,dim_change=dim_change))\n",
    "        self.input_planes = planes * block.expansion\n",
    "        for i in range(1,num_layers):\n",
    "            netLayers.append(block(self.input_planes,planes))\n",
    "            self.input_planes = planes * block.expansion\n",
    "        \n",
    "        return torch.nn.Sequential(*netLayers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.layer8(x)\n",
    "        x = self.layer9(x)\n",
    "        x = self.layer10(x)\n",
    "        x = self.layer11(x)\n",
    "        x = self.layer12(x)\n",
    "        x = self.layer13(x)\n",
    "        x = self.layer14(x)\n",
    "        x = self.layer15(x)\n",
    "        x = self.layer16(x)\n",
    "        x = self.layer17(x)\n",
    "        x = self.layer18(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad022822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (layer1): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer6): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer7): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer8): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer9): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer10): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer11): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer12): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer13): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer14): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer15): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer16): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer17): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer18): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (conv2): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load model on CPU: laptop\n",
    "device = torch.device('cpu')\n",
    "#model = TheModelClass(*args, **kwargs)\n",
    "model = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "#model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "model.load_state_dict(torch.load(model_dir, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da93a9-ac6f-4f8c-bafc-6309b3697f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the SSIM score for every image after a feedforward propagation through \n",
    "#the network.\n",
    "#Subtract the image SSIM score before the feedforward prop to obtain the net improvement for every image.\n",
    "#Print the average improvement and the average SSIM score after the reconstruction.\n",
    "SSIM_improvement = []\n",
    "SSIM_score = []\n",
    "MSE_improvement = []\n",
    "MSE_score = []\n",
    "MIE_improvement = []\n",
    "MIE_score = []\n",
    "for i in range(0,len(train_dataset)):\n",
    "    gt, image = train_dataset[i]\n",
    "    #image = image.unsqueeze(0).to('cuda:0')\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.unsqueeze(0)\n",
    "    gt = gt.unsqueeze(0).numpy()\n",
    "    output = model(image)\n",
    "  #  output = output.squeeze(1).cpu().detach().numpy()\n",
    "    output = output.squeeze(1).detach().numpy()\n",
    "    image = image.squeeze(1).numpy()\n",
    "    gt =  np.squeeze(gt)\n",
    "    output =  np.squeeze(output)\n",
    "    image =  np.squeeze(image)\n",
    "\n",
    "\n",
    "    output_loss1 = torch.tensor(ssim(gt, output))\n",
    "    output_loss2 = torch.tensor(mse(gt, output))\n",
    "    output_loss3 = np.mean(np.abs(gt - output))\n",
    "  #  image_loss = torch.tensor(ssim(gt, image.squeeze(1).cpu().numpy()))\n",
    "    image_loss1 = torch.tensor(ssim(gt, image))\n",
    "    image_loss2 = torch.tensor(mse(gt, image))\n",
    "    image_loss3 = np.mean(np.abs(gt - image))\n",
    "    SSIM_improvement.append(output_loss1.item()-image_loss1.item())\n",
    "    SSIM_score.append(output_loss1.item())\n",
    "    MSE_improvement.append(output_loss2.item()-image_loss2.item())\n",
    "    MSE_score.append(output_loss2.item())\n",
    "    MIE_improvement.append(output_loss3 -image_loss3)\n",
    "    MIE_score.append(output_loss3)\n",
    "\n",
    "print(np.nanmean(SSIM_improvement))\n",
    "print(np.nanmean(SSIM_score))\n",
    "print(np.nanmean(MSE_improvement))\n",
    "print(np.nanmean(MSE_score))\n",
    "print(np.nanmean(MIE_improvement))\n",
    "print(np.nanmean(MIE_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1563191d-9049-477d-bc60-e1ae5a9bd29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_improvement.sort()\n",
    "plt.plot(SSIM_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad74e5-06c1-44c4-b530-e1c24264b865",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_improvement.sort()\n",
    "plt.plot(MSE_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608bfd64-4ab5-4acc-83c0-5f3c2a0df031",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIE_improvement.sort()\n",
    "plt.plot(MSE_improvement)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "214e7b82-ec39-4884-b875-c23f067c170d",
   "metadata": {},
   "source": [
    "https://www.cns.nyu.edu/~lcv/ssim/\n",
    "\n",
    "-0.08275149106735159\n",
    "0.4152959283673115\n",
    "-0.0008673634965751041\n",
    "0.0009822993672129392\n",
    "-2.285560072865337\n",
    "inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d3b135-723d-435d-bb9c-ae79c2321953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec45d6-77fd-4e9b-bce1-6e1509314857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f1e7bc-59cb-4f2c-ad59-98f414e8154c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f85e662-842f-4cc4-8843-1a15346d481b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4b75fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "240b151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"data/test/images/244.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93c8308e-2523-4ff3-a3c2-fe3784abf34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from numpy.fft import fftshift, ifftshift, fftn, ifftn\n",
    "import cmath\n",
    "def noise_and_kspace(image):\n",
    "    #change to k-space\n",
    "    img_fft = fftshift(fftn(image))\n",
    "    size_img = img_fft.shape\n",
    "     #np.random.uniform, np.random.normal\n",
    "    std = np.random.normal(0.000, 0.005) * np.amax(img_fft)\n",
    "    noise = fftshift(std * np.random.standard_normal(size_img) + std * 1j * np.random.standard_normal(size_img));     #This generates a complex noise signal.\n",
    "    img_fft_noise = img_fft + noise # k-space\n",
    "    img_noise = ifftn(ifftshift(img_fft_noise))# revert k-space back to noise\n",
    "    return img_fft, img_fft_noise, img_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff493574",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:15: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  from ipykernel import kernelapp as app\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:16: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "im_frame = Image.open(file_dir)\n",
    "   \n",
    "img_fft, img_fft_noise, img_noise = noise_and_kspace(im_frame)\n",
    "\n",
    "preprocess = T.Compose([\n",
    "                       # T.Grayscale(num_output_channels=1),\n",
    "                           T.Resize(128),    #128 as maximum\n",
    "                           T.CenterCrop(128),\n",
    "                           T.ToTensor() #,\n",
    "                           #T.Normalize(\n",
    "                            #        mean=[0.485, 0.456, 0.406],\n",
    "                               #        std=[0.229, 0.224, 0.225]\n",
    "                             ##         )\n",
    "                            ])\n",
    "img_gt = preprocess(Image.fromarray(np.uint8(img_fft)).convert('L'))\n",
    "img_und = preprocess(Image.fromarray(np.uint8(img_fft_noise)).convert('L'))\n",
    "    \n",
    "n1 = (img_und**2).sum(dim=-1).sqrt()\n",
    "norm = n1.max() \n",
    "if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "img_gt, img_und = img_gt/norm , img_und/norm  \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ae03d59-a632-4194-971b-b17417c24dee",
   "metadata": {},
   "source": [
    "torchvision.transforms.ConvertImageDtype\n",
    "torchvision.transforms.ToPILImage\n",
    "torchvision.transforms.PILToTensor"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c636e0f-46e8-4054-b137-de326985c275",
   "metadata": {},
   "source": [
    "im_1 = img_gt * norm\n",
    "#im_1 = im_1.numpy()\n",
    "im_1 = T.ToPILImage()(im_1)\n",
    "im_1.size"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62042616-a908-441a-b0a7-d56a73416501",
   "metadata": {},
   "source": [
    "display(im_frame)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58edadac-1acb-4d05-b102-cb4dfcc9c7c3",
   "metadata": {},
   "source": [
    "display(im_1) # success in turning Image into right input"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21476dc7-a09b-4997-842d-7fe3ed9228e6",
   "metadata": {},
   "source": [
    "np_image1 = np.reshape(im_1, (64, 64))# image noise numpy array\n",
    "im_r1 = Image.fromarray(np_image1).convert('RGB')\n",
    "display(im_r1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "106789d1-862f-4ee8-925a-c51452b936f4",
   "metadata": {},
   "source": [
    "im_frame = Image.open(file_dir)\n",
    "   \n",
    "noise_im_frame = noise_and_kspace(im_frame)\n",
    "\n",
    "preprocess = T.Compose([\n",
    "                       # T.Grayscale(num_output_channels=1),\n",
    "                           T.Resize(64),    #128 as maximum\n",
    "                           T.CenterCrop(64),\n",
    "                           T.ToTensor() #,\n",
    "                           #T.Normalize(\n",
    "                            #        mean=[0.485, 0.456, 0.406],\n",
    "                               #        std=[0.229, 0.224, 0.225]\n",
    "                             ##         )\n",
    "                            ])\n",
    "img_gt = preprocess(Image.fromarray(np.uint8(im_frame)).convert('L'))\n",
    "im_1 = img_gt.numpy()\n",
    "im_1.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "885c0d8b-9500-41c6-bf8f-3206eabfd132",
   "metadata": {},
   "source": [
    "np_image1 = np.reshape(im_1, (64, 64))# image noise numpy array\n",
    "im_r1 = Image.fromarray(np_image1).convert('L')\n",
    "display(im_r1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ed528d5-9046-4abd-b107-0436cdd6b576",
   "metadata": {},
   "source": [
    "image = Image.open(file_dir)    \n",
    "img_fft = fftshift(fftn(image))\n",
    "np_image = np.uint8(img_fft)# image noise numpy array\n",
    "im_n = Image.fromarray(np_image).convert('L')\n",
    "display(im_n)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f705ae9e",
   "metadata": {},
   "source": [
    "https://www.kite.com/python/examples/4887/PIL-convert-between-a-pil-%60image%60-and-a-numpy-%60array%60\n",
    "\n",
    "https://stackoverflow.com/questions/2659312/how-do-i-convert-a-numpy-array-to-and-display-an-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73094677-ada9-48d4-94dd-70bb1bb0bbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "31688b6f-03c3-4cd3-8754-0ec5095a07f2",
   "metadata": {},
   "source": [
    "from IPython.display import display\n",
    "display(im_noise)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d5251b8-1579-47b4-a6c7-6851fc00cd21",
   "metadata": {},
   "source": [
    "from skimage.metrics import structural_similarity as cmp_ssim \n",
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return cmp_ssim(\n",
    "        gt, pred, multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7bafb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #noise_image = noise_image.to('cuda:0')\n",
    "#img_gt = img_gt.numpy()\n",
    "img_und = img_und.unsqueeze(0)\n",
    "output = model(img_und)\n",
    "   # output = output.squeeze(1).cpu().detach().numpy()\n",
    "output = output.squeeze(1).detach() #.numpy()   #image under numpy form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "652b07c7-95ac-47bb-a8f7-bbc59d10d02b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 128])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a0404e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:5: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "np_rescontruct_image =  output # np.reshape(output, (64, 64))# image noise numpy array\n",
    "im_reconstruct = T.ToPILImage()(np_rescontruct_image)#Image.fromarray(np_rescontruct_image).convert('L')\n",
    "inputa = np.asarray(im_reconstruct)\n",
    "final = ifftn(ifftshift(inputa))\n",
    "I = PIL.Image.fromarray(np.uint8(final))\n",
    "I.save(\"testing/test.png\") #for prediction values\n",
    "I.save(\"pred1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2c66ce9e-8dcc-47b5-9d26-8461dc6cefba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=128x128 at 0x7FC9CFC44650>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(im_reconstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b22263aa-0947-4689-b648-8edb9e9560f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIAAAACACAAAAADmVT4XAAAAuUlEQVR4nO3XWwrDIBBAUdstu47uuX8NpWoeTEYL53ylIeBlFNKUAgAAAAAAMNnj0FO1lPKaFVC3yzsa9gLq98/4hJ2A+nMnOuF5cv3WrfsCmosFF4wCOkvFFgwCood9OqArtKwfkDOASxMIbbsUEGndgKQjsPAEBNz0/+N4QJb/DIjcnn5A0iG4MoHQtEFAzghGE+gUxIYNt6C5VPBgxmegsVj0xqz+YVKmf5p9IrLeDQAAAAAAAOne6FsSNnZwjNMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=128x128 at 0x7FC9CF4D58D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(im_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04cccab-ad36-4d86-817f-6cbaa7114177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56389949-3a53-47b2-91d1-3ecef15eddd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
