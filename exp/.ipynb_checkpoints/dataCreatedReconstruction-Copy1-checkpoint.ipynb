{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "681fee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Importing all the relevant library\n",
    "%matplotlib inline\n",
    "import h5py, os\n",
    "#from functions import transforms as T\n",
    "#from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as T\n",
    "import glob\n",
    "#from functions import transforms as T \n",
    "#from functions.subsample import MaskFunc\n",
    "from PIL import Image\n",
    "import random\n",
    "from numpy.fft import fftshift, ifftshift, fftn, ifftn\n",
    "import cmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f4d478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bbd1420",
   "metadata": {},
   "source": [
    "AWS has torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57966d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class MaskFunc:\n",
    "    \"\"\"\n",
    "    MaskFunc creates a sub-sampling mask of a given shape.\n",
    "\n",
    "    The mask selects a subset of columns from the input k-space data. If the k-space data has N\n",
    "    columns, the mask picks out:\n",
    "        1. N_low_freqs = (N * center_fraction) columns in the center corresponding to\n",
    "           low-frequencies\n",
    "        2. The other columns are selected uniformly at random with a probability equal to:\n",
    "           prob = (N / acceleration - N_low_freqs) / (N - N_low_freqs).\n",
    "    This ensures that the expected number of columns selected is equal to (N / acceleration)\n",
    "\n",
    "    It is possible to use multiple center_fractions and accelerations, in which case one possible\n",
    "    (center_fraction, acceleration) is chosen uniformly at random each time the MaskFunc object is\n",
    "    called.\n",
    "\n",
    "    For example, if accelerations = [4, 8] and center_fractions = [0.08, 0.04], then there\n",
    "    is a 50% probability that 4-fold acceleration with 8% center fraction is selected and a 50%\n",
    "    probability that 8-fold acceleration with 4% center fraction is selected.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, center_fractions, accelerations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            center_fractions (List[float]): Fraction of low-frequency columns to be retained.\n",
    "                If multiple values are provided, then one of these numbers is chosen uniformly\n",
    "                each time.\n",
    "\n",
    "            accelerations (List[int]): Amount of under-sampling. This should have the same length\n",
    "                as center_fractions. If multiple values are provided, then one of these is chosen\n",
    "                uniformly each time. An acceleration of 4 retains 25% of the columns, but they may\n",
    "                not be spaced evenly.\n",
    "        \"\"\"\n",
    "        if len(center_fractions) != len(accelerations):\n",
    "            raise ValueError('Number of center fractions should match number of accelerations')\n",
    "\n",
    "        self.center_fractions = center_fractions\n",
    "        self.accelerations = accelerations\n",
    "        self.rng = np.random.RandomState()\n",
    "\n",
    "    def __call__(self, shape, seed=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            shape (iterable[int]): The shape of the mask to be created. The shape should have\n",
    "                at least 3 dimensions. Samples are drawn along the second last dimension.\n",
    "            seed (int, optional): Seed for the random number generator. Setting the seed\n",
    "                ensures the same mask is generated each time for the same shape.\n",
    "        Returns:\n",
    "            torch.Tensor: A mask of the specified shape.\n",
    "        \"\"\"\n",
    "        if len(shape) < 3:\n",
    "            raise ValueError('Shape should have 3 or more dimensions')\n",
    "\n",
    "        self.rng.seed(seed)\n",
    "        num_cols = shape[-2]\n",
    "\n",
    "        choice = self.rng.randint(0, len(self.accelerations))\n",
    "        center_fraction = self.center_fractions[choice]\n",
    "        acceleration = self.accelerations[choice]\n",
    "\n",
    "        # Create the mask\n",
    "        num_low_freqs = int(round(num_cols * center_fraction))\n",
    "        prob = (num_cols / acceleration - num_low_freqs) / (num_cols - num_low_freqs)\n",
    "        mask = self.rng.uniform(size=num_cols) < prob\n",
    "        pad = (num_cols - num_low_freqs + 1) // 2\n",
    "        mask[pad:pad + num_low_freqs] = True\n",
    "\n",
    "        # Reshape the mask\n",
    "        mask_shape = [1 for _ in shape]\n",
    "        mask_shape[-2] = num_cols\n",
    "        mask = torch.from_numpy(mask.reshape(*mask_shape).astype(np.float32))\n",
    "\n",
    "        return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0dce273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.fft import fftshift, ifftshift, fftn, ifftn\n",
    "\n",
    "def transform_kspace_to_image(k, dim=None, img_shape=None):\n",
    "    \"\"\" Computes the Fourier transform from k-space to image space\n",
    "    along a given or all dimensions\n",
    "    :param k: k-space data\n",
    "    :param dim: vector of dimensions to transform\n",
    "    :param img_shape: desired shape of output image\n",
    "    :returns: data in image space (along transformed dimensions)\n",
    "    \"\"\"\n",
    "    if not dim:\n",
    "        dim = range(k.ndim)\n",
    "\n",
    "    img = fftshift(ifftn(ifftshift(k, axes=dim), s=img_shape, axes=dim), axes=dim)\n",
    "    #img = fftshift(ifft2(ifftshift(k, dim=dim)), dim=dim)\n",
    "    img *= np.sqrt(np.prod(np.take(img.shape, dim)))\n",
    "    return img\n",
    "\n",
    "\n",
    "def transform_image_to_kspace(img, dim=None, k_shape=None):\n",
    "    \"\"\" Computes the Fourier transform from image space to k-space space\n",
    "    along a given or all dimensions\n",
    "    :param img: image space data\n",
    "    :param dim: vector of dimensions to transform\n",
    "    :param k_shape: desired shape of output k-space data\n",
    "    :returns: data in k-space (along transformed dimensions)\n",
    "    \"\"\"\n",
    "    if not dim:\n",
    "        dim = range(img.ndim)\n",
    "\n",
    "    k = fftshift(fftn(ifftshift(img, axes=dim), s=k_shape, axes=dim), axes=dim)\n",
    "    #k = fftshift(fft2(ifftshift(img, dim=dim)), dim=dim)\n",
    "    k /= np.sqrt(np.prod(np.take(img.shape, dim)))\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1579782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def add_noise(array: np.ndarray, dropout_rate: float = 0.10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function adds noise\n",
    "    :param array:\n",
    "    :param dropout_rate: percent of pixels to be dropped\n",
    "    :return:\n",
    "    \"\"\"\n",
    "   # assert len(array.shape) == 4\n",
    "   # assert array.shape[1] == 3\n",
    "\n",
    "    channels = array.shape[1]\n",
    "    height = array.shape[2]\n",
    "    width = array.shape[3]\n",
    "\n",
    "    total_pixels = height * width\n",
    "    queued_pixels = int(total_pixels * dropout_rate)\n",
    "\n",
    "    filled_pixels = 0\n",
    "    while filled_pixels < queued_pixels:\n",
    "        d_h = random.randint(1, 3)\n",
    "        d_w = random.randint(1, 3)\n",
    "        filled_pixels += d_h * d_w\n",
    "        h = random.randint(0, height - d_h)\n",
    "        w = random.randint(0, width - d_w)\n",
    "\n",
    "        # now overwrite selected pixels with random dark color\n",
    "        array[:, :, h:h+d_h, w:w+d_w] = random.randint(1, 25) / 255.0\n",
    "\n",
    "    return array\n",
    "\n",
    "\n",
    "def array_to_image(array: np.ndarray):\n",
    "    \"\"\"\n",
    "    This function converts NumPy array to image\n",
    "    :param array:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(array.shape) == 4\n",
    "    assert array.shape[1] == 1 or array.shape[1] == 3\n",
    "\n",
    "    scaled = (array * 255.0).astype(np.uint8)\n",
    "    if array.shape[1] == 1:\n",
    "        reshaped = scaled.reshape((array.shape[2], array.shape[3]))\n",
    "        return Image.fromarray(reshaped,\"L\")\n",
    "    else:\n",
    "        reshaped = scaled.reshape((3, array.shape[2], array.shape[3])).transpose([1, 2, 0])\n",
    "        return Image.fromarray(reshaped,\"RGB\")\n",
    "\n",
    "\n",
    "def image_to_array(image) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function converts image to NumPy array\n",
    "    :param image:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    image1 = np.array(image)\n",
    "    return np.expand_dims(np.asarray(image1), 0).astype(np.float) / 255.0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac2c258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "327d8054",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "\n",
    "        return get_epoch_batch(subject_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8209b3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from numpy.fft import fftshift, ifftshift, fftn, ifftn\n",
    "import cmath\n",
    "def noise_and_kspace(image):\n",
    "    #change to k-space\n",
    "    img_fft = fftshift(fftn(image))\n",
    "    size_img = img_fft.shape\n",
    "     #np.random.uniform, np.random.normal\n",
    "    std = np.random.normal(0.000, 0.005) * np.amax(img_fft)\n",
    "    noise = fftshift(std * np.random.standard_normal(size_img) + std * 1j * np.random.standard_normal(size_img));     #This generates a complex noise signal.\n",
    "    img_fft_noise = img_fft + noise # k-space\n",
    "    img_noise = ifftn(ifftshift(img_fft_noise))# revert k-space back to noise\n",
    "    return img_fft, img_fft_noise, img_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56178edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_name = subject_id  \n",
    "    \n",
    "#    with h5py.File(rawdata_name, 'r') as data:\n",
    "#        rawdata = data['kspace'][slice]\n",
    "   \n",
    "    im_frame = Image.open(rawdata_name)\n",
    "    im_fft, im_fft_noise, noise_im_frame = noise_and_kspace(im_frame)\n",
    "\n",
    "    ############################\n",
    "    #img_und = to_tensor(np.array(noise_im_frame)).unsqueeze(0) # noise image tensor form    \n",
    "    preprocess = T.Compose([\n",
    "                       # T.Grayscale(num_output_channels=1),\n",
    "                           T.Resize(128),    #128 as maximum #64\n",
    "                           T.CenterCrop(128),\n",
    "                           T.ToTensor() #,\n",
    "                            ])\n",
    "    img_gt = preprocess(Image.fromarray(np.uint8(im_fft)).convert('L'))\n",
    "    img_und = preprocess(Image.fromarray(np.uint8(im_fft_noise)).convert('L'))\n",
    "    \n",
    "    n1 = (img_und**2).sum(dim=-1).sqrt()\n",
    "    norm = n1.max() \n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "    img_gt, img_und = img_gt/norm , img_und/norm\n",
    "\n",
    "    return img_gt.squeeze(0), img_und.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575b71b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path, val_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)):\n",
    "\n",
    "        data_list[train_and_val[i]] = []\n",
    "        \n",
    "        which_data_path = data_path[i]\n",
    "        tr = 0\n",
    "        te = 0\n",
    "        alfa = 0\n",
    "    \n",
    "        for fname in sorted(os.listdir(which_data_path + '/images')):\n",
    "            if fname == '.DS_Store': continue\n",
    "            \n",
    "            subject_data_path = os.path.join(which_data_path + '/images', fname)\n",
    "                     \n",
    "            if not os.path.isfile(subject_data_path): continue \n",
    "            \n",
    "     \n",
    "            #get information from text file\n",
    "            # this will return a tuple of root and extension\n",
    "            split_tup = os.path.splitext(fname)\n",
    "\n",
    "  \n",
    "            # extract the file name and extension\n",
    "            file_name = split_tup[0]\n",
    "  \n",
    "  \n",
    "            data_list[train_and_val[i]].append((fname, subject_data_path))\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b0eeec3b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "img = imread('IMage Path'); %This is where I read the image matrix\n",
    "img_fft = fftshift(fft2(img));  %change the image to K-Space\n",
    "size_img = size(img_fft); Extract the size matrix for image to create a noise for the same size.\n",
    "\n",
    "%standard deviation decided for the noise. I am using a uniform distribution but you can choose what works best for you.\n",
    "%This noise is with 0 mean and std standard deviation\n",
    "\n",
    "std  = unifrnd(0.000,0.001)*max(img_fft(:,:),[],'all');\n",
    "\n",
    "noise = fftshift(std.*randn(size(img_fft)) + std1.*1i*randn(size(img_fft)));     %This generates a complex noise signal.\n",
    "\n",
    "img_ftt_noise = img_fft + noise;  %Here we add noise to the original image K-Space\n",
    "\n",
    "img_new = ifft2(ifftshift(img_fft));  %This gives you the noise back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0961b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb098708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74c35f60",
   "metadata": {},
   "source": [
    "# The initial model, based on the AlexNet architecture"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfb03fcb-f57e-4c4c-b858-26bfa87a57cf",
   "metadata": {},
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1), #320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64 ,64, kernel_size=3, padding=1),  # 320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # 320/320\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, padding=1),  # 320/320\n",
    "            \n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b87ba",
   "metadata": {},
   "source": [
    "# RestNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957014f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "560bd999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseBlock(torch.nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self,input_planes,planes,stride=1,dim_change=None):\n",
    "        super(baseBlock,self).__init__()\n",
    "        #declare convolutional layers with batch norms\n",
    "        self.conv1 = torch.nn.Conv2d(input_planes,planes,stride=stride,kernel_size=3,padding=1)\n",
    "        self.bn1   = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv2 = torch.nn.Conv2d(planes,planes,stride=1,kernel_size=3,padding=1)\n",
    "        self.bn2   = torch.nn.BatchNorm2d(planes)\n",
    "        self.dim_change = dim_change\n",
    "    def forward(self,x):\n",
    "        #Save the residue\n",
    "        res = x\n",
    "        output = F.relu(self.bn1(self.conv1(x)))\n",
    "        output = self.bn2(self.conv2(output))\n",
    "        if self.dim_change is not None:\n",
    "            res = self.dim_change(res)\n",
    "        \n",
    "        output += res\n",
    "        output = F.relu(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self,block,num_layers,classes=10):\n",
    "        super(ResNet,self).__init__()\n",
    "        #according to research paper:\n",
    "        self.input_planes = 64 #256\n",
    "        self.conv1 = torch.nn.Conv2d(1,64,kernel_size=3,stride=1,padding=1)\n",
    "        self.layer1 = self._layer(block,64,num_layers[0],stride=1)\n",
    "        self.layer2 = self._layer(block,64,num_layers[1],stride=1)\n",
    "        self.layer3 = self._layer(block,64,num_layers[2],stride=1)\n",
    "        self.layer4 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer5 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer6 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer7 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer8 = self._layer(block,64,num_layers[2],stride=1)\n",
    "        self.layer9 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer10 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer11 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer12 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer13 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer14 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer15 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer16 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer17 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer18 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.conv2 = torch.nn.Conv2d(64,1,kernel_size=3,stride=1, padding=1)\n",
    "        \n",
    "    \n",
    "    def _layer(self,block,planes,num_layers,stride=1):\n",
    "        dim_change = None\n",
    "        if stride!=1 or planes != self.input_planes*block.expansion:\n",
    "            dim_change = torch.nn.Sequential(torch.nn.Conv2d(self.input_planes,planes*block.expansion,kernel_size=1,stride=stride),\n",
    "                                             torch.nn.BatchNorm2d(planes*block.expansion))\n",
    "        netLayers =[]\n",
    "        netLayers.append(block(self.input_planes,planes,stride=stride,dim_change=dim_change))\n",
    "        self.input_planes = planes * block.expansion\n",
    "        for i in range(1,num_layers):\n",
    "            netLayers.append(block(self.input_planes,planes))\n",
    "            self.input_planes = planes * block.expansion\n",
    "        \n",
    "        return torch.nn.Sequential(*netLayers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.layer8(x)\n",
    "        x = self.layer9(x)\n",
    "        x = self.layer10(x)\n",
    "        x = self.layer11(x)\n",
    "        x = self.layer12(x)\n",
    "        x = self.layer13(x)\n",
    "        x = self.layer14(x)\n",
    "        x = self.layer15(x)\n",
    "        x = self.layer16(x)\n",
    "        x = self.layer17(x)\n",
    "        x = self.layer18(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8195fdba",
   "metadata": {},
   "source": [
    "Changed in version 0.16: This function was renamed from skimage.measure.compare_ssim to skimage.metrics.structural_similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b06b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as cmp_ssim \n",
    "from skimage.metrics import mean_squared_error\n",
    "from skimage.metrics import normalized_root_mse\n",
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return cmp_ssim(\n",
    "         gt, pred, multichannel=False, data_range=gt.max()\n",
    "    )\n",
    "#def ssim(gt, pred):\n",
    "#    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "#    return cmp_ssim(\n",
    " #       gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    " #   )\n",
    "def mse(gt, pred):\n",
    "    \"\"\" Compute mean squared error. \"\"\"\n",
    "    return mean_squared_error(gt, pred)\n",
    "\n",
    "def nrmse(gt, pred):\n",
    "    \"\"\" Compute normalized root mse. \"\"\"\n",
    "    return normalized_root_mse(gt, pred)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "755c3b23",
   "metadata": {},
   "source": [
    "All the code from here on is almost the same as before, only for the 8fold data. The main difference is the learning rate, which is 3 times higher for the 8fold data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44d5dbbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish data loading- now train\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "data_path_train = 'data/train'\n",
    "data_path_val = 'data/val'\n",
    "data_list = load_data_path(data_path_train, data_path_val)\n",
    "    \n",
    "\n",
    "num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "#mae_loss = nn.L1Loss().to('cuda:0')\n",
    "mae_loss = nn.L1Loss()\n",
    "lr = 3e-3\n",
    "    #acc =8 , network_8fold\n",
    "network_8fold = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "#network_8fold.to('cuda:0') #move the model on the GPU\n",
    "\n",
    "    \n",
    "optimizer2 = optim.Adam(network_8fold.parameters(), lr=lr)\n",
    "\n",
    " \n",
    "train_dataset = MRIDataset(data_list['train'])\n",
    "val_dataset = MRIDataset(data_list['val'])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, batch_size=64, num_workers=num_workers) \n",
    "print(\"finish data loading- now train\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "061c8b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (layer1): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer5): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer6): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer7): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer8): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer9): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer10): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer11): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer12): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer13): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer14): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer15): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer16): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer17): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer18): Sequential(\n",
       "    (0): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): baseBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (conv2): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_8fold"
   ]
  },
  {
   "cell_type": "raw",
   "id": "69044fe6",
   "metadata": {},
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "then = time.time() #Time before the operations start\n",
    "\n",
    "#DO YOUR OPERATIONS HERE\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    for iteration, sample in enumerate(train_loader):\n",
    "        img_nr += 1\n",
    "        img_gt, img_und = sample\n",
    "        #print(img_und.shape)\n",
    "\n",
    "now = time.time() #Time after it finished\n",
    "\n",
    "print(\"It took: \", now-then, \" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8672c18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value:  3.0358777046203613\n",
      "Loss value:  8.857916831970215\n",
      "Loss value:  1.4599742889404297\n",
      "Loss value:  2.9784045219421387\n",
      "Loss value:  1.6598483324050903\n",
      "Loss value:  0.35254162549972534\n",
      "Loss value:  0.765097975730896\n",
      "Loss value:  0.6559505462646484\n",
      "Loss value:  0.5062642693519592\n",
      "Loss value:  0.4383732080459595\n",
      "Loss value:  0.647632360458374\n",
      "Loss value:  0.3055412471294403\n",
      "Loss value:  0.5048716068267822\n",
      "Loss value:  0.7036938071250916\n",
      "Loss value:  0.6338426470756531\n",
      "Loss value:  0.5964290499687195\n",
      "Loss value:  0.26996156573295593\n",
      "Loss value:  0.19861972332000732\n",
      "Loss value:  0.30773842334747314\n",
      "Loss value:  0.1307356059551239\n",
      "L1 Loss score:  1.25047   Image number:  20   Epoch:  1\n",
      "Loss value:  0.35360559821128845\n",
      "Loss value:  0.43363648653030396\n",
      "Loss value:  0.27148595452308655\n",
      "Loss value:  0.3837927281856537\n",
      "Loss value:  0.2955552935600281\n",
      "Loss value:  0.33443909883499146\n",
      "Loss value:  0.3662016689777374\n",
      "Loss value:  0.22394882142543793\n",
      "Loss value:  0.20098024606704712\n",
      "Loss value:  0.3390505313873291\n",
      "Loss value:  0.3525870442390442\n",
      "Loss value:  0.29772722721099854\n",
      "Loss value:  0.2090839147567749\n",
      "Loss value:  0.2867293953895569\n",
      "Loss value:  0.2840018570423126\n",
      "Loss value:  0.21407365798950195\n",
      "Loss value:  0.23016081750392914\n",
      "Loss value:  0.1650574654340744\n",
      "Loss value:  0.16168466210365295\n",
      "Loss value:  0.1810266673564911\n",
      "L1 Loss score:  0.27924   Image number:  40   Epoch:  1\n",
      "Loss value:  0.23578964173793793\n",
      "Loss value:  0.16072094440460205\n",
      "Loss value:  0.36249279975891113\n",
      "Loss value:  0.10972198098897934\n",
      "Loss value:  0.556226909160614\n",
      "Loss value:  0.6334924101829529\n",
      "Loss value:  0.3454274535179138\n",
      "Loss value:  0.28827518224716187\n",
      "Loss value:  0.27459052205085754\n",
      "Loss value:  0.3332156538963318\n",
      "Loss value:  0.3387913703918457\n",
      "Loss value:  0.268412709236145\n",
      "Loss value:  0.3570977449417114\n",
      "Loss value:  0.4205682873725891\n",
      "Loss value:  0.2751266062259674\n",
      "Loss value:  0.26009732484817505\n",
      "Loss value:  0.2918958067893982\n",
      "Loss value:  0.19233447313308716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value:  0.22153037786483765\n",
      "Loss value:  0.22744429111480713\n",
      "L1 Loss score:  0.30766   Image number:  60   Epoch:  2\n",
      "Loss value:  0.18511022627353668\n",
      "Loss value:  0.3050205707550049\n",
      "Loss value:  0.36794397234916687\n",
      "Loss value:  0.2518995404243469\n",
      "Loss value:  0.1563062220811844\n",
      "Loss value:  0.26837873458862305\n",
      "Loss value:  0.21150381863117218\n",
      "Loss value:  0.25419819355010986\n",
      "Loss value:  0.17066887021064758\n",
      "Loss value:  0.2246973216533661\n",
      "Loss value:  0.17736001312732697\n",
      "Loss value:  0.20228701829910278\n",
      "Loss value:  0.1717577874660492\n",
      "Loss value:  0.12942467629909515\n",
      "Loss value:  0.2947775423526764\n",
      "Loss value:  0.24730849266052246\n",
      "Loss value:  0.1644371896982193\n",
      "Loss value:  0.19981169700622559\n",
      "Loss value:  0.21124473214149475\n",
      "Loss value:  0.10797423869371414\n",
      "L1 Loss score:  0.21511   Image number:  80   Epoch:  2\n",
      "Loss value:  0.30953481793403625\n",
      "Loss value:  0.2462417483329773\n",
      "Loss value:  0.187715545296669\n",
      "Loss value:  0.3072466254234314\n",
      "Loss value:  0.12772372364997864\n",
      "Loss value:  0.31572675704956055\n",
      "Loss value:  0.4753144383430481\n",
      "Loss value:  0.4885808825492859\n",
      "Loss value:  0.3121907114982605\n",
      "Loss value:  0.3115321397781372\n",
      "Loss value:  0.3710673451423645\n",
      "Loss value:  0.3641776442527771\n",
      "Loss value:  0.19037213921546936\n",
      "Loss value:  0.13039575517177582\n",
      "Loss value:  0.19012482464313507\n",
      "Loss value:  0.24166230857372284\n",
      "Loss value:  0.17190587520599365\n",
      "Loss value:  0.15496721863746643\n",
      "Loss value:  0.1692010462284088\n",
      "Loss value:  0.18399490416049957\n",
      "L1 Loss score:  0.26248   Image number:  100   Epoch:  2\n",
      "Loss value:  0.19301947951316833\n",
      "Loss value:  0.19860655069351196\n",
      "Loss value:  0.22168764472007751\n",
      "Loss value:  0.17596857249736786\n",
      "Loss value:  0.13842983543872833\n",
      "Loss value:  0.17414525151252747\n",
      "Loss value:  0.14431755244731903\n",
      "Loss value:  0.11898747831583023\n",
      "Loss value:  0.0899735540151596\n",
      "Loss value:  0.07188408076763153\n",
      "Loss value:  0.1590411365032196\n",
      "Loss value:  0.1392066478729248\n",
      "Loss value:  0.18791241943836212\n",
      "Loss value:  0.2667274475097656\n",
      "Loss value:  0.2785928249359131\n",
      "Loss value:  0.16243305802345276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:20: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/ipykernel_launcher.py:21: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss value:  0.1780601143836975\n",
      "Loss value:  0.22842496633529663\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "then = time.time() #Time before the operations start\n",
    "losses2=[]\n",
    "mean_loss_list = []\n",
    "img_nr = 0\n",
    "epoch_nums = 5 #64 # 5\n",
    "for epoch in range(epoch_nums):\n",
    "    for iteration, sample in enumerate(train_loader):\n",
    "        img_nr += 1\n",
    "        img_gt, img_und = sample\n",
    "        \n",
    "        img_gt = img_gt.unsqueeze(1)#.to('cuda:0') # img_gt = img_gt.unsqueeze(1).to('cuda:0')\n",
    "        img_und = img_und.unsqueeze(1)#.to('cuda:0') #img_und = img_und.unsqueeze(1).to('cuda:0')\n",
    "       # img_gt = img_gt.to('cuda:0')\n",
    "       # img_und = img_und.to('cuda:0')\n",
    "            \n",
    "        output = network_8fold(img_und)      #feedforward\n",
    "        #print(output.shape) #// debug\n",
    "        loss = mae_loss(output, img_gt)\n",
    "\n",
    "        optimizer2.zero_grad()       #set current gradients to 0\n",
    "        loss.backward()      #backpropagate\n",
    "        optimizer2.step()     #update the weights\n",
    "        mean_loss_list.append(loss.item())\n",
    "        print(\"Loss value: \", loss.item())\n",
    "            #compute and print the mean L1 lossscore for the last 20 training images.\n",
    "        if img_nr%20 == 0:\n",
    "            print(\"L1 Loss score: \", np.round(np.mean(mean_loss_list), decimals = 5), \"  Image number: \", img_nr, \"  Epoch: \", epoch+1)\n",
    "            mean_loss_list = []\n",
    "        losses2.append(loss.item() * img_gt.size(0))\n",
    "        \n",
    "now = time.time() #Time after it finished\n",
    "\n",
    "print(\"It took: \", now-then, \" seconds\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2f0d027e",
   "metadata": {},
   "source": [
    "From the Pytorch documentation on convolutional layers, Conv2d layers expect input with the shape\n",
    "\n",
    "(n_samples, channels, height, width) # e.g., (1000, 1, 224, 224)\n",
    "Passing grayscale images in their usual format (224, 224) won't work.\n",
    "\n",
    "To get the right shape, you will need to add a channel dimension. You can do it as follows:\n",
    "\n",
    "x = np.expand_dims(x, 1)      # if numpy array\n",
    "tensor = tensor.unsqueeze(1)  # if torch tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da32f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f59efc7b-fde5-47a8-a566-f5ca13563301",
   "metadata": {},
   "source": [
    "np.moveaxis(arr, 1, -3)\n",
    "https://github.com/scikit-image/scikit-image/issues/4636\n",
    "https://stackoverflow.com/questions/68079012/valueerror-win-size-exceeds-image-extent-if-the-input-is-a-multichannel-color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7795a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the SSIM score for every image after a feedforward propagation through \n",
    "#the network.\n",
    "#Subtract the image SSIM score before the feedforward prop to obtain the net improvement for every image.\n",
    "#Print the average improvement and the average SSIM score after the reconstruction.\n",
    "SSIM_improvement = []\n",
    "SSIM_score = []\n",
    "MSE_improvement = []\n",
    "MSE_score = []\n",
    "NRMSE_improvement = []\n",
    "NRMSE_score = []\n",
    "for i in range(0,len(val_dataset)):\n",
    "    gt, image = val_dataset[i]\n",
    "    #image = image.unsqueeze(0).to('cuda:0')\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.unsqueeze(0)\n",
    "    gt = gt.unsqueeze(0).numpy()\n",
    "    output = network_8fold(image)\n",
    "  #  output = output.squeeze(1).cpu().detach().numpy()\n",
    "    output = output.squeeze(1).detach().numpy()\n",
    "    image = image.squeeze(1).numpy()\n",
    "    gt =  np.squeeze(gt)\n",
    "    output =  np.squeeze(output)\n",
    "    image =  np.squeeze(image)\n",
    "\n",
    "\n",
    "    output_loss1 = torch.tensor(ssim(gt, output))\n",
    "    output_loss2 = torch.tensor(mse(gt, output))\n",
    "    output_loss3 = torch.tensor(nrmse(gt, output))\n",
    "  #  image_loss = torch.tensor(ssim(gt, image.squeeze(1).cpu().numpy()))\n",
    "    image_loss1 = torch.tensor(ssim(gt, image))\n",
    "    image_loss2 = torch.tensor(mse(gt, image))\n",
    "    image_loss3 = torch.tensor(nrmse(gt, image))\n",
    "    SSIM_improvement.append(output_loss1.item()-image_loss1.item())\n",
    "    SSIM_score.append(output_loss1.item())\n",
    "    MSE_improvement.append(output_loss2.item()-image_loss2.item())\n",
    "    MSE_score.append(output_loss2.item())\n",
    "    NRMSE_improvement.append(output_loss3.item()-image_loss3.item())\n",
    "    NRMSE_score.append(output_loss3.item())\n",
    "\n",
    "print(np.nanmean(SSIM_improvement))\n",
    "print(np.nanmean(SSIM_score))\n",
    "print(np.nanmean(MSE_improvement))\n",
    "print(np.nanmean(MSE_score))\n",
    "print(np.nanmean(NRMSE_improvement))\n",
    "print(np.nanmean(NRMSE_score))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ba92d80-ff2a-4cc7-a0d1-ba55184f0938",
   "metadata": {},
   "source": [
    "#compute the SSIM score for every image after a feedforward propagation through \n",
    "#the network.\n",
    "#Subtract the image SSIM score before the feedforward prop to obtain the net improvement for every image.\n",
    "#Print the average improvement and the average SSIM score after the reconstruction.\n",
    "SSIM_improvement = []\n",
    "SSIM_score = []\n",
    "MSE_improvement = []\n",
    "MSE_score = []\n",
    "NRMSE_improvement = []\n",
    "NRMSE_score = []\n",
    "for i in range(0,len(val_dataset)):\n",
    "    gt, image = val_dataset[i]\n",
    "    #image = image.unsqueeze(0).to('cuda:0')\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.unsqueeze(0)\n",
    "    gt = gt.unsqueeze(0).numpy()\n",
    "    output = network_8fold(image)\n",
    "  #  output = output.squeeze(1).cpu().detach().numpy()\n",
    "    output = output.squeeze(1).detach().numpy()\n",
    "    image = image.squeeze(1).numpy()\n",
    "    gt =  np.squeeze(gt)\n",
    "    output =  np.squeeze(output)\n",
    "    image =  np.squeeze(image)\n",
    "\n",
    "\n",
    "    output_loss1 = torch.tensor(ssim(gt, output))\n",
    "    output_loss2 = torch.tensor(mse(gt, output))\n",
    "    output_loss3 = torch.tensor(nrmse(gt, output))\n",
    "  #  image_loss = torch.tensor(ssim(gt, image.squeeze(1).cpu().numpy()))\n",
    "    image_loss1 = torch.tensor(ssim(gt, image))\n",
    "    image_loss2 = torch.tensor(mse(gt, image))\n",
    "    image_loss3 = torch.tensor(nrmse(gt, image))\n",
    "    SSIM_improvement.append(output_loss1.item()-image_loss1.item())\n",
    "    SSIM_score.append(output_loss1.item())\n",
    "    MSE_improvement.append(output_loss2.item()-image_loss2.item())\n",
    "    MSE_score.append(output_loss2.item())\n",
    "    NRMSE_improvement.append(output_loss3.item()-image_loss3.item())\n",
    "    NRMSE_score.append(output_loss3.item())\n",
    "\n",
    "print(np.nanmean(SSIM_improvement))\n",
    "print(np.nanmean(SSIM_score))\n",
    "print(np.nanmean(MSE_improvement))\n",
    "print(np.nanmean(MSE_score))\n",
    "print(np.nanmean(NRMSE_improvement))\n",
    "print(np.nanmean(NRMSE_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c6aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_improvement.sort()\n",
    "plt.plot(SSIM_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dbcb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_improvement.sort()\n",
    "plt.plot(MSE_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NRMSE_improvement.sort()\n",
    "plt.plot(NRMSE_improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23428529",
   "metadata": {},
   "source": [
    "## save Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eb1c0afb",
   "metadata": {},
   "source": [
    "https://learn-pytorch.oneoffcoder.com/model-persistence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed9882d",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afffbf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_dir = f\"s3://savemodels/network_8fold/restnet-model{index}.pt\"\n",
    "output_dir = f\"./network_8fold/restnet-model{index}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fcce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model to S3 bucket or data\n",
    "torch.save(network_8fold.state_dict(), output_dir)\n",
    "#torch.save(network_8fold.state_dict(), './models/resnet18-model.pt')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de009c4f",
   "metadata": {},
   "source": [
    "#save the whole model\n",
    "torch.save(network_8fold, output_dir)\n",
    "#torch.save(network_8fold, './models/resnet18-model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc56219",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Model from saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9718f388-6792-42eb-9ede-ad730317cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b955f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_dir = f\"s3://savemodels/network_8fold/restnet-model{index}.pt\"\n",
    "output_dir = f\"./network_8fold/restnet-model{index}.pt\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "e9d7ffc6-c8a7-430e-a4ad-313287b60833",
   "metadata": {},
   "source": [
    "#load model on GPU\n",
    "device = torch.device(\"cuda\")\n",
    "model = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "\n",
    "#model = model.to('cuda:0')\n",
    "model.load_state_dict(torch.load(output_dir, map_location='cuda:0'))\n",
    "#model.load_state_dict(torch.load('./models/resnet18-model.pt', map_location='cuda:0'))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5448b9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67996389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model on CPU: laptop\n",
    "device = torch.device('cpu')\n",
    "#model = TheModelClass(*args, **kwargs)\n",
    "model = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "#model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "model.load_state_dict(torch.load(output_dir, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0fb806c",
   "metadata": {},
   "source": [
    "model = torch.load(PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c0fb0",
   "metadata": {},
   "source": [
    "## Predict a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad0dc8-645b-4c17-b2b1-4029244808a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3f9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_dir = f\"s3://savemodels/network_8fold/restnet-model{index}.pt\"\n",
    "model_dir = f\"./network_8fold/restnet-model{index}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad022822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model on CPU: laptop\n",
    "device = torch.device('cpu')\n",
    "#model = TheModelClass(*args, **kwargs)\n",
    "model = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "#model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "model.load_state_dict(torch.load(model_dir, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5da93a9-ac6f-4f8c-bafc-6309b3697f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the SSIM score for every image after a feedforward propagation through \n",
    "#the network.\n",
    "#Subtract the image SSIM score before the feedforward prop to obtain the net improvement for every image.\n",
    "#Print the average improvement and the average SSIM score after the reconstruction.\n",
    "SSIM_improvement = []\n",
    "SSIM_score = []\n",
    "MSE_improvement = []\n",
    "MSE_score = []\n",
    "MIE_improvement = []\n",
    "MIE_score = []\n",
    "for i in range(0,len(train_dataset)):\n",
    "    gt, image = train_dataset[i]\n",
    "    #image = image.unsqueeze(0).to('cuda:0')\n",
    "    image = image.unsqueeze(0)\n",
    "    image = image.unsqueeze(0)\n",
    "    gt = gt.unsqueeze(0).numpy()\n",
    "    output = model(image)\n",
    "  #  output = output.squeeze(1).cpu().detach().numpy()\n",
    "    output = output.squeeze(1).detach().numpy()\n",
    "    image = image.squeeze(1).numpy()\n",
    "    gt =  np.squeeze(gt)\n",
    "    output =  np.squeeze(output)\n",
    "    image =  np.squeeze(image)\n",
    "\n",
    "\n",
    "    output_loss1 = torch.tensor(ssim(gt, output))\n",
    "    output_loss2 = torch.tensor(mse(gt, output))\n",
    "    output_loss3 = np.mean(np.abs(gt - output))\n",
    "  #  image_loss = torch.tensor(ssim(gt, image.squeeze(1).cpu().numpy()))\n",
    "    image_loss1 = torch.tensor(ssim(gt, image))\n",
    "    image_loss2 = torch.tensor(mse(gt, image))\n",
    "    image_loss3 = np.mean(np.abs(gt - image))\n",
    "    SSIM_improvement.append(output_loss1.item()-image_loss1.item())\n",
    "    SSIM_score.append(output_loss1.item())\n",
    "    MSE_improvement.append(output_loss2.item()-image_loss2.item())\n",
    "    MSE_score.append(output_loss2.item())\n",
    "    MIE_improvement.append(output_loss3 -image_loss3)\n",
    "    MIE_score.append(output_loss3)\n",
    "\n",
    "print(np.nanmean(SSIM_improvement))\n",
    "print(np.nanmean(SSIM_score))\n",
    "print(np.nanmean(MSE_improvement))\n",
    "print(np.nanmean(MSE_score))\n",
    "print(np.nanmean(MIE_improvement))\n",
    "print(np.nanmean(MIE_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1563191d-9049-477d-bc60-e1ae5a9bd29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_improvement.sort()\n",
    "plt.plot(SSIM_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ad74e5-06c1-44c4-b530-e1c24264b865",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE_improvement.sort()\n",
    "plt.plot(MSE_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608bfd64-4ab5-4acc-83c0-5f3c2a0df031",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIE_improvement.sort()\n",
    "plt.plot(MSE_improvement)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "214e7b82-ec39-4884-b875-c23f067c170d",
   "metadata": {},
   "source": [
    "https://www.cns.nyu.edu/~lcv/ssim/\n",
    "\n",
    "-0.08275149106735159\n",
    "0.4152959283673115\n",
    "-0.0008673634965751041\n",
    "0.0009822993672129392\n",
    "-2.285560072865337\n",
    "inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d3b135-723d-435d-bb9c-ae79c2321953",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec45d6-77fd-4e9b-bce1-6e1509314857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f1e7bc-59cb-4f2c-ad59-98f414e8154c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f85e662-842f-4cc4-8843-1a15346d481b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b75fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240b151f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"data/test/images/244.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff493574",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_frame = Image.open(file_dir)\n",
    "   \n",
    "img_fft, img_fft_noise, img_noise = noise_and_kspace(im_frame)\n",
    "\n",
    "preprocess = T.Compose([\n",
    "                       # T.Grayscale(num_output_channels=1),\n",
    "                           T.Resize(128),    #128 as maximum\n",
    "                           T.CenterCrop(128),\n",
    "                           T.ToTensor() #,\n",
    "                           #T.Normalize(\n",
    "                            #        mean=[0.485, 0.456, 0.406],\n",
    "                               #        std=[0.229, 0.224, 0.225]\n",
    "                             ##         )\n",
    "                            ])\n",
    "img_gt = preprocess(Image.fromarray(np.uint8(im_fft)).convert('L'))\n",
    "img_und = preprocess(Image.fromarray(np.uint8(img_fft_noise)).convert('L'))\n",
    "    \n",
    "n1 = (img_und**2).sum(dim=-1).sqrt()\n",
    "norm = n1.max() \n",
    "if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "img_gt, img_und = img_gt/norm , img_und/norm  \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ae03d59-a632-4194-971b-b17417c24dee",
   "metadata": {},
   "source": [
    "torchvision.transforms.ConvertImageDtype\n",
    "torchvision.transforms.ToPILImage\n",
    "torchvision.transforms.PILToTensor"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c636e0f-46e8-4054-b137-de326985c275",
   "metadata": {},
   "source": [
    "im_1 = img_gt * norm\n",
    "#im_1 = im_1.numpy()\n",
    "im_1 = T.ToPILImage()(im_1)\n",
    "im_1.size"
   ]
  },
  {
   "cell_type": "raw",
   "id": "62042616-a908-441a-b0a7-d56a73416501",
   "metadata": {},
   "source": [
    "display(im_frame)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "58edadac-1acb-4d05-b102-cb4dfcc9c7c3",
   "metadata": {},
   "source": [
    "display(im_1) # success in turning Image into right input"
   ]
  },
  {
   "cell_type": "raw",
   "id": "21476dc7-a09b-4997-842d-7fe3ed9228e6",
   "metadata": {},
   "source": [
    "np_image1 = np.reshape(im_1, (64, 64))# image noise numpy array\n",
    "im_r1 = Image.fromarray(np_image1).convert('RGB')\n",
    "display(im_r1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "106789d1-862f-4ee8-925a-c51452b936f4",
   "metadata": {},
   "source": [
    "im_frame = Image.open(file_dir)\n",
    "   \n",
    "noise_im_frame = noise_and_kspace(im_frame)\n",
    "\n",
    "preprocess = T.Compose([\n",
    "                       # T.Grayscale(num_output_channels=1),\n",
    "                           T.Resize(64),    #128 as maximum\n",
    "                           T.CenterCrop(64),\n",
    "                           T.ToTensor() #,\n",
    "                           #T.Normalize(\n",
    "                            #        mean=[0.485, 0.456, 0.406],\n",
    "                               #        std=[0.229, 0.224, 0.225]\n",
    "                             ##         )\n",
    "                            ])\n",
    "img_gt = preprocess(Image.fromarray(np.uint8(im_frame)).convert('L'))\n",
    "im_1 = img_gt.numpy()\n",
    "im_1.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "885c0d8b-9500-41c6-bf8f-3206eabfd132",
   "metadata": {},
   "source": [
    "np_image1 = np.reshape(im_1, (64, 64))# image noise numpy array\n",
    "im_r1 = Image.fromarray(np_image1).convert('L')\n",
    "display(im_r1)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ed528d5-9046-4abd-b107-0436cdd6b576",
   "metadata": {},
   "source": [
    "image = Image.open(file_dir)    \n",
    "img_fft = fftshift(fftn(image))\n",
    "np_image = np.uint8(img_fft)# image noise numpy array\n",
    "im_n = Image.fromarray(np_image).convert('L')\n",
    "display(im_n)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f705ae9e",
   "metadata": {},
   "source": [
    "https://www.kite.com/python/examples/4887/PIL-convert-between-a-pil-%60image%60-and-a-numpy-%60array%60\n",
    "\n",
    "https://stackoverflow.com/questions/2659312/how-do-i-convert-a-numpy-array-to-and-display-an-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73094677-ada9-48d4-94dd-70bb1bb0bbf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "31688b6f-03c3-4cd3-8754-0ec5095a07f2",
   "metadata": {},
   "source": [
    "from IPython.display import display\n",
    "display(im_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9a3f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as cmp_ssim \n",
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return cmp_ssim(\n",
    "        gt, pred, multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bafb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #noise_image = noise_image.to('cuda:0')\n",
    "#img_gt = img_gt.numpy()\n",
    "img_und = img_und.unsqueeze(0)\n",
    "output = model(img_und)\n",
    "   # output = output.squeeze(1).cpu().detach().numpy()\n",
    "output = output.squeeze(1).detach() #.numpy()   #image under numpy form\n",
    "#output_loss = torch.tensor(ssim(img_gt, output))  \n",
    "#image_loss = torch.tensor(ssim(img_gt, img_noise.squeeze(1).numpy()))\n",
    "#SSIM_improvement = (output_loss.item()-image_loss.item())\n",
    "#SSIM_score = output_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652b07c7-95ac-47bb-a8f7-bbc59d10d02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0404e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_rescontruct_image =  output # np.reshape(output, (64, 64))# image noise numpy array\n",
    "im_reconstruct = T.ToPILImage()(np_rescontruct_image)#Image.fromarray(np_rescontruct_image).convert('L')\n",
    "inputa = np.asarray(im_reconstruct)\n",
    "final = ifftn(ifftshift(inputa))\n",
    "I = PIL.Image.fromarray(numpy.uint8(final))\n",
    "I.save(\"testing/test.png\") #for prediction values\n",
    "I.save(\"pred1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66ce9e-8dcc-47b5-9d26-8461dc6cefba",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(im_reconstruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22263aa-0947-4689-b648-8edb9e9560f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(im_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04cccab-ad36-4d86-817f-6cbaa7114177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56389949-3a53-47b2-91d1-3ecef15eddd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
