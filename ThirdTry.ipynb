{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3337e4dd-fead-4890-a0bc-e7211d51000b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Importing all the relevant library\n",
    "%matplotlib inline\n",
    "import h5py, os\n",
    "#from functions import transforms as T\n",
    "#from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "#from functions import transforms as T \n",
    "#from functions.subsample import MaskFunc\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "293da5c7-ec2e-4583-835f-f1238a9dcb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class MaskFunc:\n",
    "    \"\"\"\n",
    "    MaskFunc creates a sub-sampling mask of a given shape.\n",
    "\n",
    "    The mask selects a subset of columns from the input k-space data. If the k-space data has N\n",
    "    columns, the mask picks out:\n",
    "        1. N_low_freqs = (N * center_fraction) columns in the center corresponding to\n",
    "           low-frequencies\n",
    "        2. The other columns are selected uniformly at random with a probability equal to:\n",
    "           prob = (N / acceleration - N_low_freqs) / (N - N_low_freqs).\n",
    "    This ensures that the expected number of columns selected is equal to (N / acceleration)\n",
    "\n",
    "    It is possible to use multiple center_fractions and accelerations, in which case one possible\n",
    "    (center_fraction, acceleration) is chosen uniformly at random each time the MaskFunc object is\n",
    "    called.\n",
    "\n",
    "    For example, if accelerations = [4, 8] and center_fractions = [0.08, 0.04], then there\n",
    "    is a 50% probability that 4-fold acceleration with 8% center fraction is selected and a 50%\n",
    "    probability that 8-fold acceleration with 4% center fraction is selected.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, center_fractions, accelerations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            center_fractions (List[float]): Fraction of low-frequency columns to be retained.\n",
    "                If multiple values are provided, then one of these numbers is chosen uniformly\n",
    "                each time.\n",
    "\n",
    "            accelerations (List[int]): Amount of under-sampling. This should have the same length\n",
    "                as center_fractions. If multiple values are provided, then one of these is chosen\n",
    "                uniformly each time. An acceleration of 4 retains 25% of the columns, but they may\n",
    "                not be spaced evenly.\n",
    "        \"\"\"\n",
    "        if len(center_fractions) != len(accelerations):\n",
    "            raise ValueError('Number of center fractions should match number of accelerations')\n",
    "\n",
    "        self.center_fractions = center_fractions\n",
    "        self.accelerations = accelerations\n",
    "        self.rng = np.random.RandomState()\n",
    "\n",
    "    def __call__(self, shape, seed=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            shape (iterable[int]): The shape of the mask to be created. The shape should have\n",
    "                at least 3 dimensions. Samples are drawn along the second last dimension.\n",
    "            seed (int, optional): Seed for the random number generator. Setting the seed\n",
    "                ensures the same mask is generated each time for the same shape.\n",
    "        Returns:\n",
    "            torch.Tensor: A mask of the specified shape.\n",
    "        \"\"\"\n",
    "        if len(shape) < 3:\n",
    "            raise ValueError('Shape should have 3 or more dimensions')\n",
    "\n",
    "        self.rng.seed(seed)\n",
    "        num_cols = shape[-2]\n",
    "\n",
    "        choice = self.rng.randint(0, len(self.accelerations))\n",
    "        center_fraction = self.center_fractions[choice]\n",
    "        acceleration = self.accelerations[choice]\n",
    "\n",
    "        # Create the mask\n",
    "        num_low_freqs = int(round(num_cols * center_fraction))\n",
    "        prob = (num_cols / acceleration - num_low_freqs) / (num_cols - num_low_freqs)\n",
    "        mask = self.rng.uniform(size=num_cols) < prob\n",
    "        pad = (num_cols - num_low_freqs + 1) // 2\n",
    "        mask[pad:pad + num_low_freqs] = True\n",
    "\n",
    "        # Reshape the mask\n",
    "        mask_shape = [1 for _ in shape]\n",
    "        mask_shape[-2] = num_cols\n",
    "        mask = torch.from_numpy(mask.reshape(*mask_shape).astype(np.float32))\n",
    "\n",
    "        return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98323898-90a8-4af7-b05c-67c2f5f43d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def tensor_to_complex_np(data):\n",
    "    \"\"\"\n",
    "    Converts a complex torch tensor to numpy array.\n",
    "    Args:\n",
    "        data (torch.Tensor): Input data to be converted to numpy.\n",
    "    Returns:\n",
    "        np.array: Complex numpy version of data\n",
    "    \"\"\"\n",
    "    data = data.numpy()\n",
    "    return data[..., 0] + 1j * data[..., 1]\n",
    "\n",
    "\n",
    "def to_tensor(data):\n",
    "    \"\"\"\n",
    "    Convert numpy array to PyTorch tensor. For complex arrays, the real and imaginary parts\n",
    "    are stacked along the last dimension.\n",
    "\n",
    "    Args:\n",
    "        data (np.array): Input numpy array\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: PyTorch version of data\n",
    "    \"\"\"\n",
    "    if np.iscomplexobj(data):\n",
    "        data = np.stack((data.real, data.imag), axis=-1)\n",
    "    return torch.from_numpy(data)\n",
    "\n",
    "\n",
    "def apply_mask(data, mask_func, seed=None):\n",
    "    \"\"\"\n",
    "    Subsample given k-space by multiplying with a mask.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input k-space data. This should have at least 3 dimensions, where\n",
    "            dimensions -3 and -2 are the spatial dimensions, and the final dimension has size\n",
    "            2 (for complex values).\n",
    "        mask_func (callable): A function that takes a shape (tuple of ints) and a random\n",
    "            number seed and returns a mask.\n",
    "        seed (int or 1-d array_like, optional): Seed for the random number generator.\n",
    "\n",
    "    Returns:\n",
    "        (tuple): tuple containing:\n",
    "            masked data (torch.Tensor): Subsampled k-space data\n",
    "            mask (torch.Tensor): The generated mask\n",
    "    \"\"\"\n",
    "    shape = np.array(data.shape)\n",
    "    shape[:-3] = 1\n",
    "    mask = mask_func(shape, seed)\n",
    "    return torch.where(mask == 0, torch.Tensor([0]), data), mask\n",
    "\n",
    "\n",
    "def fft2(data):\n",
    "    \"\"\"\n",
    "    Apply centered 2 dimensional Fast Fourier Transform.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): Complex valued input data containing at least 3 dimensions: dimensions\n",
    "            -3 & -2 are spatial dimensions and dimension -1 has size 2. All other dimensions are\n",
    "            assumed to be batch dimensions.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The FFT of the input.\n",
    "    \"\"\"\n",
    "   # assert data.size(-1) == 2\n",
    "    data = ifftshift(data, dim=(-3, -2))\n",
    "    data = torch.fft(data, 2, normalized=True)\n",
    "    data = fftshift(data, dim=(-3, -2))\n",
    "    return data\n",
    "\n",
    "\n",
    "def ifft2(data):\n",
    "    \"\"\"\n",
    "    Apply centered 2-dimensional Inverse Fast Fourier Transform.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): Complex valued input data containing at least 3 dimensions: dimensions\n",
    "            -3 & -2 are spatial dimensions and dimension -1 has size 2. All other dimensions are\n",
    "            assumed to be batch dimensions.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The IFFT of the input.\n",
    "    \"\"\"\n",
    "    #assert data.size(-1) == 2\n",
    "    data = ifftshift(data, dim=(-3, -2))\n",
    "    data = torch.ifft(data, 2, normalized=True)\n",
    "    data = fftshift(data, dim=(-3, -2))\n",
    "    return data\n",
    "\n",
    "\n",
    "def complex_abs(data):\n",
    "    \"\"\"\n",
    "    Compute the absolute value of a complex valued input tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): A complex valued tensor, where the size of the final dimension\n",
    "            should be 2.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Absolute value of data\n",
    "    \"\"\"\n",
    "   # assert data.size(-1) == 2\n",
    "    return (data ** 2).sum(dim=-1).sqrt()\n",
    "\n",
    "\n",
    "def root_sum_of_squares(data, dim=0):\n",
    "    \"\"\"\n",
    "    Compute the Root Sum of Squares (RSS) transform along a given dimension of a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor\n",
    "        dim (int): The dimensions along which to apply the RSS transform\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The RSS value\n",
    "    \"\"\"\n",
    "    return torch.sqrt((data ** 2).sum(dim))\n",
    "\n",
    "\n",
    "def center_crop(data, shape):\n",
    "    \"\"\"\n",
    "    Apply a center crop to the input real image or batch of real images.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor to be center cropped. It should have at\n",
    "            least 2 dimensions and the cropping is applied along the last two dimensions.\n",
    "        shape (int, int): The output shape. The shape should be smaller than the\n",
    "            corresponding dimensions of data.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The center cropped image\n",
    "    \"\"\"\n",
    "    assert 0 < shape[0] <= data.shape[-2]\n",
    "    assert 0 < shape[1] <= data.shape[-1]\n",
    "    w_from = (data.shape[-2] - shape[0]) // 2\n",
    "    h_from = (data.shape[-1] - shape[1]) // 2\n",
    "    w_to = w_from + shape[0]\n",
    "    h_to = h_from + shape[1]\n",
    "    return data[..., w_from:w_to, h_from:h_to]\n",
    "\n",
    "\n",
    "def complex_center_crop(data, shape):\n",
    "    \"\"\"\n",
    "    Apply a center crop to the input image or batch of complex images.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The complex input tensor to be center cropped. It should\n",
    "            have at least 3 dimensions and the cropping is applied along dimensions\n",
    "            -3 and -2 and the last dimensions should have a size of 2.\n",
    "        shape (int, int): The output shape. The shape should be smaller than the\n",
    "            corresponding dimensions of data.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The center cropped image\n",
    "    \"\"\"\n",
    "    assert 0 < shape[0] <= data.shape[-3]\n",
    "    assert 0 < shape[1] <= data.shape[-2]\n",
    "    w_from = (data.shape[-3] - shape[0]) // 2\n",
    "    h_from = (data.shape[-2] - shape[1]) // 2\n",
    "    w_to = w_from + shape[0]\n",
    "    h_to = h_from + shape[1]\n",
    "    return data[..., w_from:w_to, h_from:h_to, :]\n",
    "\n",
    "\n",
    "def normalize(data, mean, stddev, eps=0.):\n",
    "    \"\"\"\n",
    "    Normalize the given tensor using:\n",
    "        (data - mean) / (stddev + eps)\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): Input data to be normalized\n",
    "        mean (float): Mean value\n",
    "        stddev (float): Standard deviation\n",
    "        eps (float): Added to stddev to prevent dividing by zero\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized tensor\n",
    "    \"\"\"\n",
    "    return (data - mean) / (stddev + eps)\n",
    "\n",
    "\n",
    "def normalize_instance(data, eps=0.):\n",
    "    \"\"\"\n",
    "        Normalize the given tensor using:\n",
    "            (data - mean) / (stddev + eps)\n",
    "        where mean and stddev are computed from the data itself.\n",
    "\n",
    "        Args:\n",
    "            data (torch.Tensor): Input data to be normalized\n",
    "            eps (float): Added to stddev to prevent dividing by zero\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Normalized tensor\n",
    "        \"\"\"\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    return normalize(data, mean, std, eps), mean, std\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "def roll(x, shift, dim):\n",
    "    \"\"\"\n",
    "    Similar to np.roll but applies to PyTorch Tensors\n",
    "    \"\"\"\n",
    "    if isinstance(shift, (tuple, list)):\n",
    "        assert len(shift) == len(dim)\n",
    "        for s, d in zip(shift, dim):\n",
    "            x = roll(x, s, d)\n",
    "        return x\n",
    "\n",
    "    shift = shift % x.size(dim)\n",
    "    if shift == 0:\n",
    "        return x\n",
    "    left = x.narrow(dim, 0, x.size(dim) - shift)\n",
    "    right = x.narrow(dim, x.size(dim) - shift, shift)\n",
    "    return torch.cat((right, left), dim=dim)\n",
    "\n",
    "\n",
    "def fftshift(x, dim=None):\n",
    "    \"\"\"\n",
    "    Similar to np.fft.fftshift but applies to PyTorch Tensors\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        dim = tuple(range(x.dim()))\n",
    "        shift = [dim // 2 for dim in x.shape]\n",
    "    elif isinstance(dim, int):\n",
    "        shift = x.shape[dim] // 2\n",
    "    else:\n",
    "  #      flag = 0\n",
    " #       for i in dim:\n",
    "  #          if i < 0:\n",
    "  #              flag = 1\n",
    "  #      if flag == 1:\n",
    "   #         dim = tuple(range(x.dim()))\n",
    "   #         shift = [(dim + 1) // 2 for dim in x.shape]\n",
    "   #     else:\n",
    "   #         shift = [x.shape[i] // 2 for i in dim]\n",
    "        shift = [x.shape[i] // 2 for i in dim]\n",
    "    return roll(x, shift, dim)\n",
    "\n",
    "\n",
    "def ifftshift(x, dim=None):\n",
    "    \"\"\"\n",
    "    Similar to np.fft.ifftshift but applies to PyTorch Tensors\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        dim = tuple(range(x.dim()))\n",
    "        shift = [(dim + 1) // 2 for dim in x.shape]\n",
    "    elif isinstance(dim, int):\n",
    "        shift = (x.shape[dim] + 1) // 2\n",
    "    else:\n",
    "\n",
    "        shift = [x.shape[i] // 2 for i in dim]\n",
    "    return roll(x, shift, dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "425d445b-6022-4293-ba80-8134d4e853b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.fft import fftshift, ifftshift, fftn, ifftn\n",
    "\n",
    "def transform_kspace_to_image(k, dim=None, img_shape=None):\n",
    "    \"\"\" Computes the Fourier transform from k-space to image space\n",
    "    along a given or all dimensions\n",
    "    :param k: k-space data\n",
    "    :param dim: vector of dimensions to transform\n",
    "    :param img_shape: desired shape of output image\n",
    "    :returns: data in image space (along transformed dimensions)\n",
    "    \"\"\"\n",
    "    if not dim:\n",
    "        dim = range(k.ndim)\n",
    "\n",
    "    img = fftshift(ifftn(ifftshift(k, axes=dim), s=img_shape, axes=dim), axes=dim)\n",
    "    #img = fftshift(ifft2(ifftshift(k, dim=dim)), dim=dim)\n",
    "    img *= np.sqrt(np.prod(np.take(img.shape, dim)))\n",
    "    return img\n",
    "\n",
    "\n",
    "def transform_image_to_kspace(img, dim=None, k_shape=None):\n",
    "    \"\"\" Computes the Fourier transform from image space to k-space space\n",
    "    along a given or all dimensions\n",
    "    :param img: image space data\n",
    "    :param dim: vector of dimensions to transform\n",
    "    :param k_shape: desired shape of output k-space data\n",
    "    :returns: data in k-space (along transformed dimensions)\n",
    "    \"\"\"\n",
    "    if not dim:\n",
    "        dim = range(img.ndim)\n",
    "\n",
    "    k = fftshift(fftn(ifftshift(img, axes=dim), s=k_shape, axes=dim), axes=dim)\n",
    "    #k = fftshift(fft2(ifftshift(img, dim=dim)), dim=dim)\n",
    "    k /= np.sqrt(np.prod(np.take(img.shape, dim)))\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8334eeb1-29a4-4988-9772-7097969230cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "02712835-6085-4bb5-aa3d-c96e4761c74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80a6ccbe-17ff-48d2-a7b8-9cf6bd165a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=True):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_name, slice = subject_id  \n",
    "    \n",
    "#    with h5py.File(rawdata_name, 'r') as data:\n",
    "#        rawdata = data['kspace'][slice]\n",
    "   \n",
    "    im_frame = Image.open(rawdata_name)\n",
    "    np_frame = np.array(im_frame.getdata())\n",
    "   \n",
    "    k_frame = transform_image_to_kspace(np_frame)\n",
    "    \n",
    "    slice_kspace = to_tensor(k_frame).unsqueeze(0)\n",
    "    S, Ny, Nx, ps = slice_kspace.shape\n",
    "\n",
    "    # apply random mask\n",
    "    shape = np.array(slice_kspace.shape)\n",
    "    mask_func = MaskFunc(center_fractions=[center_fract], accelerations=[acc])\n",
    "    seed = None if not use_seed else tuple(map(ord, fname))\n",
    "    mask = mask_func(shape, seed)\n",
    "      \n",
    "    # undersample\n",
    "    masked_kspace = torch.where(mask == 0, torch.Tensor([0]), slice_kspace)\n",
    "    masks = mask.repeat(S, Ny, 1, ps)\n",
    "\n",
    "    img_gt, img_und = ifft2(slice_kspace), ifft2(masked_kspace)\n",
    "    \n",
    "    \n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = complex_abs(img_und).max()\n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "    img_gt, img_und, rawdata_und = img_gt/norm, img_und/norm, masked_kspace/norm\n",
    "      \n",
    "\n",
    "    img_gt = center_crop(complex_abs(img_gt), [320, 320])\n",
    "    img_und = center_crop(complex_abs(img_und), [320, 320])\n",
    "        \n",
    "    return img_gt.squeeze(0), img_und.squeeze(0), tr, te, alfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "60fe5d5d-3891-4fef-bb67-75b47b11f203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path, val_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)):\n",
    "\n",
    "        data_list[train_and_val[i]] = []\n",
    "        \n",
    "        which_data_path = data_path[i]\n",
    "        tr = 0\n",
    "        te = 0\n",
    "        alfa = 0\n",
    "    \n",
    "        for fname in sorted(os.listdir(which_data_path + '/images')):\n",
    "            if fname == '.DS_Store': continue\n",
    "            \n",
    "            subject_data_path = os.path.join(which_data_path + '/images', fname)\n",
    "                     \n",
    "            if not os.path.isfile(subject_data_path): continue \n",
    "            \n",
    "            im_frame = Image.open(subject_data_path)\n",
    "            np_frame = np.array(im_frame.getdata())\n",
    "\n",
    "            k_frame = transform_image_to_kspace(np_frame)\n",
    "            #result = to_tensor(k_frame).unsqueeze(0)\n",
    "            num_slice = k_frame.shape[0]\n",
    "            \n",
    "            #get information from text file\n",
    "            # this will return a tuple of root and extension\n",
    "            split_tup = os.path.splitext(fname)\n",
    "\n",
    "  \n",
    "            # extract the file name and extension\n",
    "            file_name = split_tup[0]\n",
    "  \n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "            data_list[train_and_val[i]] += [(fname, subject_data_path, slice) for slice in range(2, num_slice)]\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0334573f-a2c5-4d56-82f1-511da5275b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-8.0000e-03,  2.2737e-16],\n",
      "         [ 7.3471e-03,  5.0751e-03],\n",
      "         [ 1.9739e-02,  2.4111e-02],\n",
      "         ...,\n",
      "         [ 1.1788e-02, -9.3709e-03],\n",
      "         [ 1.9739e-02, -2.4111e-02],\n",
      "         [ 7.3471e-03, -5.0751e-03]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from scipy import misc\n",
    "import glob\n",
    "from PIL import Image\n",
    "import tensorflow as tf\n",
    "ima = 1\n",
    "for image_path in glob.glob(\"matlab/images/*.png\"):\n",
    "    im_frame = Image.open(image_path)\n",
    "    np_frame = np.array(im_frame.getdata())\n",
    "    #tf_frame = tf.convert_to_tensor(np_frame)\n",
    "    #ts_frame = to_tensor(np_frame).unsqueeze(0)\n",
    "    k_frame = transform_image_to_kspace(np_frame)\n",
    "    result = to_tensor(k_frame).unsqueeze(0)\n",
    "    if ima == 1:\n",
    "        print(result)\n",
    "        ima = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b50e30b-26a0-45d1-a10f-e245e7b45285",
   "metadata": {},
   "source": [
    "# The initial model, based on the AlexNet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8d9f2a-20b8-42bd-9364-e902a24e4884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1), #320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64 ,64, kernel_size=3, padding=1),  # 320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # 320/320\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, padding=1),  # 320/320\n",
    "            \n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94846f9-cc59-41d5-bdcb-0c4d436be193",
   "metadata": {},
   "source": [
    "# RestNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55558de4-143a-484d-bfc1-8f2e8e66fbfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fc197fc-34f5-4dc2-bab7-ffbe4aa15d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseBlock(torch.nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self,input_planes,planes,stride=1,dim_change=None):\n",
    "        super(baseBlock,self).__init__()\n",
    "        #declare convolutional layers with batch norms\n",
    "        self.conv1 = torch.nn.Conv2d(input_planes,planes,stride=stride,kernel_size=3,padding=1)\n",
    "        self.bn1   = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv2 = torch.nn.Conv2d(planes,planes,stride=1,kernel_size=3,padding=1)\n",
    "        self.bn2   = torch.nn.BatchNorm2d(planes)\n",
    "        self.dim_change = dim_change\n",
    "    def forward(self,x):\n",
    "        #Save the residue\n",
    "        res = x\n",
    "        output = F.relu(self.bn1(self.conv1(x)))\n",
    "        output = self.bn2(self.conv2(output))\n",
    "        if self.dim_change is not None:\n",
    "            res = self.dim_change(res)\n",
    "        \n",
    "        output += res\n",
    "        output = F.relu(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self,block,num_layers,classes=10):\n",
    "        super(ResNet,self).__init__()\n",
    "        #according to research paper:\n",
    "        self.input_planes = 64\n",
    "        self.conv1 = torch.nn.Conv2d(1,64,kernel_size=3,stride=1,padding=1)\n",
    "        self.layer1 = self._layer(block,64,num_layers[0],stride=1)\n",
    "        self.layer2 = self._layer(block,64,num_layers[1],stride=1)\n",
    "        self.layer3 = self._layer(block,64,num_layers[2],stride=1)\n",
    "        self.layer4 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer5 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer6 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer7 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer8 = self._layer(block,64,num_layers[2],stride=1)\n",
    "        self.layer9 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer10 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer11 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer12 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer13 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer14 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer15 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer16 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer17 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer18 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.conv2 = torch.nn.Conv2d(64,1,kernel_size=3,stride=1, padding=1)\n",
    "        \n",
    "    \n",
    "    def _layer(self,block,planes,num_layers,stride=1):\n",
    "        dim_change = None\n",
    "        if stride!=1 or planes != self.input_planes*block.expansion:\n",
    "            dim_change = torch.nn.Sequential(torch.nn.Conv2d(self.input_planes,planes*block.expansion,kernel_size=1,stride=stride),\n",
    "                                             torch.nn.BatchNorm2d(planes*block.expansion))\n",
    "        netLayers =[]\n",
    "        netLayers.append(block(self.input_planes,planes,stride=stride,dim_change=dim_change))\n",
    "        self.input_planes = planes * block.expansion\n",
    "        for i in range(1,num_layers):\n",
    "            netLayers.append(block(self.input_planes,planes))\n",
    "            self.input_planes = planes * block.expansion\n",
    "        \n",
    "        return torch.nn.Sequential(*netLayers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.layer8(x)\n",
    "        x = self.layer9(x)\n",
    "        x = self.layer10(x)\n",
    "        x = self.layer11(x)\n",
    "        x = self.layer12(x)\n",
    "        x = self.layer13(x)\n",
    "        x = self.layer14(x)\n",
    "        x = self.layer15(x)\n",
    "        x = self.layer16(x)\n",
    "        x = self.layer17(x)\n",
    "        x = self.layer18(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0767891c-ed21-45cc-a9fc-12c0ce5f2aa8",
   "metadata": {},
   "source": [
    "Changed in version 0.16: This function was renamed from skimage.measure.compare_ssim to skimage.metrics.structural_similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f0751991-d52a-42d9-a531-eb50db98c3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as cmp_ssim \n",
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return cmp_ssim(\n",
    "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89ad4291-3580-4649-8a61-9ea2859c9202",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yr/qwqgmggj2_12p38pnv1hf6380000gp/T/ipykernel_3079/1202113998.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_path_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'matlab/images'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata_path_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'matlab/images'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# first load all file names, paths and slices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/yr/qwqgmggj2_12p38pnv1hf6380000gp/T/ipykernel_3079/2195987256.py\u001b[0m in \u001b[0;36mload_data_path\u001b[0;34m(train_data_path, val_data_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mnum_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kspace'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    data_path_train = 'matlab/images'\n",
    "    data_path_val = 'matlab/images'\n",
    "    data_list = load_data_path(data_path_train, data_path_val) # first load all file names, paths and slices.\n",
    "    \n",
    "    acc = 4\n",
    "    cen_fract = 0.08\n",
    "    seed = False # random masks for each slice \n",
    "    num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "    lr = 1e-3\n",
    "    \n",
    "    network_4fold = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "    network_4fold.to('cuda:0') #move the model on the GPU\n",
    "    mae_loss = nn.L1Loss().to('cuda:0')\n",
    "    \n",
    "    optimizer = optim.Adam(network_4fold.parameters(), lr=lr)\n",
    "    fixed_train_set = []\n",
    "    for val in data_list['train']:\n",
    "        if val[2] > 5:\n",
    "            fixed_train_set.append(val)\n",
    "    #create data loader for training set. It applies same to validation set as well\n",
    "    train_dataset = MRIDataset(fixed_train_set, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "    \n",
    "    losses=[]\n",
    "    mean_loss_list = []\n",
    "    img_nr = 0\n",
    "    for epoch in range(1):\n",
    "        for iteration, sample in enumerate(train_loader):\n",
    "            img_nr += 1\n",
    "            img_gt, img_und = sample\n",
    "        \n",
    "            img_gt = img_gt.unsqueeze(1).to('cuda:0')\n",
    "            img_und = img_und.unsqueeze(1).to('cuda:0')\n",
    "\n",
    "            output = network_4fold(img_und)       #feedforward\n",
    "\n",
    "\n",
    "            loss = mae_loss(output, img_gt)\n",
    "\n",
    "            optimizer.zero_grad()       #set current gradients to 0\n",
    "            loss.backward()      #backpropagate\n",
    "            optimizer.step()     #update the weights\n",
    "            mean_loss_list.append(loss.item())\n",
    "            #compute and print the mean L1 lossscore for the last 20 training images.\n",
    "            if img_nr%20 == 0:\n",
    "                print(\"L1 Loss score: \", np.round(np.mean(mean_loss_list), decimals = 5), \"  Image number: \", img_nr, \"  Epoch: \", epoch+1)\n",
    "                mean_loss_list = []\n",
    "            losses.append(loss.item() * img_gt.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6f95bd-0ec9-487c-87cc-bb68aa2a2190",
   "metadata": {},
   "outputs": [],
   "source": [
    "#L1 loss plot ofr the 4-fold data\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde6e848-c44b-42e8-aced-f9bc29cd4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_improvement = []\n",
    "SSIM_score = []\n",
    "for i in range(0,len(train_dataset)):\n",
    "    gt, image = train_dataset[i]\n",
    "    image = image.unsqueeze(0).to('cuda:0')\n",
    "    image = image.unsqueeze(0)\n",
    "    gt = gt.unsqueeze(0).numpy()\n",
    "    output = network_4fold(image)\n",
    "    output = output.squeeze(1).cpu().detach().numpy()\n",
    "    output_loss = torch.tensor(ssim(gt, output))\n",
    "    image_loss = torch.tensor(ssim(gt, image.squeeze(1).cpu().numpy()))\n",
    "    SSIM_improvement.append(output_loss.item()-image_loss.item())\n",
    "    SSIM_score.append(output_loss.item())\n",
    "\n",
    "print(np.nanmean(SSIM_improvement))\n",
    "print(np.nanmean(SSIM_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79d1021-4e2d-42ce-b860-71fa2d192caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_improvement.sort()\n",
    "plt.plot(SSIM_improvement)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a96ab13-1e65-4a81-bc6f-af57a45583db",
   "metadata": {},
   "source": [
    "All the code from here on is almost the same as before, only for the 8fold data. The main difference is the learning rate, which is 3 times higher for the 8fold data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7db52b0-1c78-451a-84b0-b3520daf6985",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yr/qwqgmggj2_12p38pnv1hf6380000gp/T/ipykernel_2676/2649149905.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mnetwork_8fold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseBlock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mnetwork_8fold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda:0'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#move the model on the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    221\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m             raise RuntimeError(\n\u001b[1;32m    195\u001b[0m                 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0m_cudart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_cudart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_check_driver\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_check_driver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_cuda_isDriverSufficient'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Torch not compiled with CUDA enabled\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_isDriverSufficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDriverVersion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    data_path_train = 'data/train'\n",
    "    data_path_val = 'data/val'\n",
    "    data_list = load_data_path(data_path_train, data_path_val)\n",
    "    \n",
    "    acc = 8\n",
    "    cen_fract = 0.04\n",
    "    seed = False # random masks for each slice \n",
    "    num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "    mae_loss = nn.L1Loss().to('cuda:0')\n",
    "    lr = 3e-3\n",
    "    \n",
    "    network_8fold = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "    network_8fold.to('cuda:0') #move the model on the GPU\n",
    "\n",
    "    \n",
    "    optimizer2 = optim.Adam(network_8fold.parameters(), lr=lr)\n",
    "    fixed_train_set = []\n",
    "    for val in data_list['train']:\n",
    "        if val[2] > 5:\n",
    "            fixed_train_set.append(val)\n",
    "    #create data loader for training set. It applies same to validation set as well\n",
    "    train_dataset = MRIDataset(fixed_train_set, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "    \n",
    "    losses2=[]\n",
    "    mean_loss_list = []\n",
    "    img_nr = 0\n",
    "    for epoch in range(3):\n",
    "        for iteration, sample in enumerate(train_loader):\n",
    "            img_nr += 1\n",
    "            img_gt, img_und, tr, te, alfa = sample\n",
    "        \n",
    "            img_gt = img_gt.unsqueeze(1).to('cuda:0')\n",
    "            img_und = img_und.unsqueeze(1).to('cuda:0')\n",
    "\n",
    "            output = network_8fold(img_und)      #feedforward\n",
    "\n",
    "            loss = mae_loss(output, img_gt)\n",
    "\n",
    "            optimizer2.zero_grad()       #set current gradients to 0\n",
    "            loss.backward()      #backpropagate\n",
    "            optimizer2.step()     #update the weights\n",
    "            mean_loss_list.append(loss.item())\n",
    "            #compute and print the mean L1 lossscore for the last 20 training images.\n",
    "            if img_nr%20 == 0:\n",
    "                print(\"L1 Loss score: \", np.round(np.mean(mean_loss_list), decimals = 5), \"  Image number: \", img_nr, \"  Epoch: \", epoch+1)\n",
    "                mean_loss_list = []\n",
    "            losses2.append(loss.item() * img_gt.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8398d694-2653-48d9-924a-d67aa6737020",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e91a54-7283-4ed1-bac3-647061cd4224",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the SSIM score for every image after a feedforward propagation through \n",
    "#the network.\n",
    "#Subtract the image SSIM score before the feedforward prop to obtain the net improvement for every image.\n",
    "#Print the average improvement and the average SSIM score after the reconstruction.\n",
    "SSIM_improvement = []\n",
    "SSIM_score = []\n",
    "for i in range(0,len(train_dataset)):\n",
    "    gt, image = train_dataset[i]\n",
    "    image = image.unsqueeze(0).to('cuda:0')\n",
    "    image = image.unsqueeze(0)\n",
    "    gt = gt.unsqueeze(0).numpy()\n",
    "    output = network_8fold(image)\n",
    "    output = output.squeeze(1).cpu().detach().numpy()\n",
    "    output_loss = torch.tensor(ssim(gt, output))\n",
    "    image_loss = torch.tensor(ssim(gt, image.squeeze(1).cpu().numpy()))\n",
    "    SSIM_improvement.append(output_loss.item()-image_loss.item())\n",
    "    SSIM_score.append(output_loss.item())\n",
    "\n",
    "print(np.nanmean(SSIM_improvement))\n",
    "print(np.nanmean(SSIM_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a42fb6f-8b3e-49ff-a839-136dcc02a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_improvement.sort()\n",
    "plt.plot(SSIM_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46616a72-3343-43d4-bb64-34e2b74975e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
