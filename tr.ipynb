{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2b25b451-e7e8-476a-8910-cf745809ef08",
   "metadata": {},
   "source": [
    "get TR from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f951d4e-51fb-4521-81cd-48eb1c5e2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\"\"\"Import from keras_preprocessing not from keras.preprocessing, because Keras may or maynot contain the features discussed here depending upon when you read this article, until the keras_preprocessed library is updated in Keras use the github version.\"\"\"\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "#from tensorflow.keras import optimizers #., optimizers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#Importing all the relevant library\n",
    "%matplotlib inline\n",
    "import h5py, os\n",
    "#from functions import transforms as T\n",
    "#from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "#from functions import transforms as T \n",
    "#from functions.subsample import MaskFunc\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8d08e5-d611-4b99-add9-02cc854a9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path, val_data_path, test_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_val_test = ['train', 'val', 'test']\n",
    "    data_path = [train_data_path, val_data_path, test_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)):\n",
    "\n",
    "        data_list[train_val_test[i]] = []\n",
    "        \n",
    "        which_data_path = data_path[i]\n",
    "        tr = 0\n",
    "        te = 0\n",
    "        alfa = 0\n",
    "        for fname in sorted(os.listdir(which_data_path + '/images')):\n",
    "            if fname != \".DS_Store\":\n",
    "\n",
    "            \n",
    "                subject_data_path = os.path.join(which_data_path + '/images', fname)\n",
    "                     \n",
    "                if not os.path.isfile(subject_data_path): continue \n",
    "            \n",
    "          #  im_frame = Image.open(subject_data_path)\n",
    "\n",
    "            #get information from text file\n",
    "            # this will return a tuple of root and extension\n",
    "                split_tup = os.path.splitext(fname)\n",
    "\n",
    "  \n",
    "            # extract the file name and extension\n",
    "                file_name = split_tup[0]\n",
    "                file_path = os.path.join(which_data_path + '/texts', file_name) + '.txt'\n",
    "                f = open(os.path.join(which_data_path + '/texts', file_name) + '.txt', 'r')\n",
    "                line = f.readlines()[1]\n",
    "            \n",
    "                fields = line.split(',')\n",
    "                tr = int(fields[3])\n",
    "                te = int(fields[4])\n",
    "                alfa = int(fields[5])\n",
    "                f.close()\n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "                data_list[train_val_test[i]] += [(fname, tr)]\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e859bb9f-fa84-4609-bf9e-e2decd504740",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = load_data_path (\"data/train\", \"data/val\", \"data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e6528b-7dbc-44ca-86f3-810e789cba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_list['train']\n",
    "val_data = data_list['val']\n",
    "test_data = data_list['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08aa5111-5a82-4237-ba6c-72ee17a69ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_data, columns=['fnames', 'labels'])\n",
    "train_df['labels']= train_df['labels'].astype(str)\n",
    "train_df['fnames']= train_df['fnames'].astype(str)\n",
    "val_df = pd.DataFrame(val_data, columns=['fnames', 'labels'])\n",
    "val_df['labels']= val_df['labels'].astype(str)\n",
    "val_df['fnames']= val_df['fnames'].astype(str)\n",
    "test_df = pd.DataFrame(test_data, columns=['fnames', 'labels'])\n",
    "test_df['labels']= test_df['labels'].astype(str)\n",
    "test_df['fnames']= test_df['fnames'].astype(str)\n",
    "labels = train_df.labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce8641ab-96af-4fea-8b4e-2b4e38f108a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(labels)\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3c7e74a-edb4-4d1a-b1f4-52addf03c259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of         fnames labels\n",
       "0        1.png      5\n",
       "1       10.png      5\n",
       "2      100.png      5\n",
       "3     1000.png     20\n",
       "4     1001.png     16\n",
       "...        ...    ...\n",
       "3060   995.png      2\n",
       "3061   996.png     18\n",
       "3062   997.png     17\n",
       "3063   998.png     16\n",
       "3064   999.png      6\n",
       "\n",
       "[3065 rows x 2 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beca63cc-5587-4daf-b728-00942ebfa873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3065 validated image filenames belonging to 20 classes.\n",
      "Found 655 validated image filenames belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "      dataframe=train_df,\n",
    "      directory=\"./data/train/images/\",\n",
    "      x_col=\"fnames\",\n",
    "      y_col=\"labels\",\n",
    "      batch_size=64,\n",
    "      seed=42,\n",
    "      shuffle=True,\n",
    "      class_mode=\"categorical\",\n",
    "      target_size=(224,224))\n",
    "\n",
    "\n",
    "\n",
    "valid_generator=test_datagen.flow_from_dataframe(\n",
    "      dataframe=val_df,\n",
    "      directory=\"./data/val/images/\",\n",
    "      x_col=\"fnames\",\n",
    "      y_col=\"labels\",\n",
    "      batch_size=64,\n",
    "      seed=42,\n",
    "      shuffle=True,\n",
    "      class_mode=\"categorical\",\n",
    "     target_size=(224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49c790a0-c070-46fc-a219-c03f065e41d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 646 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/keras_preprocessing/image/dataframe_iterator.py:282: UserWarning: Found 9 invalid image filename(s) in x_col=\"fnames\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n"
     ]
    }
   ],
   "source": [
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "dataframe=test_df,\n",
    "      directory=\"./data/test/images\",\n",
    "      x_col=\"fnames\",\n",
    "      batch_size=1,\n",
    "      seed=42,\n",
    "      shuffle=False,\n",
    "      class_mode=None,\n",
    "      target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4403221-0e45-4cc5-9cc8-6e84aa7d76e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-01 12:45:24.801980: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(224,224,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_class, activation='sigmoid'))\n",
    "model.compile(tf.keras.optimizers.RMSprop(lr=0.001, decay=1e-6),loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5ee260d-ecc7-46a5-988f-9ca7b092e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300 #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69b861c-1472-444a-a2ee-c158e0a96e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
      "2021-12-01 12:45:25.837331: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "47/47 [==============================] - 249s 5s/step - loss: 0.3944 - accuracy: 0.0470 - val_loss: 0.2288 - val_accuracy: 0.0562\n",
      "Epoch 2/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.2179 - accuracy: 0.0511 - val_loss: 0.2066 - val_accuracy: 0.0453\n",
      "Epoch 3/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.2122 - accuracy: 0.0479 - val_loss: 0.2062 - val_accuracy: 0.0594\n",
      "Epoch 4/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.2068 - accuracy: 0.0579 - val_loss: 0.2004 - val_accuracy: 0.0641\n",
      "Epoch 5/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.2030 - accuracy: 0.0587 - val_loss: 0.2179 - val_accuracy: 0.0469\n",
      "Epoch 6/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.2038 - accuracy: 0.0761 - val_loss: 0.2028 - val_accuracy: 0.0531\n",
      "Epoch 7/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.1982 - accuracy: 0.0890 - val_loss: 0.2044 - val_accuracy: 0.0500\n",
      "Epoch 8/300\n",
      "47/47 [==============================] - 245s 5s/step - loss: 0.1984 - accuracy: 0.0828 - val_loss: 0.2038 - val_accuracy: 0.0516\n",
      "Epoch 9/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.2038 - accuracy: 0.1013 - val_loss: 0.2061 - val_accuracy: 0.0484\n",
      "Epoch 10/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.1920 - accuracy: 0.1085 - val_loss: 0.2082 - val_accuracy: 0.0516\n",
      "Epoch 11/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.1951 - accuracy: 0.1341 - val_loss: 0.2063 - val_accuracy: 0.0469\n",
      "Epoch 12/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.1874 - accuracy: 0.1469 - val_loss: 0.2139 - val_accuracy: 0.0562\n",
      "Epoch 13/300\n",
      "47/47 [==============================] - 245s 5s/step - loss: 0.1825 - accuracy: 0.1710 - val_loss: 0.2136 - val_accuracy: 0.0406\n",
      "Epoch 14/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.1771 - accuracy: 0.2015 - val_loss: 0.2232 - val_accuracy: 0.0609\n",
      "Epoch 15/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.1738 - accuracy: 0.2048 - val_loss: 0.2252 - val_accuracy: 0.0422\n",
      "Epoch 16/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.1663 - accuracy: 0.2462 - val_loss: 0.2294 - val_accuracy: 0.0391\n",
      "Epoch 17/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.1627 - accuracy: 0.2680 - val_loss: 0.2307 - val_accuracy: 0.0453\n",
      "Epoch 18/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.1604 - accuracy: 0.2747 - val_loss: 0.2284 - val_accuracy: 0.0406\n",
      "Epoch 19/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.1578 - accuracy: 0.2840 - val_loss: 0.2421 - val_accuracy: 0.0578\n",
      "Epoch 20/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.1553 - accuracy: 0.3114 - val_loss: 0.2590 - val_accuracy: 0.0484\n",
      "Epoch 21/300\n",
      "47/47 [==============================] - 252s 5s/step - loss: 0.1510 - accuracy: 0.3149 - val_loss: 0.2601 - val_accuracy: 0.0516\n",
      "Epoch 22/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.1490 - accuracy: 0.3438 - val_loss: 0.2507 - val_accuracy: 0.0547\n",
      "Epoch 23/300\n",
      "47/47 [==============================] - 254s 5s/step - loss: 0.1454 - accuracy: 0.3526 - val_loss: 0.2816 - val_accuracy: 0.0469\n",
      "Epoch 24/300\n",
      "47/47 [==============================] - 266s 6s/step - loss: 0.1396 - accuracy: 0.3745 - val_loss: 0.2680 - val_accuracy: 0.0531\n",
      "Epoch 25/300\n",
      "47/47 [==============================] - 261s 6s/step - loss: 0.1532 - accuracy: 0.3758 - val_loss: 0.2963 - val_accuracy: 0.0562\n",
      "Epoch 26/300\n",
      "47/47 [==============================] - 254s 5s/step - loss: 0.1342 - accuracy: 0.3976 - val_loss: 0.2635 - val_accuracy: 0.0562\n",
      "Epoch 27/300\n",
      "47/47 [==============================] - 274s 6s/step - loss: 0.1341 - accuracy: 0.4006 - val_loss: 0.2972 - val_accuracy: 0.0516\n",
      "Epoch 28/300\n",
      "47/47 [==============================] - 264s 6s/step - loss: 0.1309 - accuracy: 0.4082 - val_loss: 0.2834 - val_accuracy: 0.0562\n",
      "Epoch 29/300\n",
      "47/47 [==============================] - 297s 6s/step - loss: 0.1289 - accuracy: 0.4255 - val_loss: 0.3218 - val_accuracy: 0.0484\n",
      "Epoch 30/300\n",
      "47/47 [==============================] - 380s 8s/step - loss: 0.1231 - accuracy: 0.4569 - val_loss: 0.2897 - val_accuracy: 0.0594\n",
      "Epoch 31/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.1244 - accuracy: 0.4499 - val_loss: 0.3321 - val_accuracy: 0.0469\n",
      "Epoch 32/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.1208 - accuracy: 0.4603 - val_loss: 0.3635 - val_accuracy: 0.0531\n",
      "Epoch 33/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.1206 - accuracy: 0.4705 - val_loss: 0.3360 - val_accuracy: 0.0547\n",
      "Epoch 34/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.1170 - accuracy: 0.4814 - val_loss: 0.3427 - val_accuracy: 0.0469\n",
      "Epoch 35/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.1166 - accuracy: 0.4916 - val_loss: 0.3419 - val_accuracy: 0.0469\n",
      "Epoch 36/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.1151 - accuracy: 0.4908 - val_loss: 0.3438 - val_accuracy: 0.0469\n",
      "Epoch 37/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.1163 - accuracy: 0.4966 - val_loss: 0.3884 - val_accuracy: 0.0484\n",
      "Epoch 38/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.1083 - accuracy: 0.5231 - val_loss: 0.3860 - val_accuracy: 0.0437\n",
      "Epoch 39/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.1078 - accuracy: 0.5296 - val_loss: 0.4060 - val_accuracy: 0.0641\n",
      "Epoch 40/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.1270 - accuracy: 0.5091 - val_loss: 0.4452 - val_accuracy: 0.0469\n",
      "Epoch 41/300\n",
      "47/47 [==============================] - 244s 5s/step - loss: 0.1070 - accuracy: 0.5360 - val_loss: 0.3617 - val_accuracy: 0.0516\n",
      "Epoch 42/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.1049 - accuracy: 0.5400 - val_loss: 0.4327 - val_accuracy: 0.0531\n",
      "Epoch 43/300\n",
      "47/47 [==============================] - 244s 5s/step - loss: 0.1044 - accuracy: 0.5483 - val_loss: 0.3406 - val_accuracy: 0.0547\n",
      "Epoch 44/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.1031 - accuracy: 0.5503 - val_loss: 0.4533 - val_accuracy: 0.0469\n",
      "Epoch 45/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.1021 - accuracy: 0.5631 - val_loss: 0.4350 - val_accuracy: 0.0547\n",
      "Epoch 46/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.1006 - accuracy: 0.5539 - val_loss: 0.4240 - val_accuracy: 0.0516\n",
      "Epoch 47/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0982 - accuracy: 0.5744 - val_loss: 0.3780 - val_accuracy: 0.0516\n",
      "Epoch 48/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0995 - accuracy: 0.5669 - val_loss: 0.4753 - val_accuracy: 0.0531\n",
      "Epoch 49/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0970 - accuracy: 0.5761 - val_loss: 0.4184 - val_accuracy: 0.0578\n",
      "Epoch 50/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0959 - accuracy: 0.5808 - val_loss: 0.4470 - val_accuracy: 0.0531\n",
      "Epoch 51/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0972 - accuracy: 0.5774 - val_loss: 0.3824 - val_accuracy: 0.0531\n",
      "Epoch 52/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0978 - accuracy: 0.5821 - val_loss: 0.4974 - val_accuracy: 0.0500\n",
      "Epoch 53/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0972 - accuracy: 0.5699 - val_loss: 0.4212 - val_accuracy: 0.0469\n",
      "Epoch 54/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0938 - accuracy: 0.5828 - val_loss: 0.4381 - val_accuracy: 0.0469\n",
      "Epoch 55/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0940 - accuracy: 0.5834 - val_loss: 0.5017 - val_accuracy: 0.0656\n",
      "Epoch 56/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.0953 - accuracy: 0.5768 - val_loss: 0.4810 - val_accuracy: 0.0500\n",
      "Epoch 57/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0917 - accuracy: 0.6064 - val_loss: 0.5038 - val_accuracy: 0.0609\n",
      "Epoch 58/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.0924 - accuracy: 0.5941 - val_loss: 0.4789 - val_accuracy: 0.0625\n",
      "Epoch 59/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0903 - accuracy: 0.6103 - val_loss: 0.4361 - val_accuracy: 0.0484\n",
      "Epoch 60/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0933 - accuracy: 0.5894 - val_loss: 0.5755 - val_accuracy: 0.0547\n",
      "Epoch 61/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0902 - accuracy: 0.6072 - val_loss: 0.5516 - val_accuracy: 0.0578\n",
      "Epoch 62/300\n",
      "47/47 [==============================] - 243s 5s/step - loss: 0.0893 - accuracy: 0.5998 - val_loss: 0.4946 - val_accuracy: 0.0516\n",
      "Epoch 63/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.0929 - accuracy: 0.5952 - val_loss: 0.4916 - val_accuracy: 0.0578\n",
      "Epoch 64/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0913 - accuracy: 0.5908 - val_loss: 0.2977 - val_accuracy: 0.0531\n",
      "Epoch 65/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0915 - accuracy: 0.6230 - val_loss: 0.4736 - val_accuracy: 0.0578\n",
      "Epoch 66/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0886 - accuracy: 0.6052 - val_loss: 0.5026 - val_accuracy: 0.0703\n",
      "Epoch 67/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0871 - accuracy: 0.6160 - val_loss: 0.4930 - val_accuracy: 0.0547\n",
      "Epoch 68/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0862 - accuracy: 0.6301 - val_loss: 0.5411 - val_accuracy: 0.0484\n",
      "Epoch 69/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0876 - accuracy: 0.6232 - val_loss: 0.3404 - val_accuracy: 0.0500\n",
      "Epoch 70/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0877 - accuracy: 0.6229 - val_loss: 0.6205 - val_accuracy: 0.0500\n",
      "Epoch 71/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0912 - accuracy: 0.6033 - val_loss: 0.4649 - val_accuracy: 0.0453\n",
      "Epoch 72/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0897 - accuracy: 0.6093 - val_loss: 0.6729 - val_accuracy: 0.0484\n",
      "Epoch 73/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0885 - accuracy: 0.6039 - val_loss: 0.6335 - val_accuracy: 0.0547\n",
      "Epoch 74/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0906 - accuracy: 0.6055 - val_loss: 0.4522 - val_accuracy: 0.0562\n",
      "Epoch 75/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0869 - accuracy: 0.6295 - val_loss: 0.5104 - val_accuracy: 0.0625\n",
      "Epoch 76/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0841 - accuracy: 0.6387 - val_loss: 0.6606 - val_accuracy: 0.0516\n",
      "Epoch 77/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0852 - accuracy: 0.6351 - val_loss: 0.4500 - val_accuracy: 0.0484\n",
      "Epoch 78/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0867 - accuracy: 0.6245 - val_loss: 0.5931 - val_accuracy: 0.0516\n",
      "Epoch 79/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0869 - accuracy: 0.6221 - val_loss: 0.5652 - val_accuracy: 0.0531\n",
      "Epoch 80/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.0861 - accuracy: 0.6247 - val_loss: 0.6408 - val_accuracy: 0.0516\n",
      "Epoch 81/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0878 - accuracy: 0.6236 - val_loss: 0.4153 - val_accuracy: 0.0578\n",
      "Epoch 82/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0856 - accuracy: 0.6287 - val_loss: 0.5624 - val_accuracy: 0.0578\n",
      "Epoch 83/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0861 - accuracy: 0.6349 - val_loss: 0.4845 - val_accuracy: 0.0609\n",
      "Epoch 84/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0829 - accuracy: 0.6360 - val_loss: 0.4708 - val_accuracy: 0.0500\n",
      "Epoch 85/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0883 - accuracy: 0.6156 - val_loss: 0.6067 - val_accuracy: 0.0578\n",
      "Epoch 86/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0816 - accuracy: 0.6366 - val_loss: 0.6739 - val_accuracy: 0.0641\n",
      "Epoch 87/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0898 - accuracy: 0.5999 - val_loss: 0.6703 - val_accuracy: 0.0562\n",
      "Epoch 88/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0838 - accuracy: 0.6265 - val_loss: 0.6126 - val_accuracy: 0.0609\n",
      "Epoch 89/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0850 - accuracy: 0.6242 - val_loss: 0.5634 - val_accuracy: 0.0562\n",
      "Epoch 90/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0837 - accuracy: 0.6429 - val_loss: 0.4246 - val_accuracy: 0.0578\n",
      "Epoch 91/300\n",
      "47/47 [==============================] - 361s 8s/step - loss: 0.0832 - accuracy: 0.6367 - val_loss: 0.6616 - val_accuracy: 0.0531\n",
      "Epoch 92/300\n",
      "47/47 [==============================] - 468s 10s/step - loss: 0.0870 - accuracy: 0.6287 - val_loss: 0.7464 - val_accuracy: 0.0562\n",
      "Epoch 93/300\n",
      "47/47 [==============================] - 305s 6s/step - loss: 0.0793 - accuracy: 0.6529 - val_loss: 0.6519 - val_accuracy: 0.0672\n",
      "Epoch 94/300\n",
      "47/47 [==============================] - 295s 6s/step - loss: 0.0922 - accuracy: 0.6200 - val_loss: 0.5909 - val_accuracy: 0.0547\n",
      "Epoch 95/300\n",
      "47/47 [==============================] - 284s 6s/step - loss: 0.0841 - accuracy: 0.6372 - val_loss: 0.6341 - val_accuracy: 0.0484\n",
      "Epoch 96/300\n",
      "47/47 [==============================] - 273s 6s/step - loss: 0.0847 - accuracy: 0.6316 - val_loss: 0.6835 - val_accuracy: 0.0656\n",
      "Epoch 97/300\n",
      "47/47 [==============================] - 259s 6s/step - loss: 0.0868 - accuracy: 0.6277 - val_loss: 0.6489 - val_accuracy: 0.0625\n",
      "Epoch 98/300\n",
      "47/47 [==============================] - 267s 6s/step - loss: 0.0865 - accuracy: 0.6228 - val_loss: 0.6260 - val_accuracy: 0.0781\n",
      "Epoch 99/300\n",
      "47/47 [==============================] - 268s 6s/step - loss: 0.0845 - accuracy: 0.6316 - val_loss: 0.5582 - val_accuracy: 0.0531\n",
      "Epoch 100/300\n",
      "47/47 [==============================] - 262s 6s/step - loss: 0.0830 - accuracy: 0.6457 - val_loss: 0.7310 - val_accuracy: 0.0625\n",
      "Epoch 101/300\n",
      "47/47 [==============================] - 264s 6s/step - loss: 0.0851 - accuracy: 0.6387 - val_loss: 0.7982 - val_accuracy: 0.0594\n",
      "Epoch 102/300\n",
      "47/47 [==============================] - 267s 6s/step - loss: 0.0824 - accuracy: 0.6437 - val_loss: 0.5965 - val_accuracy: 0.0594\n",
      "Epoch 103/300\n",
      "47/47 [==============================] - 285s 6s/step - loss: 0.0826 - accuracy: 0.6402 - val_loss: 0.6634 - val_accuracy: 0.0734\n",
      "Epoch 104/300\n",
      "47/47 [==============================] - 280s 6s/step - loss: 0.0819 - accuracy: 0.6428 - val_loss: 0.8112 - val_accuracy: 0.0578\n",
      "Epoch 105/300\n",
      "47/47 [==============================] - 363s 8s/step - loss: 0.0827 - accuracy: 0.6491 - val_loss: 0.6638 - val_accuracy: 0.0594\n",
      "Epoch 106/300\n",
      "47/47 [==============================] - 329s 7s/step - loss: 0.0812 - accuracy: 0.6417 - val_loss: 0.6711 - val_accuracy: 0.0609\n",
      "Epoch 107/300\n",
      "47/47 [==============================] - 380s 8s/step - loss: 0.0808 - accuracy: 0.6416 - val_loss: 0.8656 - val_accuracy: 0.0625\n",
      "Epoch 108/300\n",
      "47/47 [==============================] - 368s 8s/step - loss: 0.0921 - accuracy: 0.6171 - val_loss: 0.7186 - val_accuracy: 0.0594\n",
      "Epoch 109/300\n",
      "47/47 [==============================] - 456s 10s/step - loss: 0.0788 - accuracy: 0.6583 - val_loss: 0.5767 - val_accuracy: 0.0609\n",
      "Epoch 110/300\n",
      "47/47 [==============================] - 273s 6s/step - loss: 0.0838 - accuracy: 0.6443 - val_loss: 0.6373 - val_accuracy: 0.0609\n",
      "Epoch 111/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0800 - accuracy: 0.6510 - val_loss: 0.7751 - val_accuracy: 0.0562\n",
      "Epoch 112/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0843 - accuracy: 0.6371 - val_loss: 0.9531 - val_accuracy: 0.0547\n",
      "Epoch 113/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0800 - accuracy: 0.6575 - val_loss: 0.3177 - val_accuracy: 0.0484\n",
      "Epoch 114/300\n",
      "47/47 [==============================] - 242s 5s/step - loss: 0.0848 - accuracy: 0.6387 - val_loss: 0.6751 - val_accuracy: 0.0625\n",
      "Epoch 115/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0914 - accuracy: 0.6421 - val_loss: 0.6350 - val_accuracy: 0.0578\n",
      "Epoch 116/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0792 - accuracy: 0.6572 - val_loss: 0.5308 - val_accuracy: 0.0500\n",
      "Epoch 117/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0839 - accuracy: 0.6367 - val_loss: 0.5869 - val_accuracy: 0.0594\n",
      "Epoch 118/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0829 - accuracy: 0.6504 - val_loss: 0.6652 - val_accuracy: 0.0562\n",
      "Epoch 119/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0815 - accuracy: 0.6454 - val_loss: 0.6832 - val_accuracy: 0.0641\n",
      "Epoch 120/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0835 - accuracy: 0.6400 - val_loss: 0.7869 - val_accuracy: 0.0656\n",
      "Epoch 121/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0809 - accuracy: 0.6457 - val_loss: 0.5616 - val_accuracy: 0.0594\n",
      "Epoch 122/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0831 - accuracy: 0.6480 - val_loss: 0.6076 - val_accuracy: 0.0672\n",
      "Epoch 123/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0781 - accuracy: 0.6520 - val_loss: 0.5784 - val_accuracy: 0.0625\n",
      "Epoch 124/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0840 - accuracy: 0.6493 - val_loss: 0.6954 - val_accuracy: 0.0547\n",
      "Epoch 125/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0797 - accuracy: 0.6609 - val_loss: 0.7227 - val_accuracy: 0.0531\n",
      "Epoch 126/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0809 - accuracy: 0.6576 - val_loss: 0.7497 - val_accuracy: 0.0609\n",
      "Epoch 127/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0854 - accuracy: 0.6251 - val_loss: 0.7335 - val_accuracy: 0.0625\n",
      "Epoch 128/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0846 - accuracy: 0.6400 - val_loss: 0.6670 - val_accuracy: 0.0641\n",
      "Epoch 129/300\n",
      "47/47 [==============================] - 244s 5s/step - loss: 0.0800 - accuracy: 0.6441 - val_loss: 0.6334 - val_accuracy: 0.0594\n",
      "Epoch 130/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0819 - accuracy: 0.6470 - val_loss: 0.6559 - val_accuracy: 0.0609\n",
      "Epoch 131/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0805 - accuracy: 0.6483 - val_loss: 0.6615 - val_accuracy: 0.0594\n",
      "Epoch 132/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0797 - accuracy: 0.6520 - val_loss: 0.6084 - val_accuracy: 0.0609\n",
      "Epoch 133/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0834 - accuracy: 0.6434 - val_loss: 0.6332 - val_accuracy: 0.0672\n",
      "Epoch 134/300\n",
      "47/47 [==============================] - 242s 5s/step - loss: 0.0834 - accuracy: 0.6455 - val_loss: 0.5661 - val_accuracy: 0.0562\n",
      "Epoch 135/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0839 - accuracy: 0.6450 - val_loss: 0.6641 - val_accuracy: 0.0531\n",
      "Epoch 136/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0830 - accuracy: 0.6568 - val_loss: 0.8197 - val_accuracy: 0.0672\n",
      "Epoch 137/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.1518 - accuracy: 0.6402 - val_loss: 0.5161 - val_accuracy: 0.0547\n",
      "Epoch 138/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0926 - accuracy: 0.6102 - val_loss: 0.7002 - val_accuracy: 0.0625\n",
      "Epoch 139/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0840 - accuracy: 0.6344 - val_loss: 0.6156 - val_accuracy: 0.0578\n",
      "Epoch 140/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0800 - accuracy: 0.6553 - val_loss: 0.6756 - val_accuracy: 0.0469\n",
      "Epoch 141/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0819 - accuracy: 0.6419 - val_loss: 0.7246 - val_accuracy: 0.0516\n",
      "Epoch 142/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0821 - accuracy: 0.6492 - val_loss: 0.6722 - val_accuracy: 0.0703\n",
      "Epoch 143/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0800 - accuracy: 0.6561 - val_loss: 0.7997 - val_accuracy: 0.0578\n",
      "Epoch 144/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0789 - accuracy: 0.6528 - val_loss: 0.8313 - val_accuracy: 0.0641\n",
      "Epoch 145/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0832 - accuracy: 0.6420 - val_loss: 0.7528 - val_accuracy: 0.0609\n",
      "Epoch 146/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0824 - accuracy: 0.6430 - val_loss: 0.7210 - val_accuracy: 0.0578\n",
      "Epoch 147/300\n",
      "47/47 [==============================] - 243s 5s/step - loss: 0.0826 - accuracy: 0.6420 - val_loss: 0.7357 - val_accuracy: 0.0641\n",
      "Epoch 148/300\n",
      "33/47 [====================>.........] - ETA: 1:11 - loss: 0.0830 - accuracy: 0.6489"
     ]
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9323567d-93a3-4500-b39a-a00318629079",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict_generator(test_generator,\n",
    "steps=STEP_SIZE_TEST,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f169b3-9e88-43d7-9480-c17a80cb9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d306d73-b624-46e9-aaa3-dbcca93dd9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bool = (pred >0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a65ff-7ab7-4ec6-a745-bbf13c9c73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "labels = train_generator.class_indices\n",
    "\n",
    "textfile = open(\"TR_labels.txt\", \"w\")\n",
    "for element in labels:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "for row in pred_bool:\n",
    "    l=[]\n",
    "    for index,cls in enumerate(row):\n",
    "        if cls:\n",
    "            l.append(labels[index])\n",
    "    predictions.append(\",\".join(l))\n",
    "filenames=test_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})\n",
    "results.to_csv(\"TRresults.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385e1fa-c041-40a3-ad3a-c47482ed3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "476521a0-bfa6-411f-9793-2fd3f3d8e161",
   "metadata": {},
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "#load the image\n",
    "my_image = load_img('data/test/images/127.png', target_size=(224, 224))\n",
    "\n",
    "#preprocess the image\n",
    "my_image = img_to_array(my_image)\n",
    "my_image = my_image.reshape((1, my_image.shape[0], my_image.shape[1], my_image.shape[2]))\n",
    "my_image = preprocess_input(my_image)\n",
    "\n",
    "#make the prediction\n",
    "prediction = model.predict(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c30c92-b894-4711-a67e-0bf044228268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "original = PIL.Image.open(\"data/test/images/127.png\")\n",
    "file_type = original.format\n",
    "\n",
    "original.save(\"testing/test.png\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed903af-3ed6-445b-ad90-5300d4304062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "original = PIL.Image.open(\"data/test/images/127.png\") # replace later with image input\n",
    "file_type = original.format\n",
    "\n",
    "original.save(\"testing/test.png\", format=\"png\")\n",
    "##########################\n",
    "testdata = []\n",
    "for fname in sorted(os.listdir('testing')):\n",
    "    if fname == \".DS_Store\": continue\n",
    "            \n",
    "    subject_data_path = os.path.join('testing', fname)                   \n",
    "    if not os.path.isfile(subject_data_path): continue          \n",
    "    testdata.append(fname)\n",
    "    \n",
    "df = pd.DataFrame(testdata, columns=['fnames'])\n",
    "df['fnames']= df['fnames'].astype(str)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "test_g=test_datagen.flow_from_dataframe(\n",
    "     dataframe= df,\n",
    "      directory=\"./testing\",\n",
    "      x_col=\"fnames\",\n",
    "      batch_size=1,\n",
    "      seed=42,\n",
    "      shuffle=False,\n",
    "      class_mode=None,\n",
    "      target_size=(224,224))\n",
    "STEP_SIZE_TEST=test_g.n//test_g.batch_size\n",
    "pred = model.predict_generator(test_g,\n",
    "                               steps=STEP_SIZE_TEST,\n",
    "                               verbose=1)\n",
    "pred_bool = (pred >0.5)\n",
    "predictions=[]\n",
    "#labels = train_generator.class_indices\n",
    "#labels = dict((v,k) for k,v in labels.items())\n",
    "labels = {}\n",
    "file1 = open('TR_labels.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    " \n",
    "count = 0\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    labels[co\n",
    "    count += 1\n",
    "    print(\"Line{}: {}\".format(count, line.strip()))\n",
    "for row in pred_bool:\n",
    "    l=[]\n",
    "    for index,cls in enumerate(row):\n",
    "        if cls:\n",
    "            l.append(labels[index])\n",
    "    predictions.append(\",\".join(l))\n",
    "           \n",
    "if predictions[0] == '':\n",
    "    result = 0\n",
    "else: \n",
    "    result = float (predictions[0] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e81fe7-eaf1-4b6c-bb83-60163f2051ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06603d7a-be6f-4b00-a566-274256c1fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c159d-64be-4d98-b12d-98ff511626a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2964a2d6-0d92-4019-b93e-f908bd728ac7",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('data/test/images/127.png', target_size =(224,224))\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "result = model.predict(test_image)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd6718b4-dc5f-4115-b886-424f6fcfe55d",
   "metadata": {},
   "source": [
    "test_image = image.load_img('data/test/images/228.png', target_size =(224,224))\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be769dfd-b348-4067-bb46-276897728b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "filename2 = 'model_tr.h5' \n",
    "model.save(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607599c9-9b5d-44b2-a74a-95516d370e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# load model\n",
    "model = load_model('model_tr.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "652a1b62-1de5-4853-b3c4-d36882a6a62b",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2020/10/create-image-classification-model-python-keras/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23e021e2-aaed-440a-bda5-a8b1b4e313fd",
   "metadata": {},
   "source": [
    "    def flow_from_dataframe(self,\n",
    "                            dataframe,\n",
    "                            directory=None,\n",
    "                            x_col=\"filename\",\n",
    "                            y_col=\"class\",\n",
    "                            weight_col=None,\n",
    "                            target_size=(256, 256),\n",
    "                            color_mode='rgb',\n",
    "                            classes=None,\n",
    "                            class_mode='categorical',\n",
    "                            batch_size=32,\n",
    "                            shuffle=True,\n",
    "                            seed=None,\n",
    "                            save_to_dir=None,\n",
    "                            save_prefix='',\n",
    "                            save_format='png',\n",
    "                            subset=None,\n",
    "                            interpolation='nearest',\n",
    "                            validate_filenames=True,\n",
    "                            **kwargs):\n",
    "        \"\"\"Takes the dataframe and the path to a directory\n",
    "         and generates batches of augmented/normalized data.\n",
    "        **A simple tutorial can be found **[here](\n",
    "                                    http://bit.ly/keras_flow_from_dataframe).\n",
    "        # Arguments\n",
    "            dataframe: Pandas dataframe containing the filepaths relative to\n",
    "                `directory` (or absolute paths if `directory` is None) of the\n",
    "                images in a string column. It should include other column/s\n",
    "                depending on the `class_mode`:\n",
    "                - if `class_mode` is `\"categorical\"` (default value) it must\n",
    "                    include the `y_col` column with the class/es of each image.\n",
    "                    Values in column can be string/list/tuple if a single class\n",
    "                    or list/tuple if multiple classes.\n",
    "                - if `class_mode` is `\"binary\"` or `\"sparse\"` it must include\n",
    "                    the given `y_col` column with class values as strings.\n",
    "                - if `class_mode` is `\"raw\"` or `\"multi_output\"` it should contain\n",
    "                the columns specified in `y_col`.\n",
    "                - if `class_mode` is `\"input\"` or `None` no extra column is needed.\n",
    "            directory: string, path to the directory to read images from. If `None`,\n",
    "                data in `x_col` column should be absolute paths.\n",
    "            x_col: string, column in `dataframe` that contains the filenames (or\n",
    "                absolute paths if `directory` is `None`).\n",
    "            y_col: string or list, column/s in `dataframe` that has the target data.\n",
    "            weight_col: string, column in `dataframe` that contains the sample\n",
    "                weights. Default: `None`.\n",
    "            target_size: tuple of integers `(height, width)`, default: `(256, 256)`.\n",
    "                The dimensions to which all images found will be resized.\n",
    "            color_mode: one of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
    "                Whether the images will be converted to have 1 or 3 color channels.\n",
    "            classes: optional list of classes (e.g. `['dogs', 'cats']`).\n",
    "                Default: None. If not provided, the list of classes will be\n",
    "                automatically inferred from the `y_col`,\n",
    "                which will map to the label indices, will be alphanumeric).\n",
    "                The dictionary containing the mapping from class names to class\n",
    "                indices can be obtained via the attribute `class_indices`.\n",
    "            class_mode: one of \"binary\", \"categorical\", \"input\", \"multi_output\",\n",
    "                \"raw\", sparse\" or None. Default: \"categorical\".\n",
    "                Mode for yielding the targets:\n",
    "                - `\"binary\"`: 1D NumPy array of binary labels,\n",
    "                - `\"categorical\"`: 2D NumPy array of one-hot encoded labels.\n",
    "                    Supports multi-label output.\n",
    "                - `\"input\"`: images identical to input images (mainly used to\n",
    "                    work with autoencoders),\n",
    "                - `\"multi_output\"`: list with the values of the different columns,\n",
    "                - `\"raw\"`: NumPy array of values in `y_col` column(s),\n",
    "                - `\"sparse\"`: 1D NumPy array of integer labels,\n",
    "                - `None`, no targets are returned (the generator will only yield\n",
    "                    batches of image data, which is useful to use in\n",
    "                    `model.predict_generator()`).\n",
    "            batch_size: size of the batches of data (default: 32).\n",
    "            shuffle: whether to shuffle the data (default: True)\n",
    "            seed: optional random seed for shuffling and transformations.\n",
    "            save_to_dir: None or str (default: None).\n",
    "                This allows you to optionally specify a directory\n",
    "                to which to save the augmented pictures being generated\n",
    "                (useful for visualizing what you are doing).\n",
    "            save_prefix: str. Prefix to use for filenames of saved pictures\n",
    "                (only relevant if `save_to_dir` is set).\n",
    "            save_format: one of \"png\", \"jpeg\"\n",
    "                (only relevant if `save_to_dir` is set). Default: \"png\".\n",
    "            follow_links: whether to follow symlinks inside class subdirectories\n",
    "                (default: False).\n",
    "            subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
    "                `validation_split` is set in `ImageDataGenerator`.\n",
    "            interpolation: Interpolation method used to resample the image if the\n",
    "                target size is different from that of the loaded image.\n",
    "                Supported methods are `\"nearest\"`, `\"bilinear\"`, and `\"bicubic\"`.\n",
    "                If PIL version 1.1.3 or newer is installed, `\"lanczos\"` is also\n",
    "                supported. If PIL version 3.4.0 or newer is installed, `\"box\"` and\n",
    "                `\"hamming\"` are also supported. By default, `\"nearest\"` is used.\n",
    "            validate_filenames: Boolean, whether to validate image filenames in\n",
    "                `x_col`. If `True`, invalid images will be ignored. Disabling this\n",
    "                option can lead to speed-up in the execution of this function.\n",
    "                Default: `True`.\n",
    "        # Returns\n",
    "            A `DataFrameIterator` yielding tuples of `(x, y)`\n",
    "            where `x` is a NumPy array containing a batch\n",
    "            of images with shape `(batch_size, *target_size, channels)`\n",
    "            and `y` is a NumPy array of corresponding labels.\n",
    "        \"\"\"\n",
    "        if 'has_ext' in kwargs:\n",
    "            warnings.warn('has_ext is deprecated, filenames in the dataframe have '\n",
    "                          'to match the exact filenames in disk.',\n",
    "                          DeprecationWarning)\n",
    "        if 'sort' in kwargs:\n",
    "            warnings.warn('sort is deprecated, batches will be created in the'\n",
    "                          'same order than the filenames provided if shuffle'\n",
    "                          'is set to False.', DeprecationWarning)\n",
    "        if class_mode == 'other':\n",
    "            warnings.warn('`class_mode` \"other\" is deprecated, please use '\n",
    "                          '`class_mode` \"raw\".', DeprecationWarning)\n",
    "            class_mode = 'raw'\n",
    "        if 'drop_duplicates' in kwargs:\n",
    "            warnings.warn('drop_duplicates is deprecated, you can drop duplicates '\n",
    "                          'by using the pandas.DataFrame.drop_duplicates method.',\n",
    "                          DeprecationWarning)\n",
    "\n",
    "        return DataFrameIterator(\n",
    "            dataframe,\n",
    "            directory,\n",
    "            self,\n",
    "            x_col=x_col,\n",
    "            y_col=y_col,\n",
    "            weight_col=weight_col,\n",
    "            target_size=target_size,\n",
    "            color_mode=color_mode,\n",
    "            classes=classes,\n",
    "            class_mode=class_mode,\n",
    "            data_format=self.data_format,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format,\n",
    "            subset=subset,\n",
    "            interpolation=interpolation,\n",
    "            validate_filenames=validate_filenames,\n",
    "            dtype=self.dtype\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0415ad83-e8ea-42f0-af3f-7c72baacf2fa",
   "metadata": {},
   "source": [
    "\"\"\"Utilities for real-time data augmentation on image data.\n",
    "\"\"\"\n",
    "import os\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from .iterator import BatchFromFilesMixin, Iterator\n",
    "from .utils import validate_filename\n",
    "\n",
    "\n",
    "class DataFrameIterator(BatchFromFilesMixin, Iterator):\n",
    "    \"\"\"Iterator capable of reading images from a directory on disk\n",
    "        through a dataframe.\n",
    "    # Arguments\n",
    "        dataframe: Pandas dataframe containing the filepaths relative to\n",
    "            `directory` (or absolute paths if `directory` is None) of the\n",
    "            images in a string column. It should include other column/s\n",
    "            depending on the `class_mode`:\n",
    "            - if `class_mode` is `\"categorical\"` (default value) it must\n",
    "                include the `y_col` column with the class/es of each image.\n",
    "                Values in column can be string/list/tuple if a single class\n",
    "                or list/tuple if multiple classes.\n",
    "            - if `class_mode` is `\"binary\"` or `\"sparse\"` it must include\n",
    "                the given `y_col` column with class values as strings.\n",
    "            - if `class_mode` is `\"raw\"` or `\"multi_output\"` it should contain\n",
    "                the columns specified in `y_col`.\n",
    "            - if `class_mode` is `\"input\"` or `None` no extra column is needed.\n",
    "        directory: string, path to the directory to read images from. If `None`,\n",
    "            data in `x_col` column should be absolute paths.\n",
    "        image_data_generator: Instance of `ImageDataGenerator` to use for\n",
    "            random transformations and normalization. If None, no transformations\n",
    "            and normalizations are made.\n",
    "        x_col: string, column in `dataframe` that contains the filenames (or\n",
    "            absolute paths if `directory` is `None`).\n",
    "        y_col: string or list, column/s in `dataframe` that has the target data.\n",
    "        weight_col: string, column in `dataframe` that contains the sample\n",
    "            weights. Default: `None`.\n",
    "        target_size: tuple of integers, dimensions to resize input images to.\n",
    "        color_mode: One of `\"rgb\"`, `\"rgba\"`, `\"grayscale\"`.\n",
    "            Color mode to read images.\n",
    "        classes: Optional list of strings, classes to use (e.g. `[\"dogs\", \"cats\"]`).\n",
    "            If None, all classes in `y_col` will be used.\n",
    "        class_mode: one of \"binary\", \"categorical\", \"input\", \"multi_output\",\n",
    "            \"raw\", \"sparse\" or None. Default: \"categorical\".\n",
    "            Mode for yielding the targets:\n",
    "            - `\"binary\"`: 1D numpy array of binary labels,\n",
    "            - `\"categorical\"`: 2D numpy array of one-hot encoded labels.\n",
    "                Supports multi-label output.\n",
    "            - `\"input\"`: images identical to input images (mainly used to\n",
    "                work with autoencoders),\n",
    "            - `\"multi_output\"`: list with the values of the different columns,\n",
    "            - `\"raw\"`: numpy array of values in `y_col` column(s),\n",
    "            - `\"sparse\"`: 1D numpy array of integer labels,\n",
    "            - `None`, no targets are returned (the generator will only yield\n",
    "                batches of image data, which is useful to use in\n",
    "                `model.predict_generator()`).\n",
    "        batch_size: Integer, size of a batch.\n",
    "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
    "        seed: Random seed for data shuffling.\n",
    "        data_format: String, one of `channels_first`, `channels_last`.\n",
    "        save_to_dir: Optional directory where to save the pictures\n",
    "            being yielded, in a viewable format. This is useful\n",
    "            for visualizing the random transformations being\n",
    "            applied, for debugging purposes.\n",
    "        save_prefix: String prefix to use for saving sample\n",
    "            images (if `save_to_dir` is set).\n",
    "        save_format: Format to use for saving sample images\n",
    "            (if `save_to_dir` is set).\n",
    "        subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
    "            validation_split is set in ImageDataGenerator.\n",
    "        interpolation: Interpolation method used to resample the image if the\n",
    "            target size is different from that of the loaded image.\n",
    "            Supported methods are \"nearest\", \"bilinear\", and \"bicubic\".\n",
    "            If PIL version 1.1.3 or newer is installed, \"lanczos\" is also\n",
    "            supported. If PIL version 3.4.0 or newer is installed, \"box\" and\n",
    "            \"hamming\" are also supported. By default, \"nearest\" is used.\n",
    "        keep_aspect_ratio: Boolean, whether to resize images to a target size\n",
    "            without aspect ratio distortion. The image is cropped in the center\n",
    "            with target aspect ratio before resizing.\n",
    "        dtype: Dtype to use for the generated arrays.\n",
    "        validate_filenames: Boolean, whether to validate image filenames in\n",
    "        `x_col`. If `True`, invalid images will be ignored. Disabling this option\n",
    "        can lead to speed-up in the instantiation of this class. Default: `True`.\n",
    "    \"\"\"\n",
    "    allowed_class_modes = {\n",
    "        'binary', 'categorical', 'input', 'multi_output', 'raw', 'sparse', None\n",
    "    }\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        try:\n",
    "            from tensorflow.keras.utils import Sequence as TFSequence\n",
    "            if TFSequence not in cls.__bases__:\n",
    "                cls.__bases__ = cls.__bases__ + (TFSequence,)\n",
    "        except ImportError:\n",
    "            pass\n",
    "        return super(DataFrameIterator, cls).__new__(cls)\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataframe,\n",
    "                 directory=None,\n",
    "                 image_data_generator=None,\n",
    "                 x_col=\"filename\",\n",
    "                 y_col=\"class\",\n",
    "                 weight_col=None,\n",
    "                 target_size=(256, 256),\n",
    "                 color_mode='rgb',\n",
    "                 classes=None,\n",
    "                 class_mode='categorical',\n",
    "                 batch_size=32,\n",
    "                 shuffle=True,\n",
    "                 seed=None,\n",
    "                 data_format='channels_last',\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='png',\n",
    "                 subset=None,\n",
    "                 interpolation='nearest',\n",
    "                 keep_aspect_ratio=False,\n",
    "                 dtype='float32',\n",
    "                 validate_filenames=True):\n",
    "\n",
    "        super(DataFrameIterator, self).set_processing_attrs(image_data_generator,\n",
    "                                                            target_size,\n",
    "                                                            color_mode,\n",
    "                                                            data_format,\n",
    "                                                            save_to_dir,\n",
    "                                                            save_prefix,\n",
    "                                                            save_format,\n",
    "                                                            subset,\n",
    "                                                            interpolation,\n",
    "                                                            keep_aspect_ratio)\n",
    "        df = dataframe.copy()\n",
    "        self.directory = directory or ''\n",
    "        self.class_mode = class_mode\n",
    "        self.dtype = dtype\n",
    "        # check that inputs match the required class_mode\n",
    "        self._check_params(df, x_col, y_col, weight_col, classes)\n",
    "        if validate_filenames:  # check which image files are valid and keep them\n",
    "            df = self._filter_valid_filepaths(df, x_col)\n",
    "        if class_mode not in [\"input\", \"multi_output\", \"raw\", None]:\n",
    "            df, classes = self._filter_classes(df, y_col, classes)\n",
    "            num_classes = len(classes)\n",
    "            # build an index of all the unique classes\n",
    "            self.class_indices = dict(zip(classes, range(len(classes))))\n",
    "        # retrieve only training or validation set\n",
    "        if self.split:\n",
    "            num_files = len(df)\n",
    "            start = int(self.split[0] * num_files)\n",
    "            stop = int(self.split[1] * num_files)\n",
    "            df = df.iloc[start: stop, :]\n",
    "        # get labels for each observation\n",
    "        if class_mode not in [\"input\", \"multi_output\", \"raw\", None]:\n",
    "            self.classes = self.get_classes(df, y_col)\n",
    "        self.filenames = df[x_col].tolist()\n",
    "        self._sample_weight = df[weight_col].values if weight_col else None\n",
    "\n",
    "        if class_mode == \"multi_output\":\n",
    "            self._targets = [np.array(df[col].tolist()) for col in y_col]\n",
    "        if class_mode == \"raw\":\n",
    "            self._targets = df[y_col].values\n",
    "        self.samples = len(self.filenames)\n",
    "        validated_string = 'validated' if validate_filenames else 'non-validated'\n",
    "        if class_mode in [\"input\", \"multi_output\", \"raw\", None]:\n",
    "            print('Found {} {} image filenames.'\n",
    "                  .format(self.samples, validated_string))\n",
    "        else:\n",
    "            print('Found {} {} image filenames belonging to {} classes.'\n",
    "                  .format(self.samples, validated_string, num_classes))\n",
    "        self._filepaths = [\n",
    "            os.path.join(self.directory, fname) for fname in self.filenames\n",
    "        ]\n",
    "        super(DataFrameIterator, self).__init__(self.samples,\n",
    "                                                batch_size,\n",
    "                                                shuffle,\n",
    "                                                seed)\n",
    "\n",
    "    def _check_params(self, df, x_col, y_col, weight_col, classes):\n",
    "        # check class mode is one of the currently supported\n",
    "        if self.class_mode not in self.allowed_class_modes:\n",
    "            raise ValueError('Invalid class_mode: {}; expected one of: {}'\n",
    "                             .format(self.class_mode, self.allowed_class_modes))\n",
    "        # check that y_col has several column names if class_mode is multi_output\n",
    "        if (self.class_mode == 'multi_output') and not isinstance(y_col, list):\n",
    "            raise TypeError(\n",
    "                'If class_mode=\"{}\", y_col must be a list. Received {}.'\n",
    "                .format(self.class_mode, type(y_col).__name__)\n",
    "            )\n",
    "        # check that filenames/filepaths column values are all strings\n",
    "        if not all(df[x_col].apply(lambda x: isinstance(x, str))):\n",
    "            raise TypeError('All values in column x_col={} must be strings.'\n",
    "                            .format(x_col))\n",
    "        # check labels are string if class_mode is binary or sparse\n",
    "        if self.class_mode in {'binary', 'sparse'}:\n",
    "            if not all(df[y_col].apply(lambda x: isinstance(x, str))):\n",
    "                raise TypeError('If class_mode=\"{}\", y_col=\"{}\" column '\n",
    "                                'values must be strings.'\n",
    "                                .format(self.class_mode, y_col))\n",
    "        # check that if binary there are only 2 different classes\n",
    "        if self.class_mode == 'binary':\n",
    "            if classes:\n",
    "                classes = set(classes)\n",
    "                if len(classes) != 2:\n",
    "                    raise ValueError('If class_mode=\"binary\" there must be 2 '\n",
    "                                     'classes. {} class/es were given.'\n",
    "                                     .format(len(classes)))\n",
    "            elif df[y_col].nunique() != 2:\n",
    "                raise ValueError('If class_mode=\"binary\" there must be 2 classes. '\n",
    "                                 'Found {} classes.'.format(df[y_col].nunique()))\n",
    "        # check values are string, list or tuple if class_mode is categorical\n",
    "        if self.class_mode == 'categorical':\n",
    "            types = (str, list, tuple)\n",
    "            if not all(df[y_col].apply(lambda x: isinstance(x, types))):\n",
    "                raise TypeError('If class_mode=\"{}\", y_col=\"{}\" column '\n",
    "                                'values must be type string, list or tuple.'\n",
    "                                .format(self.class_mode, y_col))\n",
    "        # raise warning if classes are given but will be unused\n",
    "        if classes and self.class_mode in {\"input\", \"multi_output\", \"raw\", None}:\n",
    "            warnings.warn('`classes` will be ignored given the class_mode=\"{}\"'\n",
    "                          .format(self.class_mode))\n",
    "        # check that if weight column that the values are numerical\n",
    "        if weight_col and not issubclass(df[weight_col].dtype.type, np.number):\n",
    "            raise TypeError('Column weight_col={} must be numeric.'\n",
    "                            .format(weight_col))\n",
    "\n",
    "    def get_classes(self, df, y_col):\n",
    "        labels = []\n",
    "        for label in df[y_col]:\n",
    "            if isinstance(label, (list, tuple)):\n",
    "                labels.append([self.class_indices[lbl] for lbl in label])\n",
    "            else:\n",
    "                labels.append(self.class_indices[label])\n",
    "        return labels\n",
    "\n",
    "    @staticmethod\n",
    "    def _filter_classes(df, y_col, classes):\n",
    "        df = df.copy()\n",
    "\n",
    "        def remove_classes(labels, classes):\n",
    "            if isinstance(labels, (list, tuple)):\n",
    "                labels = [cls for cls in labels if cls in classes]\n",
    "                return labels or None\n",
    "            elif isinstance(labels, str):\n",
    "                return labels if labels in classes else None\n",
    "            else:\n",
    "                raise TypeError(\n",
    "                    \"Expect string, list or tuple but found {} in {} column \"\n",
    "                    .format(type(labels), y_col)\n",
    "                )\n",
    "\n",
    "        if classes:\n",
    "            # prepare for membership lookup\n",
    "            classes = list(OrderedDict.fromkeys(classes).keys())\n",
    "            df[y_col] = df[y_col].apply(lambda x: remove_classes(x, classes))\n",
    "        else:\n",
    "            classes = set()\n",
    "            for v in df[y_col]:\n",
    "                if isinstance(v, (list, tuple)):\n",
    "                    classes.update(v)\n",
    "                else:\n",
    "                    classes.add(v)\n",
    "            classes = sorted(classes)\n",
    "        return df.dropna(subset=[y_col]), classes\n",
    "\n",
    "    def _filter_valid_filepaths(self, df, x_col):\n",
    "        \"\"\"Keep only dataframe rows with valid filenames\n",
    "        # Arguments\n",
    "            df: Pandas dataframe containing filenames in a column\n",
    "            x_col: string, column in `df` that contains the filenames or filepaths\n",
    "        # Returns\n",
    "            absolute paths to image files\n",
    "        \"\"\"\n",
    "        filepaths = df[x_col].map(\n",
    "            lambda fname: os.path.join(self.directory, fname)\n",
    "        )\n",
    "        mask = filepaths.apply(validate_filename, args=(self.white_list_formats,))\n",
    "        n_invalid = (~mask).sum()\n",
    "        if n_invalid:\n",
    "            warnings.warn(\n",
    "                'Found {} invalid image filename(s) in x_col=\"{}\". '\n",
    "                'These filename(s) will be ignored.'\n",
    "                .format(n_invalid, x_col)\n",
    "            )\n",
    "        return df[mask]\n",
    "\n",
    "    @property\n",
    "    def filepaths(self):\n",
    "        return self._filepaths\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        if self.class_mode in {\"multi_output\", \"raw\"}:\n",
    "            return self._targets\n",
    "        else:\n",
    "            return self.classes\n",
    "\n",
    "    @property\n",
    "    def sample_weight(self):\n",
    "        return self._sample_weight"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f606ee1-be74-4bad-8ac7-7f71faf4010b",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/image/iterator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "19a8004a-07d0-47b6-8b87-2d70c9c5b0b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matlab.engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yr/qwqgmggj2_12p38pnv1hf6380000gp/T/ipykernel_5075/1620819283.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matlab.engine'"
     ]
    }
   ],
   "source": [
    "import matlab.engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee961c0-0365-415e-919f-e1a38ffe7d85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
