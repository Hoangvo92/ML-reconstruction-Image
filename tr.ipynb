{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2b25b451-e7e8-476a-8910-cf745809ef08",
   "metadata": {},
   "source": [
    "get TR from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f951d4e-51fb-4521-81cd-48eb1c5e2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\"\"\"Import from keras_preprocessing not from keras.preprocessing, because Keras may or maynot contain the features discussed here depending upon when you read this article, until the keras_preprocessed library is updated in Keras use the github version.\"\"\"\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "#from tensorflow.keras import optimizers #., optimizers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#Importing all the relevant library\n",
    "%matplotlib inline\n",
    "import h5py, os\n",
    "#from functions import transforms as T\n",
    "#from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "#from functions import transforms as T \n",
    "#from functions.subsample import MaskFunc\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8d08e5-d611-4b99-add9-02cc854a9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path, val_data_path, test_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_val_test = ['train', 'val', 'test']\n",
    "    data_path = [train_data_path, val_data_path, test_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)):\n",
    "\n",
    "        data_list[train_val_test[i]] = []\n",
    "        \n",
    "        which_data_path = data_path[i]\n",
    "        tr = 0\n",
    "        te = 0\n",
    "        alfa = 0\n",
    "        for fname in sorted(os.listdir(which_data_path + '/images')):\n",
    "            if fname != \".DS_Store\":\n",
    "\n",
    "            \n",
    "                subject_data_path = os.path.join(which_data_path + '/images', fname)\n",
    "                     \n",
    "                if not os.path.isfile(subject_data_path): continue \n",
    "            \n",
    "          #  im_frame = Image.open(subject_data_path)\n",
    "\n",
    "            #get information from text file\n",
    "            # this will return a tuple of root and extension\n",
    "                split_tup = os.path.splitext(fname)\n",
    "\n",
    "  \n",
    "            # extract the file name and extension\n",
    "                file_name = split_tup[0]\n",
    "                file_path = os.path.join(which_data_path + '/texts', file_name) + '.txt'\n",
    "                f = open(os.path.join(which_data_path + '/texts', file_name) + '.txt', 'r')\n",
    "                line = f.readlines()[1]\n",
    "            \n",
    "                fields = line.split(',')\n",
    "                tr = int(fields[3])\n",
    "                te = int(fields[4])\n",
    "                alfa = int(fields[5])\n",
    "                f.close()\n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "                data_list[train_val_test[i]] += [(fname, tr)]\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e859bb9f-fa84-4609-bf9e-e2decd504740",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = load_data_path (\"data/train\", \"data/val\", \"data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e6528b-7dbc-44ca-86f3-810e789cba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_list['train']\n",
    "val_data = data_list['val']\n",
    "test_data = data_list['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08aa5111-5a82-4237-ba6c-72ee17a69ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_data, columns=['fnames', 'labels'])\n",
    "train_df['labels']= train_df['labels'].astype(str)\n",
    "train_df['fnames']= train_df['fnames'].astype(str)\n",
    "val_df = pd.DataFrame(val_data, columns=['fnames', 'labels'])\n",
    "val_df['labels']= val_df['labels'].astype(str)\n",
    "val_df['fnames']= val_df['fnames'].astype(str)\n",
    "test_df = pd.DataFrame(test_data, columns=['fnames', 'labels'])\n",
    "test_df['labels']= test_df['labels'].astype(str)\n",
    "test_df['fnames']= test_df['fnames'].astype(str)\n",
    "labels = train_df.labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce8641ab-96af-4fea-8b4e-2b4e38f108a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(labels)\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3c7e74a-edb4-4d1a-b1f4-52addf03c259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of         fnames labels\n",
       "0        1.png      5\n",
       "1       10.png      5\n",
       "2      100.png      5\n",
       "3     1000.png     20\n",
       "4     1001.png     16\n",
       "...        ...    ...\n",
       "3060   995.png      2\n",
       "3061   996.png     18\n",
       "3062   997.png     17\n",
       "3063   998.png     16\n",
       "3064   999.png      6\n",
       "\n",
       "[3065 rows x 2 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beca63cc-5587-4daf-b728-00942ebfa873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3065 validated image filenames belonging to 20 classes.\n",
      "Found 655 validated image filenames belonging to 20 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "      dataframe=train_df,\n",
    "      directory=\"./data/train/images/\",\n",
    "      x_col=\"fnames\",\n",
    "      y_col=\"labels\",\n",
    "      batch_size=64,\n",
    "      seed=42,\n",
    "      shuffle=True,\n",
    "      class_mode=\"other\",\n",
    "      target_size=(224,224))\n",
    "\n",
    "\n",
    "\n",
    "valid_generator=test_datagen.flow_from_dataframe(\n",
    "      dataframe=val_df,\n",
    "      directory=\"./data/val/images/\",\n",
    "      x_col=\"fnames\",\n",
    "      y_col=\"labels\",\n",
    "      batch_size=64,\n",
    "      seed=42,\n",
    "      shuffle=True,\n",
    "      class_mode=\"other\",\n",
    "     target_size=(224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49c790a0-c070-46fc-a219-c03f065e41d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 646 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/keras_preprocessing/image/dataframe_iterator.py:282: UserWarning: Found 9 invalid image filename(s) in x_col=\"fnames\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n"
     ]
    }
   ],
   "source": [
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "dataframe=test_df,\n",
    "      directory=\"./data/test/images\",\n",
    "      x_col=\"fnames\",\n",
    "      batch_size=1,\n",
    "      seed=42,\n",
    "      shuffle=False,\n",
    "      class_mode=None,\n",
    "      target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4403221-0e45-4cc5-9cc8-6e84aa7d76e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-04 17:56:24.297918: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(224,224,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_class, activation='linear'))\n",
    "opt = SGD(lr=0.001, momentum=0.9)\n",
    "#opt =tf.keras.optimizers.RMSprop(lr=0.001, decay=1e-6)\n",
    "model.compile(optimizer=opt,loss=\"mean_squared_error\",metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5ee260d-ecc7-46a5-988f-9ca7b092e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100 #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f69b861c-1472-444a-a2ee-c158e0a96e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
      "2021-12-04 17:56:45.475037: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "47/47 [==============================] - 314s 7s/step - loss: 0.4504 - accuracy: 0.0487 - val_loss: 0.2084 - val_accuracy: 0.0531\n",
      "Epoch 2/300\n",
      "47/47 [==============================] - 298s 6s/step - loss: 0.2181 - accuracy: 0.0474 - val_loss: 0.2131 - val_accuracy: 0.0453\n",
      "Epoch 3/300\n",
      "47/47 [==============================] - 330s 7s/step - loss: 0.2119 - accuracy: 0.0557 - val_loss: 0.2054 - val_accuracy: 0.0500\n",
      "Epoch 4/300\n",
      "47/47 [==============================] - 285s 6s/step - loss: 0.2077 - accuracy: 0.0701 - val_loss: 0.2016 - val_accuracy: 0.0344\n",
      "Epoch 5/300\n",
      "47/47 [==============================] - 327s 7s/step - loss: 0.2037 - accuracy: 0.0739 - val_loss: 0.2071 - val_accuracy: 0.0531\n",
      "Epoch 6/300\n",
      "47/47 [==============================] - 257s 5s/step - loss: 0.2008 - accuracy: 0.0825 - val_loss: 0.2015 - val_accuracy: 0.0500\n",
      "Epoch 7/300\n",
      "47/47 [==============================] - 280s 6s/step - loss: 0.1995 - accuracy: 0.0973 - val_loss: 0.2065 - val_accuracy: 0.0547\n",
      "Epoch 8/300\n",
      "47/47 [==============================] - 257s 5s/step - loss: 0.1958 - accuracy: 0.0954 - val_loss: 0.2057 - val_accuracy: 0.0500\n",
      "Epoch 9/300\n",
      "47/47 [==============================] - 280s 6s/step - loss: 0.1930 - accuracy: 0.1080 - val_loss: 0.2151 - val_accuracy: 0.0594\n",
      "Epoch 10/300\n",
      "47/47 [==============================] - 264s 6s/step - loss: 0.1914 - accuracy: 0.1113 - val_loss: 0.2092 - val_accuracy: 0.0562\n",
      "Epoch 11/300\n",
      "47/47 [==============================] - 280s 6s/step - loss: 0.1888 - accuracy: 0.1424 - val_loss: 0.2111 - val_accuracy: 0.0750\n",
      "Epoch 12/300\n",
      "47/47 [==============================] - 286s 6s/step - loss: 0.1835 - accuracy: 0.1614 - val_loss: 0.2163 - val_accuracy: 0.0578\n",
      "Epoch 13/300\n",
      "47/47 [==============================] - 264s 6s/step - loss: 0.1815 - accuracy: 0.1765 - val_loss: 0.2211 - val_accuracy: 0.0594\n",
      "Epoch 14/300\n",
      "47/47 [==============================] - 292s 6s/step - loss: 0.1884 - accuracy: 0.1838 - val_loss: 0.2198 - val_accuracy: 0.0516\n",
      "Epoch 15/300\n",
      "47/47 [==============================] - 259s 6s/step - loss: 0.2034 - accuracy: 0.2172 - val_loss: 0.2345 - val_accuracy: 0.0484\n",
      "Epoch 16/300\n",
      "47/47 [==============================] - 292s 6s/step - loss: 0.1702 - accuracy: 0.2224 - val_loss: 0.2295 - val_accuracy: 0.0547\n",
      "Epoch 17/300\n",
      "47/47 [==============================] - 256s 5s/step - loss: 0.1642 - accuracy: 0.2561 - val_loss: 0.2379 - val_accuracy: 0.0500\n",
      "Epoch 18/300\n",
      "47/47 [==============================] - 284s 6s/step - loss: 0.1618 - accuracy: 0.2694 - val_loss: 0.2436 - val_accuracy: 0.0562\n",
      "Epoch 19/300\n",
      "47/47 [==============================] - 262s 6s/step - loss: 0.1578 - accuracy: 0.2909 - val_loss: 0.2660 - val_accuracy: 0.0531\n",
      "Epoch 20/300\n",
      "47/47 [==============================] - 273s 6s/step - loss: 0.2163 - accuracy: 0.2955 - val_loss: 0.2532 - val_accuracy: 0.0578\n",
      "Epoch 21/300\n",
      "47/47 [==============================] - 294s 6s/step - loss: 0.1505 - accuracy: 0.3158 - val_loss: 0.2589 - val_accuracy: 0.0625\n",
      "Epoch 22/300\n",
      "47/47 [==============================] - 255s 5s/step - loss: 0.1505 - accuracy: 0.3258 - val_loss: 0.2587 - val_accuracy: 0.0531\n",
      "Epoch 23/300\n",
      "47/47 [==============================] - 278s 6s/step - loss: 0.1476 - accuracy: 0.3501 - val_loss: 0.2691 - val_accuracy: 0.0625\n",
      "Epoch 24/300\n",
      "47/47 [==============================] - 255s 5s/step - loss: 0.1454 - accuracy: 0.3405 - val_loss: 0.2791 - val_accuracy: 0.0578\n",
      "Epoch 25/300\n",
      "47/47 [==============================] - 279s 6s/step - loss: 0.1410 - accuracy: 0.3605 - val_loss: 0.2794 - val_accuracy: 0.0516\n",
      "Epoch 26/300\n",
      "47/47 [==============================] - 258s 5s/step - loss: 0.1440 - accuracy: 0.3633 - val_loss: 0.2856 - val_accuracy: 0.0578\n",
      "Epoch 27/300\n",
      "47/47 [==============================] - 284s 6s/step - loss: 0.1378 - accuracy: 0.3814 - val_loss: 0.2790 - val_accuracy: 0.0453\n",
      "Epoch 28/300\n",
      "47/47 [==============================] - 267s 6s/step - loss: 0.1349 - accuracy: 0.4020 - val_loss: 0.3012 - val_accuracy: 0.0516\n",
      "Epoch 29/300\n",
      "47/47 [==============================] - 269s 6s/step - loss: 0.1312 - accuracy: 0.4078 - val_loss: 0.3106 - val_accuracy: 0.0516\n",
      "Epoch 30/300\n",
      "47/47 [==============================] - 281s 6s/step - loss: 0.1296 - accuracy: 0.4261 - val_loss: 0.3122 - val_accuracy: 0.0500\n",
      "Epoch 31/300\n",
      "47/47 [==============================] - 264s 6s/step - loss: 0.1292 - accuracy: 0.4319 - val_loss: 0.3242 - val_accuracy: 0.0484\n",
      "Epoch 32/300\n",
      "47/47 [==============================] - 293s 6s/step - loss: 0.1223 - accuracy: 0.4486 - val_loss: 0.3397 - val_accuracy: 0.0531\n",
      "Epoch 33/300\n",
      "47/47 [==============================] - 263s 6s/step - loss: 0.1623 - accuracy: 0.4483 - val_loss: 0.3718 - val_accuracy: 0.0437\n",
      "Epoch 34/300\n",
      "47/47 [==============================] - 283s 6s/step - loss: 0.1240 - accuracy: 0.4414 - val_loss: 0.3672 - val_accuracy: 0.0422\n",
      "Epoch 35/300\n",
      "47/47 [==============================] - 267s 6s/step - loss: 0.1156 - accuracy: 0.4873 - val_loss: 0.3736 - val_accuracy: 0.0516\n",
      "Epoch 36/300\n",
      "47/47 [==============================] - 280s 6s/step - loss: 0.1171 - accuracy: 0.4800 - val_loss: 0.3916 - val_accuracy: 0.0516\n",
      "Epoch 37/300\n",
      "47/47 [==============================] - 269s 6s/step - loss: 0.1101 - accuracy: 0.5153 - val_loss: 0.3930 - val_accuracy: 0.0547\n",
      "Epoch 38/300\n",
      "47/47 [==============================] - 273s 6s/step - loss: 0.1098 - accuracy: 0.5121 - val_loss: 0.3693 - val_accuracy: 0.0500\n",
      "Epoch 39/300\n",
      "47/47 [==============================] - 287s 6s/step - loss: 0.1128 - accuracy: 0.5086 - val_loss: 0.4367 - val_accuracy: 0.0453\n",
      "Epoch 40/300\n",
      "47/47 [==============================] - 260s 6s/step - loss: 0.1107 - accuracy: 0.5034 - val_loss: 0.4216 - val_accuracy: 0.0578\n",
      "Epoch 41/300\n",
      "47/47 [==============================] - 290s 6s/step - loss: 0.1114 - accuracy: 0.5114 - val_loss: 0.3631 - val_accuracy: 0.0484\n",
      "Epoch 42/300\n",
      "47/47 [==============================] - 258s 5s/step - loss: 0.1089 - accuracy: 0.5235 - val_loss: 0.4154 - val_accuracy: 0.0484\n",
      "Epoch 43/300\n",
      "47/47 [==============================] - 289s 6s/step - loss: 0.1037 - accuracy: 0.5504 - val_loss: 0.3630 - val_accuracy: 0.0547\n",
      "Epoch 44/300\n",
      "47/47 [==============================] - 258s 5s/step - loss: 0.1062 - accuracy: 0.5385 - val_loss: 0.3429 - val_accuracy: 0.0391\n",
      "Epoch 45/300\n",
      "47/47 [==============================] - 280s 6s/step - loss: 0.1059 - accuracy: 0.5281 - val_loss: 0.4474 - val_accuracy: 0.0562\n",
      "Epoch 46/300\n",
      "47/47 [==============================] - 267s 6s/step - loss: 0.1029 - accuracy: 0.5522 - val_loss: 0.4971 - val_accuracy: 0.0578\n",
      "Epoch 47/300\n",
      "47/47 [==============================] - 278s 6s/step - loss: 0.1022 - accuracy: 0.5533 - val_loss: 0.4589 - val_accuracy: 0.0562\n",
      "Epoch 48/300\n",
      "47/47 [==============================] - 274s 6s/step - loss: 0.1017 - accuracy: 0.5492 - val_loss: 0.3853 - val_accuracy: 0.0516\n",
      "Epoch 49/300\n",
      "47/47 [==============================] - 267s 6s/step - loss: 0.0992 - accuracy: 0.5604 - val_loss: 0.3743 - val_accuracy: 0.0547\n",
      "Epoch 50/300\n",
      "47/47 [==============================] - 285s 6s/step - loss: 0.1001 - accuracy: 0.5635 - val_loss: 0.3850 - val_accuracy: 0.0547\n",
      "Epoch 51/300\n",
      "47/47 [==============================] - 257s 5s/step - loss: 0.0951 - accuracy: 0.5800 - val_loss: 0.3770 - val_accuracy: 0.0531\n",
      "Epoch 52/300\n",
      "47/47 [==============================] - 287s 6s/step - loss: 0.0958 - accuracy: 0.5737 - val_loss: 0.4059 - val_accuracy: 0.0625\n",
      "Epoch 53/300\n",
      "47/47 [==============================] - 256s 5s/step - loss: 0.0955 - accuracy: 0.5711 - val_loss: 0.5391 - val_accuracy: 0.0453\n",
      "Epoch 54/300\n",
      "47/47 [==============================] - 282s 6s/step - loss: 0.0963 - accuracy: 0.5715 - val_loss: 0.4069 - val_accuracy: 0.0531\n",
      "Epoch 55/300\n",
      "47/47 [==============================] - 261s 6s/step - loss: 0.0958 - accuracy: 0.5774 - val_loss: 0.3819 - val_accuracy: 0.0500\n",
      "Epoch 56/300\n",
      "47/47 [==============================] - 285s 6s/step - loss: 0.0985 - accuracy: 0.5591 - val_loss: 0.3870 - val_accuracy: 0.0500\n",
      "Epoch 57/300\n",
      "47/47 [==============================] - 271s 6s/step - loss: 0.0952 - accuracy: 0.5795 - val_loss: 0.5128 - val_accuracy: 0.0531\n",
      "Epoch 58/300\n",
      "47/47 [==============================] - 270s 6s/step - loss: 0.0955 - accuracy: 0.5837 - val_loss: 0.6751 - val_accuracy: 0.0437\n",
      "Epoch 59/300\n",
      "47/47 [==============================] - 281s 6s/step - loss: 0.0974 - accuracy: 0.5753 - val_loss: 0.4836 - val_accuracy: 0.0594\n",
      "Epoch 60/300\n",
      "47/47 [==============================] - 257s 5s/step - loss: 0.0936 - accuracy: 0.5826 - val_loss: 0.4132 - val_accuracy: 0.0500\n",
      "Epoch 61/300\n",
      "47/47 [==============================] - 280s 6s/step - loss: 0.0901 - accuracy: 0.5946 - val_loss: 0.3991 - val_accuracy: 0.0547\n",
      "Epoch 62/300\n",
      "47/47 [==============================] - 255s 5s/step - loss: 0.0962 - accuracy: 0.5702 - val_loss: 0.5423 - val_accuracy: 0.0469\n",
      "Epoch 63/300\n",
      "47/47 [==============================] - 286s 6s/step - loss: 0.0897 - accuracy: 0.6037 - val_loss: 0.3799 - val_accuracy: 0.0578\n",
      "Epoch 64/300\n",
      "47/47 [==============================] - 258s 5s/step - loss: 0.0899 - accuracy: 0.5938 - val_loss: 0.4842 - val_accuracy: 0.0453\n",
      "Epoch 65/300\n",
      "47/47 [==============================] - 278s 6s/step - loss: 0.0921 - accuracy: 0.5944 - val_loss: 0.5365 - val_accuracy: 0.0453\n",
      "Epoch 66/300\n",
      "47/47 [==============================] - 268s 6s/step - loss: 0.0907 - accuracy: 0.5924 - val_loss: 0.5267 - val_accuracy: 0.0531\n",
      "Epoch 67/300\n",
      "47/47 [==============================] - 274s 6s/step - loss: 0.0889 - accuracy: 0.6135 - val_loss: 0.4944 - val_accuracy: 0.0625\n",
      "Epoch 68/300\n",
      "47/47 [==============================] - 275s 6s/step - loss: 0.0892 - accuracy: 0.5976 - val_loss: 0.5210 - val_accuracy: 0.0500\n",
      "Epoch 69/300\n",
      "47/47 [==============================] - 254s 5s/step - loss: 0.0904 - accuracy: 0.6163 - val_loss: 0.5558 - val_accuracy: 0.0500\n",
      "Epoch 70/300\n",
      "47/47 [==============================] - 276s 6s/step - loss: 0.0902 - accuracy: 0.5965 - val_loss: 0.4953 - val_accuracy: 0.0609\n",
      "Epoch 71/300\n",
      "47/47 [==============================] - 256s 5s/step - loss: 0.0928 - accuracy: 0.5895 - val_loss: 0.4777 - val_accuracy: 0.0469\n",
      "Epoch 72/300\n",
      "47/47 [==============================] - 290s 6s/step - loss: 0.0879 - accuracy: 0.6117 - val_loss: 0.4952 - val_accuracy: 0.0578\n",
      "Epoch 73/300\n",
      "47/47 [==============================] - 257s 5s/step - loss: 0.0892 - accuracy: 0.6102 - val_loss: 0.5283 - val_accuracy: 0.0547\n",
      "Epoch 74/300\n",
      "47/47 [==============================] - 277s 6s/step - loss: 0.0887 - accuracy: 0.6083 - val_loss: 0.4319 - val_accuracy: 0.0453\n",
      "Epoch 75/300\n",
      "47/47 [==============================] - 269s 6s/step - loss: 0.0870 - accuracy: 0.6115 - val_loss: 0.3971 - val_accuracy: 0.0641\n",
      "Epoch 76/300\n",
      "47/47 [==============================] - 274s 6s/step - loss: 0.0874 - accuracy: 0.6101 - val_loss: 0.4269 - val_accuracy: 0.0547\n",
      "Epoch 77/300\n",
      "47/47 [==============================] - 276s 6s/step - loss: 0.0873 - accuracy: 0.6234 - val_loss: 0.5490 - val_accuracy: 0.0531\n",
      "Epoch 78/300\n",
      "47/47 [==============================] - 256s 5s/step - loss: 0.0892 - accuracy: 0.6116 - val_loss: 0.4462 - val_accuracy: 0.0484\n",
      "Epoch 79/300\n",
      "47/47 [==============================] - 291s 6s/step - loss: 0.0875 - accuracy: 0.6065 - val_loss: 0.5505 - val_accuracy: 0.0562\n",
      "Epoch 80/300\n",
      "47/47 [==============================] - 257s 5s/step - loss: 0.0870 - accuracy: 0.6143 - val_loss: 0.5778 - val_accuracy: 0.0359\n",
      "Epoch 81/300\n",
      "47/47 [==============================] - 281s 6s/step - loss: 0.0875 - accuracy: 0.6093 - val_loss: 0.3812 - val_accuracy: 0.0516\n",
      "Epoch 82/300\n",
      "47/47 [==============================] - 256s 5s/step - loss: 0.0878 - accuracy: 0.6123 - val_loss: 0.5936 - val_accuracy: 0.0609\n",
      "Epoch 83/300\n",
      "47/47 [==============================] - 279s 6s/step - loss: 0.0967 - accuracy: 0.6228 - val_loss: 0.6862 - val_accuracy: 0.0437\n",
      "Epoch 84/300\n",
      "47/47 [==============================] - 264s 6s/step - loss: 0.0872 - accuracy: 0.6156 - val_loss: 0.6128 - val_accuracy: 0.0500\n",
      "Epoch 85/300\n",
      "47/47 [==============================] - 271s 6s/step - loss: 0.0883 - accuracy: 0.6166 - val_loss: 0.5929 - val_accuracy: 0.0469\n",
      "Epoch 86/300\n",
      "47/47 [==============================] - 285s 6s/step - loss: 0.0847 - accuracy: 0.6249 - val_loss: 0.6244 - val_accuracy: 0.0516\n",
      "Epoch 87/300\n",
      "47/47 [==============================] - 255s 5s/step - loss: 0.0869 - accuracy: 0.6209 - val_loss: 0.7361 - val_accuracy: 0.0594\n",
      "Epoch 88/300\n",
      "47/47 [==============================] - 274s 6s/step - loss: 0.0846 - accuracy: 0.6223 - val_loss: 0.4693 - val_accuracy: 0.0547\n",
      "Epoch 89/300\n",
      "47/47 [==============================] - 252s 5s/step - loss: 0.0866 - accuracy: 0.6251 - val_loss: 0.6423 - val_accuracy: 0.0516\n",
      "Epoch 90/300\n",
      "47/47 [==============================] - 273s 6s/step - loss: 0.0848 - accuracy: 0.6282 - val_loss: 0.5663 - val_accuracy: 0.0562\n",
      "Epoch 91/300\n",
      "47/47 [==============================] - 253s 5s/step - loss: 0.0856 - accuracy: 0.6281 - val_loss: 0.4541 - val_accuracy: 0.0469\n",
      "Epoch 92/300\n",
      "47/47 [==============================] - 274s 6s/step - loss: 0.0816 - accuracy: 0.6414 - val_loss: 0.3911 - val_accuracy: 0.0578\n",
      "Epoch 93/300\n",
      "47/47 [==============================] - 261s 6s/step - loss: 0.0829 - accuracy: 0.6398 - val_loss: 0.4557 - val_accuracy: 0.0578\n",
      "Epoch 94/300\n",
      "47/47 [==============================] - 268s 6s/step - loss: 0.0853 - accuracy: 0.6327 - val_loss: 0.6369 - val_accuracy: 0.0719\n",
      "Epoch 95/300\n",
      "47/47 [==============================] - 273s 6s/step - loss: 0.0922 - accuracy: 0.6230 - val_loss: 0.5379 - val_accuracy: 0.0562\n",
      "Epoch 96/300\n",
      "47/47 [==============================] - 255s 5s/step - loss: 0.0825 - accuracy: 0.6342 - val_loss: 0.4995 - val_accuracy: 0.0609\n",
      "Epoch 97/300\n",
      "47/47 [==============================] - 275s 6s/step - loss: 0.0828 - accuracy: 0.6333 - val_loss: 0.5320 - val_accuracy: 0.0562\n",
      "Epoch 98/300\n",
      "47/47 [==============================] - 252s 5s/step - loss: 0.0854 - accuracy: 0.6375 - val_loss: 0.6001 - val_accuracy: 0.0609\n",
      "Epoch 99/300\n",
      "47/47 [==============================] - 271s 6s/step - loss: 0.0862 - accuracy: 0.6258 - val_loss: 0.4810 - val_accuracy: 0.0688\n",
      "Epoch 100/300\n",
      "47/47 [==============================] - 253s 5s/step - loss: 0.0829 - accuracy: 0.6418 - val_loss: 0.5272 - val_accuracy: 0.0578\n",
      "Epoch 101/300\n",
      "47/47 [==============================] - 272s 6s/step - loss: 0.0821 - accuracy: 0.6400 - val_loss: 0.7261 - val_accuracy: 0.0547\n",
      "Epoch 102/300\n",
      "47/47 [==============================] - 263s 6s/step - loss: 0.0849 - accuracy: 0.6332 - val_loss: 0.4373 - val_accuracy: 0.0547\n",
      "Epoch 103/300\n",
      "47/47 [==============================] - 270s 6s/step - loss: 0.0820 - accuracy: 0.6509 - val_loss: 0.6471 - val_accuracy: 0.0609\n",
      "Epoch 104/300\n",
      "47/47 [==============================] - 274s 6s/step - loss: 0.0805 - accuracy: 0.6484 - val_loss: 0.4491 - val_accuracy: 0.0578\n",
      "Epoch 105/300\n",
      "47/47 [==============================] - 252s 5s/step - loss: 0.0868 - accuracy: 0.6235 - val_loss: 0.6524 - val_accuracy: 0.0484\n",
      "Epoch 106/300\n",
      "47/47 [==============================] - 273s 6s/step - loss: 0.0836 - accuracy: 0.6385 - val_loss: 0.6048 - val_accuracy: 0.0484\n",
      "Epoch 107/300\n",
      "47/47 [==============================] - 267s 6s/step - loss: 0.0860 - accuracy: 0.6232 - val_loss: 0.7455 - val_accuracy: 0.0594\n",
      "Epoch 108/300\n",
      "47/47 [==============================] - 273s 6s/step - loss: 0.0829 - accuracy: 0.6292 - val_loss: 0.6138 - val_accuracy: 0.0469\n",
      "Epoch 109/300\n",
      "47/47 [==============================] - 283s 6s/step - loss: 0.0825 - accuracy: 0.6343 - val_loss: 0.5229 - val_accuracy: 0.0547\n",
      "Epoch 110/300\n",
      "47/47 [==============================] - 271s 6s/step - loss: 0.0836 - accuracy: 0.6369 - val_loss: 0.5670 - val_accuracy: 0.0578\n",
      "Epoch 111/300\n",
      "47/47 [==============================] - 26477s 575s/step - loss: 0.0865 - accuracy: 0.6383 - val_loss: 0.5840 - val_accuracy: 0.0500\n",
      "Epoch 112/300\n",
      "47/47 [==============================] - 268s 6s/step - loss: 0.0814 - accuracy: 0.6458 - val_loss: 0.3849 - val_accuracy: 0.0594\n",
      "Epoch 113/300\n",
      "47/47 [==============================] - 287s 6s/step - loss: 0.0856 - accuracy: 0.6245 - val_loss: 0.6692 - val_accuracy: 0.0578\n",
      "Epoch 114/300\n",
      "47/47 [==============================] - 251s 5s/step - loss: 0.0831 - accuracy: 0.6300 - val_loss: 0.7651 - val_accuracy: 0.0547\n",
      "Epoch 115/300\n",
      "47/47 [==============================] - 278s 6s/step - loss: 0.0794 - accuracy: 0.6628 - val_loss: 0.3030 - val_accuracy: 0.0531\n",
      "Epoch 116/300\n",
      "47/47 [==============================] - 253s 5s/step - loss: 0.0849 - accuracy: 0.6313 - val_loss: 0.6114 - val_accuracy: 0.0469\n",
      "Epoch 117/300\n",
      "47/47 [==============================] - 275s 6s/step - loss: 0.0827 - accuracy: 0.6355 - val_loss: 0.4524 - val_accuracy: 0.0531\n",
      "Epoch 118/300\n",
      "47/47 [==============================] - 263s 6s/step - loss: 0.0847 - accuracy: 0.6424 - val_loss: 0.5811 - val_accuracy: 0.0500\n",
      "Epoch 119/300\n",
      "47/47 [==============================] - 271s 6s/step - loss: 0.0819 - accuracy: 0.6368 - val_loss: 0.4436 - val_accuracy: 0.0578\n",
      "Epoch 120/300\n",
      "47/47 [==============================] - 269s 6s/step - loss: 0.0835 - accuracy: 0.6381 - val_loss: 0.5871 - val_accuracy: 0.0547\n",
      "Epoch 121/300\n",
      "47/47 [==============================] - 261s 5s/step - loss: 0.0835 - accuracy: 0.6439 - val_loss: 0.3696 - val_accuracy: 0.0500\n",
      "Epoch 122/300\n",
      "47/47 [==============================] - 275s 6s/step - loss: 0.0805 - accuracy: 0.6412 - val_loss: 0.5491 - val_accuracy: 0.0609\n",
      "Epoch 123/300\n",
      "47/47 [==============================] - 252s 5s/step - loss: 0.0813 - accuracy: 0.6511 - val_loss: 0.4618 - val_accuracy: 0.0625\n",
      "Epoch 124/300\n",
      "47/47 [==============================] - 295s 6s/step - loss: 0.0862 - accuracy: 0.6384 - val_loss: 0.6648 - val_accuracy: 0.0516\n",
      "Epoch 125/300\n",
      "47/47 [==============================] - 253s 5s/step - loss: 0.0845 - accuracy: 0.6257 - val_loss: 0.6087 - val_accuracy: 0.0562\n",
      "Epoch 126/300\n",
      "47/47 [==============================] - 301s 6s/step - loss: 0.0846 - accuracy: 0.6199 - val_loss: 0.4652 - val_accuracy: 0.0578\n",
      "Epoch 127/300\n",
      "47/47 [==============================] - 257s 5s/step - loss: 0.0844 - accuracy: 0.6439 - val_loss: 0.3906 - val_accuracy: 0.0594\n",
      "Epoch 128/300\n",
      "47/47 [==============================] - 297s 6s/step - loss: 0.0858 - accuracy: 0.6379 - val_loss: 0.4789 - val_accuracy: 0.0531\n",
      "Epoch 129/300\n",
      "47/47 [==============================] - 259s 6s/step - loss: 0.0826 - accuracy: 0.6523 - val_loss: 0.6355 - val_accuracy: 0.0641\n",
      "Epoch 130/300\n",
      "47/47 [==============================] - 276s 6s/step - loss: 0.0832 - accuracy: 0.6345 - val_loss: 0.5943 - val_accuracy: 0.0531\n",
      "Epoch 131/300\n",
      "47/47 [==============================] - 268s 6s/step - loss: 0.0804 - accuracy: 0.6548 - val_loss: 0.6636 - val_accuracy: 0.0531\n",
      "Epoch 132/300\n",
      "47/47 [==============================] - 270s 6s/step - loss: 0.0854 - accuracy: 0.6274 - val_loss: 0.6765 - val_accuracy: 0.0625\n",
      "Epoch 133/300\n",
      "47/47 [==============================] - 276s 6s/step - loss: 0.0825 - accuracy: 0.6509 - val_loss: 0.6182 - val_accuracy: 0.0500\n",
      "Epoch 134/300\n",
      "47/47 [==============================] - 260s 5s/step - loss: 0.0804 - accuracy: 0.6479 - val_loss: 0.5760 - val_accuracy: 0.0531\n",
      "Epoch 135/300\n",
      "47/47 [==============================] - 278s 6s/step - loss: 0.0813 - accuracy: 0.6486 - val_loss: 0.7113 - val_accuracy: 0.0609\n",
      "Epoch 136/300\n",
      "47/47 [==============================] - 253s 5s/step - loss: 0.0805 - accuracy: 0.6493 - val_loss: 0.6885 - val_accuracy: 0.0625\n",
      "Epoch 137/300\n",
      "47/47 [==============================] - 285s 6s/step - loss: 0.0863 - accuracy: 0.6357 - val_loss: 0.6533 - val_accuracy: 0.0531\n",
      "Epoch 138/300\n",
      "47/47 [==============================] - 254s 5s/step - loss: 0.0835 - accuracy: 0.6371 - val_loss: 0.5303 - val_accuracy: 0.0516\n",
      "Epoch 139/300\n",
      "47/47 [==============================] - 289s 6s/step - loss: 0.0813 - accuracy: 0.6523 - val_loss: 0.6570 - val_accuracy: 0.0562\n",
      "Epoch 140/300\n",
      "47/47 [==============================] - 261s 6s/step - loss: 0.0794 - accuracy: 0.6515 - val_loss: 0.5079 - val_accuracy: 0.0437\n",
      "Epoch 141/300\n",
      "47/47 [==============================] - 275s 6s/step - loss: 0.0871 - accuracy: 0.6286 - val_loss: 0.5525 - val_accuracy: 0.0484\n",
      "Epoch 142/300\n",
      "47/47 [==============================] - 269s 6s/step - loss: 0.0829 - accuracy: 0.6460 - val_loss: 0.5400 - val_accuracy: 0.0609\n",
      "Epoch 143/300\n",
      "47/47 [==============================] - 258s 5s/step - loss: 0.0821 - accuracy: 0.6528 - val_loss: 0.5043 - val_accuracy: 0.0531\n",
      "Epoch 144/300\n",
      "47/47 [==============================] - 273s 6s/step - loss: 0.0797 - accuracy: 0.6512 - val_loss: 0.6514 - val_accuracy: 0.0562\n",
      "Epoch 145/300\n",
      "47/47 [==============================] - 252s 5s/step - loss: 0.0827 - accuracy: 0.6538 - val_loss: 0.6197 - val_accuracy: 0.0531\n",
      "Epoch 146/300\n",
      "47/47 [==============================] - 273s 6s/step - loss: 0.0848 - accuracy: 0.6406 - val_loss: 0.5516 - val_accuracy: 0.0594\n",
      "Epoch 147/300\n",
      "47/47 [==============================] - 257s 5s/step - loss: 0.0813 - accuracy: 0.6549 - val_loss: 0.6704 - val_accuracy: 0.0625\n",
      "Epoch 148/300\n",
      "47/47 [==============================] - 289s 6s/step - loss: 0.0780 - accuracy: 0.6572 - val_loss: 0.7568 - val_accuracy: 0.0562\n",
      "Epoch 149/300\n",
      "47/47 [==============================] - 261s 6s/step - loss: 0.0840 - accuracy: 0.6403 - val_loss: 0.7377 - val_accuracy: 0.0484\n",
      "Epoch 150/300\n",
      "47/47 [==============================] - 285s 6s/step - loss: 0.0782 - accuracy: 0.6608 - val_loss: 0.5428 - val_accuracy: 0.0406\n",
      "Epoch 151/300\n",
      "47/47 [==============================] - 329s 7s/step - loss: 0.0820 - accuracy: 0.6413 - val_loss: 0.5553 - val_accuracy: 0.0484\n",
      "Epoch 152/300\n",
      "47/47 [==============================] - 275s 6s/step - loss: 0.0846 - accuracy: 0.6479 - val_loss: 0.6184 - val_accuracy: 0.0469\n",
      "Epoch 153/300\n",
      "47/47 [==============================] - 277s 6s/step - loss: 0.0836 - accuracy: 0.6372 - val_loss: 0.5827 - val_accuracy: 0.0562\n",
      "Epoch 154/300\n",
      "47/47 [==============================] - 253s 5s/step - loss: 0.0834 - accuracy: 0.6373 - val_loss: 0.6349 - val_accuracy: 0.0609\n",
      "Epoch 155/300\n",
      "47/47 [==============================] - 280s 6s/step - loss: 0.0808 - accuracy: 0.6537 - val_loss: 0.6123 - val_accuracy: 0.0516\n",
      "Epoch 156/300\n",
      "47/47 [==============================] - 254s 5s/step - loss: 0.0808 - accuracy: 0.6459 - val_loss: 0.4157 - val_accuracy: 0.0688\n",
      "Epoch 157/300\n",
      "47/47 [==============================] - 276s 6s/step - loss: 0.0825 - accuracy: 0.6486 - val_loss: 0.7304 - val_accuracy: 0.0500\n",
      "Epoch 158/300\n",
      "47/47 [==============================] - 258s 6s/step - loss: 0.0827 - accuracy: 0.6423 - val_loss: 0.7835 - val_accuracy: 0.0453\n",
      "Epoch 159/300\n",
      "47/47 [==============================] - 361s 8s/step - loss: 0.0822 - accuracy: 0.6553 - val_loss: 0.5022 - val_accuracy: 0.0453\n",
      "Epoch 160/300\n",
      "47/47 [==============================] - 295s 6s/step - loss: 0.0824 - accuracy: 0.6500 - val_loss: 0.3155 - val_accuracy: 0.0578\n",
      "Epoch 161/300\n",
      "47/47 [==============================] - 314s 7s/step - loss: 0.0803 - accuracy: 0.6637 - val_loss: 0.5478 - val_accuracy: 0.0516\n",
      "Epoch 162/300\n",
      "47/47 [==============================] - 267s 6s/step - loss: 0.0812 - accuracy: 0.6470 - val_loss: 0.5571 - val_accuracy: 0.0516\n",
      "Epoch 163/300\n",
      "47/47 [==============================] - 269s 6s/step - loss: 0.0777 - accuracy: 0.6669 - val_loss: 0.6555 - val_accuracy: 0.0562\n",
      "Epoch 164/300\n",
      "47/47 [==============================] - 284s 6s/step - loss: 0.0800 - accuracy: 0.6602 - val_loss: 0.5365 - val_accuracy: 0.0688\n",
      "Epoch 165/300\n",
      "47/47 [==============================] - 255s 5s/step - loss: 0.0801 - accuracy: 0.6519 - val_loss: 0.4363 - val_accuracy: 0.0547\n",
      "Epoch 166/300\n",
      "47/47 [==============================] - 275s 6s/step - loss: 0.1428 - accuracy: 0.5934 - val_loss: 0.4775 - val_accuracy: 0.0484\n",
      "Epoch 167/300\n",
      "47/47 [==============================] - 254s 5s/step - loss: 0.0816 - accuracy: 0.6500 - val_loss: 0.6548 - val_accuracy: 0.0500\n",
      "Epoch 168/300\n",
      "47/47 [==============================] - 276s 6s/step - loss: 0.0817 - accuracy: 0.6499 - val_loss: 0.8478 - val_accuracy: 0.0625\n",
      "Epoch 169/300\n",
      "47/47 [==============================] - 260s 6s/step - loss: 0.0824 - accuracy: 0.6465 - val_loss: 0.7986 - val_accuracy: 0.0562\n",
      "Epoch 170/300\n",
      "47/47 [==============================] - 345s 7s/step - loss: 0.0775 - accuracy: 0.6684 - val_loss: 0.6125 - val_accuracy: 0.0531\n",
      "Epoch 171/300\n",
      "47/47 [==============================] - 279s 6s/step - loss: 0.0825 - accuracy: 0.6485 - val_loss: 0.6782 - val_accuracy: 0.0562\n",
      "Epoch 172/300\n",
      "47/47 [==============================] - 286s 6s/step - loss: 0.0827 - accuracy: 0.6497 - val_loss: 0.6193 - val_accuracy: 0.0578\n",
      "Epoch 173/300\n",
      "47/47 [==============================] - 260s 6s/step - loss: 0.0803 - accuracy: 0.6589 - val_loss: 0.6038 - val_accuracy: 0.0562\n",
      "Epoch 174/300\n",
      "47/47 [==============================] - 284s 6s/step - loss: 0.0814 - accuracy: 0.6368 - val_loss: 0.7862 - val_accuracy: 0.0578\n",
      "Epoch 175/300\n",
      "47/47 [==============================] - 273s 6s/step - loss: 0.0808 - accuracy: 0.6556 - val_loss: 0.5434 - val_accuracy: 0.0516\n",
      "Epoch 176/300\n",
      "47/47 [==============================] - 264s 6s/step - loss: 0.0833 - accuracy: 0.6369 - val_loss: 0.5695 - val_accuracy: 0.0531\n",
      "Epoch 177/300\n",
      "47/47 [==============================] - 299s 6s/step - loss: 0.0849 - accuracy: 0.6409 - val_loss: 0.3023 - val_accuracy: 0.0562\n",
      "Epoch 178/300\n",
      "47/47 [==============================] - 303s 6s/step - loss: 0.0837 - accuracy: 0.6474 - val_loss: 0.6848 - val_accuracy: 0.0500\n",
      "Epoch 179/300\n",
      "47/47 [==============================] - 333s 7s/step - loss: 0.0794 - accuracy: 0.6576 - val_loss: 0.6894 - val_accuracy: 0.0516\n",
      "Epoch 180/300\n",
      "47/47 [==============================] - 315s 7s/step - loss: 0.0786 - accuracy: 0.6557 - val_loss: 0.3602 - val_accuracy: 0.0562\n",
      "Epoch 181/300\n",
      "47/47 [==============================] - 380s 8s/step - loss: 0.0800 - accuracy: 0.6631 - val_loss: 0.7645 - val_accuracy: 0.0578\n",
      "Epoch 182/300\n",
      "47/47 [==============================] - 310s 7s/step - loss: 0.0842 - accuracy: 0.6514 - val_loss: 1.0256 - val_accuracy: 0.0625\n",
      "Epoch 183/300\n",
      "47/47 [==============================] - 345s 7s/step - loss: 0.0805 - accuracy: 0.6520 - val_loss: 0.6374 - val_accuracy: 0.0719\n",
      "Epoch 184/300\n",
      "47/47 [==============================] - 296s 6s/step - loss: 0.0797 - accuracy: 0.6559 - val_loss: 0.7145 - val_accuracy: 0.0672\n",
      "Epoch 185/300\n",
      "47/47 [==============================] - 343s 7s/step - loss: 0.0808 - accuracy: 0.6572 - val_loss: 0.8985 - val_accuracy: 0.0516\n",
      "Epoch 186/300\n",
      "47/47 [==============================] - 288s 6s/step - loss: 0.0781 - accuracy: 0.6593 - val_loss: 0.5645 - val_accuracy: 0.0578\n",
      "Epoch 187/300\n",
      "47/47 [==============================] - 403s 9s/step - loss: 0.0769 - accuracy: 0.6691 - val_loss: 0.7108 - val_accuracy: 0.0578\n",
      "Epoch 188/300\n",
      "47/47 [==============================] - 479s 10s/step - loss: 0.0836 - accuracy: 0.6539 - val_loss: 0.5554 - val_accuracy: 0.0641\n",
      "Epoch 189/300\n",
      "47/47 [==============================] - 438s 9s/step - loss: 0.0802 - accuracy: 0.6511 - val_loss: 0.6518 - val_accuracy: 0.0562\n",
      "Epoch 190/300\n",
      "47/47 [==============================] - 319s 7s/step - loss: 0.0823 - accuracy: 0.6473 - val_loss: 0.8546 - val_accuracy: 0.0531\n",
      "Epoch 191/300\n",
      "47/47 [==============================] - 396s 8s/step - loss: 0.0811 - accuracy: 0.6592 - val_loss: 0.6231 - val_accuracy: 0.0516\n",
      "Epoch 192/300\n",
      "47/47 [==============================] - 300s 6s/step - loss: 0.0867 - accuracy: 0.6390 - val_loss: 0.5960 - val_accuracy: 0.0500\n",
      "Epoch 193/300\n",
      "47/47 [==============================] - 379s 8s/step - loss: 0.0809 - accuracy: 0.6466 - val_loss: 0.6866 - val_accuracy: 0.0484\n",
      "Epoch 194/300\n",
      "47/47 [==============================] - 311s 7s/step - loss: 0.0800 - accuracy: 0.6516 - val_loss: 0.6060 - val_accuracy: 0.0547\n",
      "Epoch 195/300\n",
      "47/47 [==============================] - 329s 7s/step - loss: 0.0788 - accuracy: 0.6606 - val_loss: 0.7031 - val_accuracy: 0.0562\n",
      "Epoch 196/300\n",
      "47/47 [==============================] - 404s 9s/step - loss: 0.0993 - accuracy: 0.6245 - val_loss: 0.6461 - val_accuracy: 0.0469\n",
      "Epoch 197/300\n",
      "47/47 [==============================] - 317s 7s/step - loss: 0.0809 - accuracy: 0.6538 - val_loss: 0.6177 - val_accuracy: 0.0578\n",
      "Epoch 198/300\n",
      "11/47 [======>.......................] - ETA: 6:08 - loss: 0.0855 - accuracy: 0.6398"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yr/qwqgmggj2_12p38pnv1hf6380000gp/T/ipykernel_977/1930868873.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_generator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEP_SIZE_VALID\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1859\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1860\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1861\u001b[0;31m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1862\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1863\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2943\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2945\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1919\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    558\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    561\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931b5047-464d-48c8-a4b2-634e143cca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9323567d-93a3-4500-b39a-a00318629079",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict_generator(test_generator,\n",
    "steps=STEP_SIZE_TEST,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f169b3-9e88-43d7-9480-c17a80cb9e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d306d73-b624-46e9-aaa3-dbcca93dd9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bool = (pred >0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a65ff-7ab7-4ec6-a745-bbf13c9c73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "labels = train_generator.class_indices\n",
    "\n",
    "textfile = open(\"TR_labels.txt\", \"w\")\n",
    "for element in labels:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "for row in pred_bool:\n",
    "    l=[]\n",
    "    for index,cls in enumerate(row):\n",
    "        if cls:\n",
    "            l.append(labels[index])\n",
    "    predictions.append(\",\".join(l))\n",
    "filenames=test_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})\n",
    "results.to_csv(\"TRresults.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385e1fa-c041-40a3-ad3a-c47482ed3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "476521a0-bfa6-411f-9793-2fd3f3d8e161",
   "metadata": {},
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "#load the image\n",
    "my_image = load_img('data/test/images/127.png', target_size=(224, 224))\n",
    "\n",
    "#preprocess the image\n",
    "my_image = img_to_array(my_image)\n",
    "my_image = my_image.reshape((1, my_image.shape[0], my_image.shape[1], my_image.shape[2]))\n",
    "my_image = preprocess_input(my_image)\n",
    "\n",
    "#make the prediction\n",
    "prediction = model.predict(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c30c92-b894-4711-a67e-0bf044228268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "original = PIL.Image.open(\"data/test/images/127.png\")\n",
    "file_type = original.format\n",
    "\n",
    "original.save(\"testing/test.png\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed903af-3ed6-445b-ad90-5300d4304062",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "original = PIL.Image.open(\"data/test/images/127.png\") # replace later with image input\n",
    "file_type = original.format\n",
    "\n",
    "original.save(\"testing/test.png\", format=\"png\")\n",
    "##########################\n",
    "testdata = []\n",
    "for fname in sorted(os.listdir('testing')):\n",
    "    if fname == \".DS_Store\": continue\n",
    "            \n",
    "    subject_data_path = os.path.join('testing', fname)                   \n",
    "    if not os.path.isfile(subject_data_path): continue          \n",
    "    testdata.append(fname)\n",
    "    \n",
    "df = pd.DataFrame(testdata, columns=['fnames'])\n",
    "df['fnames']= df['fnames'].astype(str)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "test_g=test_datagen.flow_from_dataframe(\n",
    "     dataframe= df,\n",
    "      directory=\"./testing\",\n",
    "      x_col=\"fnames\",\n",
    "      batch_size=1,\n",
    "      seed=42,\n",
    "      shuffle=False,\n",
    "      class_mode=None,\n",
    "      target_size=(224,224))\n",
    "STEP_SIZE_TEST=test_g.n//test_g.batch_size\n",
    "pred = model.predict_generator(test_g,\n",
    "                               steps=STEP_SIZE_TEST,\n",
    "                               verbose=1)\n",
    "pred_bool = (pred >0.5)\n",
    "predictions=[]\n",
    "#labels = train_generator.class_indices\n",
    "#labels = dict((v,k) for k,v in labels.items())\n",
    "labels = {}\n",
    "file1 = open('TR_labels.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    " \n",
    "count = 0\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    labels[co\n",
    "    count += 1\n",
    "    print(\"Line{}: {}\".format(count, line.strip()))\n",
    "for row in pred_bool:\n",
    "    l=[]\n",
    "    for index,cls in enumerate(row):\n",
    "        if cls:\n",
    "            l.append(labels[index])\n",
    "    predictions.append(\",\".join(l))\n",
    "           \n",
    "if predictions[0] == '':\n",
    "    result = 0\n",
    "else: \n",
    "    result = float (predictions[0] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e81fe7-eaf1-4b6c-bb83-60163f2051ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06603d7a-be6f-4b00-a566-274256c1fbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c159d-64be-4d98-b12d-98ff511626a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2964a2d6-0d92-4019-b93e-f908bd728ac7",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('data/test/images/127.png', target_size =(224,224))\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "result = model.predict(test_image)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd6718b4-dc5f-4115-b886-424f6fcfe55d",
   "metadata": {},
   "source": [
    "test_image = image.load_img('data/test/images/228.png', target_size =(224,224))\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be769dfd-b348-4067-bb46-276897728b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "filename2 = 'model_tr.h5' \n",
    "model.save(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607599c9-9b5d-44b2-a74a-95516d370e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# load model\n",
    "model = load_model('model_tr.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "652a1b62-1de5-4853-b3c4-d36882a6a62b",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2020/10/create-image-classification-model-python-keras/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23e021e2-aaed-440a-bda5-a8b1b4e313fd",
   "metadata": {},
   "source": [
    "    def flow_from_dataframe(self,\n",
    "                            dataframe,\n",
    "                            directory=None,\n",
    "                            x_col=\"filename\",\n",
    "                            y_col=\"class\",\n",
    "                            weight_col=None,\n",
    "                            target_size=(256, 256),\n",
    "                            color_mode='rgb',\n",
    "                            classes=None,\n",
    "                            class_mode='categorical',\n",
    "                            batch_size=32,\n",
    "                            shuffle=True,\n",
    "                            seed=None,\n",
    "                            save_to_dir=None,\n",
    "                            save_prefix='',\n",
    "                            save_format='png',\n",
    "                            subset=None,\n",
    "                            interpolation='nearest',\n",
    "                            validate_filenames=True,\n",
    "                            **kwargs):\n",
    "        \"\"\"Takes the dataframe and the path to a directory\n",
    "         and generates batches of augmented/normalized data.\n",
    "        **A simple tutorial can be found **[here](\n",
    "                                    http://bit.ly/keras_flow_from_dataframe).\n",
    "        # Arguments\n",
    "            dataframe: Pandas dataframe containing the filepaths relative to\n",
    "                `directory` (or absolute paths if `directory` is None) of the\n",
    "                images in a string column. It should include other column/s\n",
    "                depending on the `class_mode`:\n",
    "                - if `class_mode` is `\"categorical\"` (default value) it must\n",
    "                    include the `y_col` column with the class/es of each image.\n",
    "                    Values in column can be string/list/tuple if a single class\n",
    "                    or list/tuple if multiple classes.\n",
    "                - if `class_mode` is `\"binary\"` or `\"sparse\"` it must include\n",
    "                    the given `y_col` column with class values as strings.\n",
    "                - if `class_mode` is `\"raw\"` or `\"multi_output\"` it should contain\n",
    "                the columns specified in `y_col`.\n",
    "                - if `class_mode` is `\"input\"` or `None` no extra column is needed.\n",
    "            directory: string, path to the directory to read images from. If `None`,\n",
    "                data in `x_col` column should be absolute paths.\n",
    "            x_col: string, column in `dataframe` that contains the filenames (or\n",
    "                absolute paths if `directory` is `None`).\n",
    "            y_col: string or list, column/s in `dataframe` that has the target data.\n",
    "            weight_col: string, column in `dataframe` that contains the sample\n",
    "                weights. Default: `None`.\n",
    "            target_size: tuple of integers `(height, width)`, default: `(256, 256)`.\n",
    "                The dimensions to which all images found will be resized.\n",
    "            color_mode: one of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
    "                Whether the images will be converted to have 1 or 3 color channels.\n",
    "            classes: optional list of classes (e.g. `['dogs', 'cats']`).\n",
    "                Default: None. If not provided, the list of classes will be\n",
    "                automatically inferred from the `y_col`,\n",
    "                which will map to the label indices, will be alphanumeric).\n",
    "                The dictionary containing the mapping from class names to class\n",
    "                indices can be obtained via the attribute `class_indices`.\n",
    "            class_mode: one of \"binary\", \"categorical\", \"input\", \"multi_output\",\n",
    "                \"raw\", sparse\" or None. Default: \"categorical\".\n",
    "                Mode for yielding the targets:\n",
    "                - `\"binary\"`: 1D NumPy array of binary labels,\n",
    "                - `\"categorical\"`: 2D NumPy array of one-hot encoded labels.\n",
    "                    Supports multi-label output.\n",
    "                - `\"input\"`: images identical to input images (mainly used to\n",
    "                    work with autoencoders),\n",
    "                - `\"multi_output\"`: list with the values of the different columns,\n",
    "                - `\"raw\"`: NumPy array of values in `y_col` column(s),\n",
    "                - `\"sparse\"`: 1D NumPy array of integer labels,\n",
    "                - `None`, no targets are returned (the generator will only yield\n",
    "                    batches of image data, which is useful to use in\n",
    "                    `model.predict_generator()`).\n",
    "            batch_size: size of the batches of data (default: 32).\n",
    "            shuffle: whether to shuffle the data (default: True)\n",
    "            seed: optional random seed for shuffling and transformations.\n",
    "            save_to_dir: None or str (default: None).\n",
    "                This allows you to optionally specify a directory\n",
    "                to which to save the augmented pictures being generated\n",
    "                (useful for visualizing what you are doing).\n",
    "            save_prefix: str. Prefix to use for filenames of saved pictures\n",
    "                (only relevant if `save_to_dir` is set).\n",
    "            save_format: one of \"png\", \"jpeg\"\n",
    "                (only relevant if `save_to_dir` is set). Default: \"png\".\n",
    "            follow_links: whether to follow symlinks inside class subdirectories\n",
    "                (default: False).\n",
    "            subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
    "                `validation_split` is set in `ImageDataGenerator`.\n",
    "            interpolation: Interpolation method used to resample the image if the\n",
    "                target size is different from that of the loaded image.\n",
    "                Supported methods are `\"nearest\"`, `\"bilinear\"`, and `\"bicubic\"`.\n",
    "                If PIL version 1.1.3 or newer is installed, `\"lanczos\"` is also\n",
    "                supported. If PIL version 3.4.0 or newer is installed, `\"box\"` and\n",
    "                `\"hamming\"` are also supported. By default, `\"nearest\"` is used.\n",
    "            validate_filenames: Boolean, whether to validate image filenames in\n",
    "                `x_col`. If `True`, invalid images will be ignored. Disabling this\n",
    "                option can lead to speed-up in the execution of this function.\n",
    "                Default: `True`.\n",
    "        # Returns\n",
    "            A `DataFrameIterator` yielding tuples of `(x, y)`\n",
    "            where `x` is a NumPy array containing a batch\n",
    "            of images with shape `(batch_size, *target_size, channels)`\n",
    "            and `y` is a NumPy array of corresponding labels.\n",
    "        \"\"\"\n",
    "        if 'has_ext' in kwargs:\n",
    "            warnings.warn('has_ext is deprecated, filenames in the dataframe have '\n",
    "                          'to match the exact filenames in disk.',\n",
    "                          DeprecationWarning)\n",
    "        if 'sort' in kwargs:\n",
    "            warnings.warn('sort is deprecated, batches will be created in the'\n",
    "                          'same order than the filenames provided if shuffle'\n",
    "                          'is set to False.', DeprecationWarning)\n",
    "        if class_mode == 'other':\n",
    "            warnings.warn('`class_mode` \"other\" is deprecated, please use '\n",
    "                          '`class_mode` \"raw\".', DeprecationWarning)\n",
    "            class_mode = 'raw'\n",
    "        if 'drop_duplicates' in kwargs:\n",
    "            warnings.warn('drop_duplicates is deprecated, you can drop duplicates '\n",
    "                          'by using the pandas.DataFrame.drop_duplicates method.',\n",
    "                          DeprecationWarning)\n",
    "\n",
    "        return DataFrameIterator(\n",
    "            dataframe,\n",
    "            directory,\n",
    "            self,\n",
    "            x_col=x_col,\n",
    "            y_col=y_col,\n",
    "            weight_col=weight_col,\n",
    "            target_size=target_size,\n",
    "            color_mode=color_mode,\n",
    "            classes=classes,\n",
    "            class_mode=class_mode,\n",
    "            data_format=self.data_format,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format,\n",
    "            subset=subset,\n",
    "            interpolation=interpolation,\n",
    "            validate_filenames=validate_filenames,\n",
    "            dtype=self.dtype\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0415ad83-e8ea-42f0-af3f-7c72baacf2fa",
   "metadata": {},
   "source": [
    "\"\"\"Utilities for real-time data augmentation on image data.\n",
    "\"\"\"\n",
    "import os\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from .iterator import BatchFromFilesMixin, Iterator\n",
    "from .utils import validate_filename\n",
    "\n",
    "\n",
    "class DataFrameIterator(BatchFromFilesMixin, Iterator):\n",
    "    \"\"\"Iterator capable of reading images from a directory on disk\n",
    "        through a dataframe.\n",
    "    # Arguments\n",
    "        dataframe: Pandas dataframe containing the filepaths relative to\n",
    "            `directory` (or absolute paths if `directory` is None) of the\n",
    "            images in a string column. It should include other column/s\n",
    "            depending on the `class_mode`:\n",
    "            - if `class_mode` is `\"categorical\"` (default value) it must\n",
    "                include the `y_col` column with the class/es of each image.\n",
    "                Values in column can be string/list/tuple if a single class\n",
    "                or list/tuple if multiple classes.\n",
    "            - if `class_mode` is `\"binary\"` or `\"sparse\"` it must include\n",
    "                the given `y_col` column with class values as strings.\n",
    "            - if `class_mode` is `\"raw\"` or `\"multi_output\"` it should contain\n",
    "                the columns specified in `y_col`.\n",
    "            - if `class_mode` is `\"input\"` or `None` no extra column is needed.\n",
    "        directory: string, path to the directory to read images from. If `None`,\n",
    "            data in `x_col` column should be absolute paths.\n",
    "        image_data_generator: Instance of `ImageDataGenerator` to use for\n",
    "            random transformations and normalization. If None, no transformations\n",
    "            and normalizations are made.\n",
    "        x_col: string, column in `dataframe` that contains the filenames (or\n",
    "            absolute paths if `directory` is `None`).\n",
    "        y_col: string or list, column/s in `dataframe` that has the target data.\n",
    "        weight_col: string, column in `dataframe` that contains the sample\n",
    "            weights. Default: `None`.\n",
    "        target_size: tuple of integers, dimensions to resize input images to.\n",
    "        color_mode: One of `\"rgb\"`, `\"rgba\"`, `\"grayscale\"`.\n",
    "            Color mode to read images.\n",
    "        classes: Optional list of strings, classes to use (e.g. `[\"dogs\", \"cats\"]`).\n",
    "            If None, all classes in `y_col` will be used.\n",
    "        class_mode: one of \"binary\", \"categorical\", \"input\", \"multi_output\",\n",
    "            \"raw\", \"sparse\" or None. Default: \"categorical\".\n",
    "            Mode for yielding the targets:\n",
    "            - `\"binary\"`: 1D numpy array of binary labels,\n",
    "            - `\"categorical\"`: 2D numpy array of one-hot encoded labels.\n",
    "                Supports multi-label output.\n",
    "            - `\"input\"`: images identical to input images (mainly used to\n",
    "                work with autoencoders),\n",
    "            - `\"multi_output\"`: list with the values of the different columns,\n",
    "            - `\"raw\"`: numpy array of values in `y_col` column(s),\n",
    "            - `\"sparse\"`: 1D numpy array of integer labels,\n",
    "            - `None`, no targets are returned (the generator will only yield\n",
    "                batches of image data, which is useful to use in\n",
    "                `model.predict_generator()`).\n",
    "        batch_size: Integer, size of a batch.\n",
    "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
    "        seed: Random seed for data shuffling.\n",
    "        data_format: String, one of `channels_first`, `channels_last`.\n",
    "        save_to_dir: Optional directory where to save the pictures\n",
    "            being yielded, in a viewable format. This is useful\n",
    "            for visualizing the random transformations being\n",
    "            applied, for debugging purposes.\n",
    "        save_prefix: String prefix to use for saving sample\n",
    "            images (if `save_to_dir` is set).\n",
    "        save_format: Format to use for saving sample images\n",
    "            (if `save_to_dir` is set).\n",
    "        subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
    "            validation_split is set in ImageDataGenerator.\n",
    "        interpolation: Interpolation method used to resample the image if the\n",
    "            target size is different from that of the loaded image.\n",
    "            Supported methods are \"nearest\", \"bilinear\", and \"bicubic\".\n",
    "            If PIL version 1.1.3 or newer is installed, \"lanczos\" is also\n",
    "            supported. If PIL version 3.4.0 or newer is installed, \"box\" and\n",
    "            \"hamming\" are also supported. By default, \"nearest\" is used.\n",
    "        keep_aspect_ratio: Boolean, whether to resize images to a target size\n",
    "            without aspect ratio distortion. The image is cropped in the center\n",
    "            with target aspect ratio before resizing.\n",
    "        dtype: Dtype to use for the generated arrays.\n",
    "        validate_filenames: Boolean, whether to validate image filenames in\n",
    "        `x_col`. If `True`, invalid images will be ignored. Disabling this option\n",
    "        can lead to speed-up in the instantiation of this class. Default: `True`.\n",
    "    \"\"\"\n",
    "    allowed_class_modes = {\n",
    "        'binary', 'categorical', 'input', 'multi_output', 'raw', 'sparse', None\n",
    "    }\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        try:\n",
    "            from tensorflow.keras.utils import Sequence as TFSequence\n",
    "            if TFSequence not in cls.__bases__:\n",
    "                cls.__bases__ = cls.__bases__ + (TFSequence,)\n",
    "        except ImportError:\n",
    "            pass\n",
    "        return super(DataFrameIterator, cls).__new__(cls)\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataframe,\n",
    "                 directory=None,\n",
    "                 image_data_generator=None,\n",
    "                 x_col=\"filename\",\n",
    "                 y_col=\"class\",\n",
    "                 weight_col=None,\n",
    "                 target_size=(256, 256),\n",
    "                 color_mode='rgb',\n",
    "                 classes=None,\n",
    "                 class_mode='categorical',\n",
    "                 batch_size=32,\n",
    "                 shuffle=True,\n",
    "                 seed=None,\n",
    "                 data_format='channels_last',\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='png',\n",
    "                 subset=None,\n",
    "                 interpolation='nearest',\n",
    "                 keep_aspect_ratio=False,\n",
    "                 dtype='float32',\n",
    "                 validate_filenames=True):\n",
    "\n",
    "        super(DataFrameIterator, self).set_processing_attrs(image_data_generator,\n",
    "                                                            target_size,\n",
    "                                                            color_mode,\n",
    "                                                            data_format,\n",
    "                                                            save_to_dir,\n",
    "                                                            save_prefix,\n",
    "                                                            save_format,\n",
    "                                                            subset,\n",
    "                                                            interpolation,\n",
    "                                                            keep_aspect_ratio)\n",
    "        df = dataframe.copy()\n",
    "        self.directory = directory or ''\n",
    "        self.class_mode = class_mode\n",
    "        self.dtype = dtype\n",
    "        # check that inputs match the required class_mode\n",
    "        self._check_params(df, x_col, y_col, weight_col, classes)\n",
    "        if validate_filenames:  # check which image files are valid and keep them\n",
    "            df = self._filter_valid_filepaths(df, x_col)\n",
    "        if class_mode not in [\"input\", \"multi_output\", \"raw\", None]:\n",
    "            df, classes = self._filter_classes(df, y_col, classes)\n",
    "            num_classes = len(classes)\n",
    "            # build an index of all the unique classes\n",
    "            self.class_indices = dict(zip(classes, range(len(classes))))\n",
    "        # retrieve only training or validation set\n",
    "        if self.split:\n",
    "            num_files = len(df)\n",
    "            start = int(self.split[0] * num_files)\n",
    "            stop = int(self.split[1] * num_files)\n",
    "            df = df.iloc[start: stop, :]\n",
    "        # get labels for each observation\n",
    "        if class_mode not in [\"input\", \"multi_output\", \"raw\", None]:\n",
    "            self.classes = self.get_classes(df, y_col)\n",
    "        self.filenames = df[x_col].tolist()\n",
    "        self._sample_weight = df[weight_col].values if weight_col else None\n",
    "\n",
    "        if class_mode == \"multi_output\":\n",
    "            self._targets = [np.array(df[col].tolist()) for col in y_col]\n",
    "        if class_mode == \"raw\":\n",
    "            self._targets = df[y_col].values\n",
    "        self.samples = len(self.filenames)\n",
    "        validated_string = 'validated' if validate_filenames else 'non-validated'\n",
    "        if class_mode in [\"input\", \"multi_output\", \"raw\", None]:\n",
    "            print('Found {} {} image filenames.'\n",
    "                  .format(self.samples, validated_string))\n",
    "        else:\n",
    "            print('Found {} {} image filenames belonging to {} classes.'\n",
    "                  .format(self.samples, validated_string, num_classes))\n",
    "        self._filepaths = [\n",
    "            os.path.join(self.directory, fname) for fname in self.filenames\n",
    "        ]\n",
    "        super(DataFrameIterator, self).__init__(self.samples,\n",
    "                                                batch_size,\n",
    "                                                shuffle,\n",
    "                                                seed)\n",
    "\n",
    "    def _check_params(self, df, x_col, y_col, weight_col, classes):\n",
    "        # check class mode is one of the currently supported\n",
    "        if self.class_mode not in self.allowed_class_modes:\n",
    "            raise ValueError('Invalid class_mode: {}; expected one of: {}'\n",
    "                             .format(self.class_mode, self.allowed_class_modes))\n",
    "        # check that y_col has several column names if class_mode is multi_output\n",
    "        if (self.class_mode == 'multi_output') and not isinstance(y_col, list):\n",
    "            raise TypeError(\n",
    "                'If class_mode=\"{}\", y_col must be a list. Received {}.'\n",
    "                .format(self.class_mode, type(y_col).__name__)\n",
    "            )\n",
    "        # check that filenames/filepaths column values are all strings\n",
    "        if not all(df[x_col].apply(lambda x: isinstance(x, str))):\n",
    "            raise TypeError('All values in column x_col={} must be strings.'\n",
    "                            .format(x_col))\n",
    "        # check labels are string if class_mode is binary or sparse\n",
    "        if self.class_mode in {'binary', 'sparse'}:\n",
    "            if not all(df[y_col].apply(lambda x: isinstance(x, str))):\n",
    "                raise TypeError('If class_mode=\"{}\", y_col=\"{}\" column '\n",
    "                                'values must be strings.'\n",
    "                                .format(self.class_mode, y_col))\n",
    "        # check that if binary there are only 2 different classes\n",
    "        if self.class_mode == 'binary':\n",
    "            if classes:\n",
    "                classes = set(classes)\n",
    "                if len(classes) != 2:\n",
    "                    raise ValueError('If class_mode=\"binary\" there must be 2 '\n",
    "                                     'classes. {} class/es were given.'\n",
    "                                     .format(len(classes)))\n",
    "            elif df[y_col].nunique() != 2:\n",
    "                raise ValueError('If class_mode=\"binary\" there must be 2 classes. '\n",
    "                                 'Found {} classes.'.format(df[y_col].nunique()))\n",
    "        # check values are string, list or tuple if class_mode is categorical\n",
    "        if self.class_mode == 'categorical':\n",
    "            types = (str, list, tuple)\n",
    "            if not all(df[y_col].apply(lambda x: isinstance(x, types))):\n",
    "                raise TypeError('If class_mode=\"{}\", y_col=\"{}\" column '\n",
    "                                'values must be type string, list or tuple.'\n",
    "                                .format(self.class_mode, y_col))\n",
    "        # raise warning if classes are given but will be unused\n",
    "        if classes and self.class_mode in {\"input\", \"multi_output\", \"raw\", None}:\n",
    "            warnings.warn('`classes` will be ignored given the class_mode=\"{}\"'\n",
    "                          .format(self.class_mode))\n",
    "        # check that if weight column that the values are numerical\n",
    "        if weight_col and not issubclass(df[weight_col].dtype.type, np.number):\n",
    "            raise TypeError('Column weight_col={} must be numeric.'\n",
    "                            .format(weight_col))\n",
    "\n",
    "    def get_classes(self, df, y_col):\n",
    "        labels = []\n",
    "        for label in df[y_col]:\n",
    "            if isinstance(label, (list, tuple)):\n",
    "                labels.append([self.class_indices[lbl] for lbl in label])\n",
    "            else:\n",
    "                labels.append(self.class_indices[label])\n",
    "        return labels\n",
    "\n",
    "    @staticmethod\n",
    "    def _filter_classes(df, y_col, classes):\n",
    "        df = df.copy()\n",
    "\n",
    "        def remove_classes(labels, classes):\n",
    "            if isinstance(labels, (list, tuple)):\n",
    "                labels = [cls for cls in labels if cls in classes]\n",
    "                return labels or None\n",
    "            elif isinstance(labels, str):\n",
    "                return labels if labels in classes else None\n",
    "            else:\n",
    "                raise TypeError(\n",
    "                    \"Expect string, list or tuple but found {} in {} column \"\n",
    "                    .format(type(labels), y_col)\n",
    "                )\n",
    "\n",
    "        if classes:\n",
    "            # prepare for membership lookup\n",
    "            classes = list(OrderedDict.fromkeys(classes).keys())\n",
    "            df[y_col] = df[y_col].apply(lambda x: remove_classes(x, classes))\n",
    "        else:\n",
    "            classes = set()\n",
    "            for v in df[y_col]:\n",
    "                if isinstance(v, (list, tuple)):\n",
    "                    classes.update(v)\n",
    "                else:\n",
    "                    classes.add(v)\n",
    "            classes = sorted(classes)\n",
    "        return df.dropna(subset=[y_col]), classes\n",
    "\n",
    "    def _filter_valid_filepaths(self, df, x_col):\n",
    "        \"\"\"Keep only dataframe rows with valid filenames\n",
    "        # Arguments\n",
    "            df: Pandas dataframe containing filenames in a column\n",
    "            x_col: string, column in `df` that contains the filenames or filepaths\n",
    "        # Returns\n",
    "            absolute paths to image files\n",
    "        \"\"\"\n",
    "        filepaths = df[x_col].map(\n",
    "            lambda fname: os.path.join(self.directory, fname)\n",
    "        )\n",
    "        mask = filepaths.apply(validate_filename, args=(self.white_list_formats,))\n",
    "        n_invalid = (~mask).sum()\n",
    "        if n_invalid:\n",
    "            warnings.warn(\n",
    "                'Found {} invalid image filename(s) in x_col=\"{}\". '\n",
    "                'These filename(s) will be ignored.'\n",
    "                .format(n_invalid, x_col)\n",
    "            )\n",
    "        return df[mask]\n",
    "\n",
    "    @property\n",
    "    def filepaths(self):\n",
    "        return self._filepaths\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        if self.class_mode in {\"multi_output\", \"raw\"}:\n",
    "            return self._targets\n",
    "        else:\n",
    "            return self.classes\n",
    "\n",
    "    @property\n",
    "    def sample_weight(self):\n",
    "        return self._sample_weight"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f606ee1-be74-4bad-8ac7-7f71faf4010b",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/image/iterator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "19a8004a-07d0-47b6-8b87-2d70c9c5b0b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matlab.engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yr/qwqgmggj2_12p38pnv1hf6380000gp/T/ipykernel_5075/1620819283.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matlab.engine'"
     ]
    }
   ],
   "source": [
    "import matlab.engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee961c0-0365-415e-919f-e1a38ffe7d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_df['labels'].astype(float).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc694e0-fc60-471e-8189-f737b8b189d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "test_g=test_datagen.flow_from_dataframe(\n",
    "     dataframe= test_df,\n",
    "      directory=\"./data/test/images\",\n",
    "      x_col=\"fnames\",\n",
    "      batch_size=1,\n",
    "      seed=42,\n",
    "      shuffle=False,\n",
    "      class_mode=None,\n",
    "      target_size=(224,224),\n",
    "      validate_filenames=False)\n",
    "STEP_SIZE_TEST=test_g.n//test_g.batch_size\n",
    "pred = model.predict_generator(test_g,\n",
    "                               steps=STEP_SIZE_TEST,\n",
    "                               verbose=1)\n",
    "pred_bool = (pred >0.5)\n",
    "predictions=[]\n",
    "#labels = train_generator.class_indices\n",
    "#labels = dict((v,k) for k,v in labels.items())\n",
    "labels = {}\n",
    "file1 = open('TR_labels.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    " \n",
    "count = 0\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    labels[count] = line.strip()\n",
    "    count += 1\n",
    "    \n",
    "for row in pred_bool:\n",
    "    l=[]\n",
    "    for index,cls in enumerate(row):\n",
    "        if cls:\n",
    "            l.append(labels[index])\n",
    "    predictions.append(\",\".join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b5543c-8a2a-461c-8392-6dac49c8f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred =[]\n",
    "for i in range(0, len(predictions)):\n",
    "    if predictions[i] == '':\n",
    "        y_pred.append(0)\n",
    "    else:\n",
    "        listValue  = list(map(float, predictions[i].split(',')))\n",
    "\n",
    "        y_pred.append(max(listValue))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b613e41-f57e-4cba-bd45-16e6218bb2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing all necessary libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Calculating the accuracy of classifier\n",
    "print(f\"Accuracy of the classifier is: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f66a8c2-9390-43dd-bcc3-03005837f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# confusion_matrix funnction a matrix containing the summary of predictions\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b675c58f-6f95-4edc-ab5c-d400826c3432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "# Calculating the precision score of classifier\n",
    "print(f\"Precision Score of the classifier is: {precision_score(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350b0028-72fe-430a-a879-d7a5838074f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "# Calculating the recall score of classifier\n",
    "print(f\"Recall Score of the classifier is: {recall_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc4b070-a1ac-49c4-b8e7-4a8980d4a546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all necessary libraries\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Calculating the F1 score of classifier\n",
    "print(f\"F1 Score of the classifier is: {f1_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb2639-1b2b-4ff4-b7b3-f1eef97e5847",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Printing AUC\n",
    "print(f\"AUC for our classifier is: {roc_auc}\")\n",
    "\n",
    "# Plotting the ROC\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd100e8-c53c-433d-acab-d55208bd0b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "  # Calculation of Mean Squared Error (MSE)\n",
    "mean_squared_error(Y_test,Y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
