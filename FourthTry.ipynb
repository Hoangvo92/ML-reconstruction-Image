{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6db28101",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Importing all the relevant library\n",
    "%matplotlib inline\n",
    "import h5py, os\n",
    "#from functions import transforms as T\n",
    "#from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "#from functions import transforms as T \n",
    "#from functions.subsample import MaskFunc\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25ab479c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class MaskFunc:\n",
    "    \"\"\"\n",
    "    MaskFunc creates a sub-sampling mask of a given shape.\n",
    "\n",
    "    The mask selects a subset of columns from the input k-space data. If the k-space data has N\n",
    "    columns, the mask picks out:\n",
    "        1. N_low_freqs = (N * center_fraction) columns in the center corresponding to\n",
    "           low-frequencies\n",
    "        2. The other columns are selected uniformly at random with a probability equal to:\n",
    "           prob = (N / acceleration - N_low_freqs) / (N - N_low_freqs).\n",
    "    This ensures that the expected number of columns selected is equal to (N / acceleration)\n",
    "\n",
    "    It is possible to use multiple center_fractions and accelerations, in which case one possible\n",
    "    (center_fraction, acceleration) is chosen uniformly at random each time the MaskFunc object is\n",
    "    called.\n",
    "\n",
    "    For example, if accelerations = [4, 8] and center_fractions = [0.08, 0.04], then there\n",
    "    is a 50% probability that 4-fold acceleration with 8% center fraction is selected and a 50%\n",
    "    probability that 8-fold acceleration with 4% center fraction is selected.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, center_fractions, accelerations):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            center_fractions (List[float]): Fraction of low-frequency columns to be retained.\n",
    "                If multiple values are provided, then one of these numbers is chosen uniformly\n",
    "                each time.\n",
    "\n",
    "            accelerations (List[int]): Amount of under-sampling. This should have the same length\n",
    "                as center_fractions. If multiple values are provided, then one of these is chosen\n",
    "                uniformly each time. An acceleration of 4 retains 25% of the columns, but they may\n",
    "                not be spaced evenly.\n",
    "        \"\"\"\n",
    "        if len(center_fractions) != len(accelerations):\n",
    "            raise ValueError('Number of center fractions should match number of accelerations')\n",
    "\n",
    "        self.center_fractions = center_fractions\n",
    "        self.accelerations = accelerations\n",
    "        self.rng = np.random.RandomState()\n",
    "\n",
    "    def __call__(self, shape, seed=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            shape (iterable[int]): The shape of the mask to be created. The shape should have\n",
    "                at least 3 dimensions. Samples are drawn along the second last dimension.\n",
    "            seed (int, optional): Seed for the random number generator. Setting the seed\n",
    "                ensures the same mask is generated each time for the same shape.\n",
    "        Returns:\n",
    "            torch.Tensor: A mask of the specified shape.\n",
    "        \"\"\"\n",
    "        if len(shape) < 3:\n",
    "            raise ValueError('Shape should have 3 or more dimensions')\n",
    "\n",
    "        self.rng.seed(seed)\n",
    "        num_cols = shape[-2]\n",
    "\n",
    "        choice = self.rng.randint(0, len(self.accelerations))\n",
    "        center_fraction = self.center_fractions[choice]\n",
    "        acceleration = self.accelerations[choice]\n",
    "\n",
    "        # Create the mask\n",
    "        num_low_freqs = int(round(num_cols * center_fraction))\n",
    "        prob = (num_cols / acceleration - num_low_freqs) / (num_cols - num_low_freqs)\n",
    "        mask = self.rng.uniform(size=num_cols) < prob\n",
    "        pad = (num_cols - num_low_freqs + 1) // 2\n",
    "        mask[pad:pad + num_low_freqs] = True\n",
    "\n",
    "        # Reshape the mask\n",
    "        mask_shape = [1 for _ in shape]\n",
    "        mask_shape[-2] = num_cols\n",
    "        mask = torch.from_numpy(mask.reshape(*mask_shape).astype(np.float32))\n",
    "\n",
    "        return mask\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12d1dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def tensor_to_complex_np(data):\n",
    "    \"\"\"\n",
    "    Converts a complex torch tensor to numpy array.\n",
    "    Args:\n",
    "        data (torch.Tensor): Input data to be converted to numpy.\n",
    "    Returns:\n",
    "        np.array: Complex numpy version of data\n",
    "    \"\"\"\n",
    "    data = data.numpy()\n",
    "    return data[..., 0] + 1j * data[..., 1]\n",
    "\n",
    "\n",
    "def to_tensor(data):\n",
    "    \"\"\"\n",
    "    Convert numpy array to PyTorch tensor. For complex arrays, the real and imaginary parts\n",
    "    are stacked along the last dimension.\n",
    "\n",
    "    Args:\n",
    "        data (np.array): Input numpy array\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: PyTorch version of data\n",
    "    \"\"\"\n",
    "    if np.iscomplexobj(data):\n",
    "        data = np.stack((data.real, data.imag), axis=-1)\n",
    "    return torch.from_numpy(data)\n",
    "\n",
    "\n",
    "def apply_mask(data, mask_func, seed=None):\n",
    "    \"\"\"\n",
    "    Subsample given k-space by multiplying with a mask.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input k-space data. This should have at least 3 dimensions, where\n",
    "            dimensions -3 and -2 are the spatial dimensions, and the final dimension has size\n",
    "            2 (for complex values).\n",
    "        mask_func (callable): A function that takes a shape (tuple of ints) and a random\n",
    "            number seed and returns a mask.\n",
    "        seed (int or 1-d array_like, optional): Seed for the random number generator.\n",
    "\n",
    "    Returns:\n",
    "        (tuple): tuple containing:\n",
    "            masked data (torch.Tensor): Subsampled k-space data\n",
    "            mask (torch.Tensor): The generated mask\n",
    "    \"\"\"\n",
    "    shape = np.array(data.shape)\n",
    "    shape[:-3] = 1\n",
    "    mask = mask_func(shape, seed)\n",
    "    return torch.where(mask == 0, torch.Tensor([0]), data), mask\n",
    "\n",
    "\n",
    "def fft2(data):\n",
    "    \"\"\"\n",
    "    Apply centered 2 dimensional Fast Fourier Transform.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): Complex valued input data containing at least 3 dimensions: dimensions\n",
    "            -3 & -2 are spatial dimensions and dimension -1 has size 2. All other dimensions are\n",
    "            assumed to be batch dimensions.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The FFT of the input.\n",
    "    \"\"\"\n",
    "   # assert data.size(-1) == 2\n",
    "    data = ifftshift(data, dim=(-3, -2))\n",
    "    data = torch.fft(data, 2, normalized=True)\n",
    "    data = fftshift(data, dim=(-3, -2))\n",
    "    return data\n",
    "\n",
    "\n",
    "def ifft2(data):\n",
    "    \"\"\"\n",
    "    Apply centered 2-dimensional Inverse Fast Fourier Transform.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): Complex valued input data containing at least 3 dimensions: dimensions\n",
    "            -3 & -2 are spatial dimensions and dimension -1 has size 2. All other dimensions are\n",
    "            assumed to be batch dimensions.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The IFFT of the input.\n",
    "    \"\"\"\n",
    "    #assert data.size(-1) == 2\n",
    "    data = ifftshift(data, dim=(-3, -2))\n",
    "    data = torch.ifft(data, 2, normalized=True)\n",
    "    data = fftshift(data, dim=(-3, -2))\n",
    "    return data\n",
    "\n",
    "\n",
    "def complex_abs(data):\n",
    "    \"\"\"\n",
    "    Compute the absolute value of a complex valued input tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): A complex valued tensor, where the size of the final dimension\n",
    "            should be 2.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Absolute value of data\n",
    "    \"\"\"\n",
    "   # assert data.size(-1) == 2\n",
    "    return (data ** 2).sum(dim=-1).sqrt()\n",
    "\n",
    "\n",
    "def root_sum_of_squares(data, dim=0):\n",
    "    \"\"\"\n",
    "    Compute the Root Sum of Squares (RSS) transform along a given dimension of a tensor.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor\n",
    "        dim (int): The dimensions along which to apply the RSS transform\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The RSS value\n",
    "    \"\"\"\n",
    "    return torch.sqrt((data ** 2).sum(dim))\n",
    "\n",
    "\n",
    "def center_crop(data, shape):\n",
    "    \"\"\"\n",
    "    Apply a center crop to the input real image or batch of real images.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The input tensor to be center cropped. It should have at\n",
    "            least 2 dimensions and the cropping is applied along the last two dimensions.\n",
    "        shape (int, int): The output shape. The shape should be smaller than the\n",
    "            corresponding dimensions of data.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The center cropped image\n",
    "    \"\"\"\n",
    "    assert 0 < shape[0] <= data.shape[-2]\n",
    "    assert 0 < shape[1] <= data.shape[-1]\n",
    "    w_from = (data.shape[-2] - shape[0]) // 2\n",
    "    h_from = (data.shape[-1] - shape[1]) // 2\n",
    "    w_to = w_from + shape[0]\n",
    "    h_to = h_from + shape[1]\n",
    "    return data[..., w_from:w_to, h_from:h_to]\n",
    "\n",
    "\n",
    "def complex_center_crop(data, shape):\n",
    "    \"\"\"\n",
    "    Apply a center crop to the input image or batch of complex images.\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): The complex input tensor to be center cropped. It should\n",
    "            have at least 3 dimensions and the cropping is applied along dimensions\n",
    "            -3 and -2 and the last dimensions should have a size of 2.\n",
    "        shape (int, int): The output shape. The shape should be smaller than the\n",
    "            corresponding dimensions of data.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The center cropped image\n",
    "    \"\"\"\n",
    "    assert 0 < shape[0] <= data.shape[-3]\n",
    "    assert 0 < shape[1] <= data.shape[-2]\n",
    "    w_from = (data.shape[-3] - shape[0]) // 2\n",
    "    h_from = (data.shape[-2] - shape[1]) // 2\n",
    "    w_to = w_from + shape[0]\n",
    "    h_to = h_from + shape[1]\n",
    "    return data[..., w_from:w_to, h_from:h_to, :]\n",
    "\n",
    "\n",
    "def normalize(data, mean, stddev, eps=0.):\n",
    "    \"\"\"\n",
    "    Normalize the given tensor using:\n",
    "        (data - mean) / (stddev + eps)\n",
    "\n",
    "    Args:\n",
    "        data (torch.Tensor): Input data to be normalized\n",
    "        mean (float): Mean value\n",
    "        stddev (float): Standard deviation\n",
    "        eps (float): Added to stddev to prevent dividing by zero\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Normalized tensor\n",
    "    \"\"\"\n",
    "    return (data - mean) / (stddev + eps)\n",
    "\n",
    "\n",
    "def normalize_instance(data, eps=0.):\n",
    "    \"\"\"\n",
    "        Normalize the given tensor using:\n",
    "            (data - mean) / (stddev + eps)\n",
    "        where mean and stddev are computed from the data itself.\n",
    "\n",
    "        Args:\n",
    "            data (torch.Tensor): Input data to be normalized\n",
    "            eps (float): Added to stddev to prevent dividing by zero\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Normalized tensor\n",
    "        \"\"\"\n",
    "    mean = data.mean()\n",
    "    std = data.std()\n",
    "    return normalize(data, mean, std, eps), mean, std\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "\n",
    "def roll(x, shift, dim):\n",
    "    \"\"\"\n",
    "    Similar to np.roll but applies to PyTorch Tensors\n",
    "    \"\"\"\n",
    "    if isinstance(shift, (tuple, list)):\n",
    "        assert len(shift) == len(dim)\n",
    "        for s, d in zip(shift, dim):\n",
    "            x = roll(x, s, d)\n",
    "        return x\n",
    "\n",
    "    shift = shift % x.size(dim)\n",
    "    if shift == 0:\n",
    "        return x\n",
    "    left = x.narrow(dim, 0, x.size(dim) - shift)\n",
    "    right = x.narrow(dim, x.size(dim) - shift, shift)\n",
    "    return torch.cat((right, left), dim=dim)\n",
    "\n",
    "\n",
    "def fftshift(x, dim=None):\n",
    "    \"\"\"\n",
    "    Similar to np.fft.fftshift but applies to PyTorch Tensors\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        dim = tuple(range(x.dim()))\n",
    "        shift = [dim // 2 for dim in x.shape]\n",
    "    elif isinstance(dim, int):\n",
    "        shift = x.shape[dim] // 2\n",
    "    else:\n",
    "  #      flag = 0\n",
    " #       for i in dim:\n",
    "  #          if i < 0:\n",
    "  #              flag = 1\n",
    "  #      if flag == 1:\n",
    "   #         dim = tuple(range(x.dim()))\n",
    "   #         shift = [(dim + 1) // 2 for dim in x.shape]\n",
    "   #     else:\n",
    "   #         shift = [x.shape[i] // 2 for i in dim]\n",
    "        shift = [x.shape[i] // 2 for i in dim]\n",
    "    return roll(x, shift, dim)\n",
    "\n",
    "\n",
    "def ifftshift(x, dim=None):\n",
    "    \"\"\"\n",
    "    Similar to np.fft.ifftshift but applies to PyTorch Tensors\n",
    "    \"\"\"\n",
    "    if dim is None:\n",
    "        dim = tuple(range(x.dim()))\n",
    "        shift = [(dim + 1) // 2 for dim in x.shape]\n",
    "    elif isinstance(dim, int):\n",
    "        shift = (x.shape[dim] + 1) // 2\n",
    "    else:\n",
    "\n",
    "        shift = [x.shape[i] // 2 for i in dim]\n",
    "    return roll(x, shift, dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25861eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.fft import fftshift, ifftshift, fftn, ifftn\n",
    "\n",
    "def transform_kspace_to_image(k, dim=None, img_shape=None):\n",
    "    \"\"\" Computes the Fourier transform from k-space to image space\n",
    "    along a given or all dimensions\n",
    "    :param k: k-space data\n",
    "    :param dim: vector of dimensions to transform\n",
    "    :param img_shape: desired shape of output image\n",
    "    :returns: data in image space (along transformed dimensions)\n",
    "    \"\"\"\n",
    "    if not dim:\n",
    "        dim = range(k.ndim)\n",
    "\n",
    "    img = fftshift(ifftn(ifftshift(k, axes=dim), s=img_shape, axes=dim), axes=dim)\n",
    "    #img = fftshift(ifft2(ifftshift(k, dim=dim)), dim=dim)\n",
    "    img *= np.sqrt(np.prod(np.take(img.shape, dim)))\n",
    "    return img\n",
    "\n",
    "\n",
    "def transform_image_to_kspace(img, dim=None, k_shape=None):\n",
    "    \"\"\" Computes the Fourier transform from image space to k-space space\n",
    "    along a given or all dimensions\n",
    "    :param img: image space data\n",
    "    :param dim: vector of dimensions to transform\n",
    "    :param k_shape: desired shape of output k-space data\n",
    "    :returns: data in k-space (along transformed dimensions)\n",
    "    \"\"\"\n",
    "    if not dim:\n",
    "        dim = range(img.ndim)\n",
    "\n",
    "    k = fftshift(fftn(ifftshift(img, axes=dim), s=k_shape, axes=dim), axes=dim)\n",
    "    #k = fftshift(fft2(ifftshift(img, dim=dim)), dim=dim)\n",
    "    k /= np.sqrt(np.prod(np.take(img.shape, dim)))\n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e40fb11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def add_noise(array: np.ndarray, dropout_rate: float = 0.10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function adds noise\n",
    "    :param array:\n",
    "    :param dropout_rate: percent of pixels to be dropped\n",
    "    :return:\n",
    "    \"\"\"\n",
    "   # assert len(array.shape) == 4\n",
    "   # assert array.shape[1] == 3\n",
    "\n",
    "    channels = array.shape[1]\n",
    "    height = array.shape[2]\n",
    "    width = array.shape[3]\n",
    "\n",
    "    total_pixels = height * width\n",
    "    queued_pixels = int(total_pixels * dropout_rate)\n",
    "\n",
    "    filled_pixels = 0\n",
    "    while filled_pixels < queued_pixels:\n",
    "        d_h = random.randint(1, 3)\n",
    "        d_w = random.randint(1, 3)\n",
    "        filled_pixels += d_h * d_w\n",
    "        h = random.randint(0, height - d_h)\n",
    "        w = random.randint(0, width - d_w)\n",
    "\n",
    "        # now overwrite selected pixels with random dark color\n",
    "        array[:, :, h:h+d_h, w:w+d_w] = random.randint(1, 25) / 255.0\n",
    "\n",
    "    return array\n",
    "\n",
    "\n",
    "def array_to_image(array: np.ndarray):\n",
    "    \"\"\"\n",
    "    This function converts NumPy array to image\n",
    "    :param array:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    assert len(array.shape) == 4\n",
    "    assert array.shape[1] == 1 or array.shape[1] == 3\n",
    "\n",
    "    scaled = (array * 255.0).astype(np.uint8)\n",
    "    if array.shape[1] == 1:\n",
    "        reshaped = scaled.reshape((array.shape[2], array.shape[3]))\n",
    "        return Image.fromarray(reshaped,\"L\")\n",
    "    else:\n",
    "        reshaped = scaled.reshape((3, array.shape[2], array.shape[3])).transpose([1, 2, 0])\n",
    "        return Image.fromarray(reshaped,\"RGB\")\n",
    "\n",
    "\n",
    "def image_to_array(image) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This function converts image to NumPy array\n",
    "    :param image:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    image1 = np.array(image)\n",
    "    return np.expand_dims(np.asarray(image1), 0).astype(np.float) / 255.0\n",
    "\n",
    "\n",
    "def sample_grayscale(filepath: str, height: int = 128, width: int = 128) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This method samples patch out of image\n",
    "    :param filepath:\n",
    "    :param height:\n",
    "    :param width:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    image = Image.open(filepath)\n",
    "    #image = np.array(img)\n",
    "    grayscale = image.convert('L')\n",
    "    g_array = np.expand_dims(np.asarray(grayscale), 0).astype(dtype=np.float32) / 255.0\n",
    "    g_array = np.expand_dims(g_array, 0)\n",
    "    \n",
    "    return g_array\n",
    "    \n",
    "  #  h = random.randint(0, g_array.shape[2] - height)\n",
    "  #  w = random.randint(0, g_array.shape[3] - width)\n",
    "   # return g_array[:, :, h:h+height, w:w+width]\n",
    "\n",
    "\n",
    "def sample_rgb(filepath: str, height: int = 128, width: int = 128) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    This method samples patch out of image\n",
    "    :param filepath:\n",
    "    :param height:\n",
    "    :param width:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    img = Image.open(filepath)\n",
    "    image = np.array(img)\n",
    "    rgb_array = np.expand_dims(np.asarray(image).transpose([2, 0, 1]), 0).astype(dtype=np.float32) / 255.0\n",
    "    return rgb_array\n",
    "    # h = random.randint(0, rgb_array.shape[2] - height)\n",
    "  #  w = random.randint(0, rgb_array.shape[3] - width)\n",
    "   # return rgb_array[:, :, h:h+height, w:w+width]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa8e980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_slices(data, slice_nums, cmap=None): # visualisation\n",
    "    fig = plt.figure(figsize=(15,10))\n",
    "    for i, num in enumerate(slice_nums):\n",
    "        plt.subplot(1, len(slice_nums), i + 1)\n",
    "        plt.imshow(data[num], cmap=cmap)\n",
    "        plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be1f4551",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MRIDataset(DataLoader):\n",
    "    def __init__(self, data_list, acceleration, center_fraction, use_seed):\n",
    "        self.data_list = data_list\n",
    "        self.acceleration = acceleration\n",
    "        self.center_fraction = center_fraction\n",
    "        self.use_seed = use_seed\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        subject_id = self.data_list[idx]\n",
    "\n",
    "        return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b461eb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_epoch_batch(subject_id, acc, center_fract, use_seed=True):\n",
    "    ''' random select a few slices (batch_size) from each volume'''\n",
    "\n",
    "    fname, rawdata_name = subject_id  \n",
    "    \n",
    "#    with h5py.File(rawdata_name, 'r') as data:\n",
    "#        rawdata = data['kspace'][slice]\n",
    "   \n",
    "    im_frame = Image.open(rawdata_name)\n",
    "    np_frame = np.array(im_frame.getdata())\n",
    "   \n",
    "    k_frame = transform_image_to_kspace(np_frame)\n",
    "     #add noise\n",
    "    noise_k_frame = add_noise(k_frame, 0.3) #noise np array of image\n",
    "                                        # noise_k_frame + revert k_space = np_noise\n",
    "    \n",
    "    slice_kspace = to_tensor(k_frame).unsqueeze(0)\n",
    "    masked_kspace = to_tensor(noise_k_frame).unsqueeze(0)\n",
    "\n",
    "    img_gt, img_und = ifft2(slice_kspace), ifft2(masked_kspace)\n",
    "    \n",
    "    \n",
    "    # perform data normalization which is important for network to learn useful features\n",
    "    # during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "    norm = complex_abs(img_und).max()\n",
    "    if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "    img_gt, img_und, rawdata_und = img_gt/norm, img_und/norm, masked_kspace/norm\n",
    "      \n",
    "\n",
    "    img_gt = center_crop(complex_abs(img_gt), [320, 320])\n",
    "    img_und = center_crop(complex_abs(img_und), [320, 320])\n",
    "        \n",
    "    return img_gt.squeeze(0), img_und.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fa9499d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path, val_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_and_val = ['train', 'val']\n",
    "    data_path = [train_data_path, val_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)):\n",
    "\n",
    "        data_list[train_and_val[i]] = []\n",
    "        \n",
    "        which_data_path = data_path[i]\n",
    "        tr = 0\n",
    "        te = 0\n",
    "        alfa = 0\n",
    "    \n",
    "        for fname in sorted(os.listdir(which_data_path + '/images')):\n",
    "            if fname == '.DS_Store': continue\n",
    "            \n",
    "            subject_data_path = os.path.join(which_data_path + '/images', fname)\n",
    "                     \n",
    "            if not os.path.isfile(subject_data_path): continue \n",
    "            \n",
    "     \n",
    "            #get information from text file\n",
    "            # this will return a tuple of root and extension\n",
    "            split_tup = os.path.splitext(fname)\n",
    "\n",
    "  \n",
    "            # extract the file name and extension\n",
    "            file_name = split_tup[0]\n",
    "  \n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "            data_list[train_and_val[i]].append((fname, subject_data_path))\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62a1b551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 3.6000e-02, -4.3343e-16],\n",
      "         [ 1.5944e-02,  1.7316e-02],\n",
      "         [-1.6015e-02,  4.8264e-03],\n",
      "         ...,\n",
      "         [-2.1239e-02,  1.8685e-02],\n",
      "         [-1.6015e-02, -4.8264e-03],\n",
      "         [ 1.5944e-02, -1.7316e-02]]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "from scipy import misc\n",
    "import glob\n",
    "from PIL import Image\n",
    "#import tensorflow as tf\n",
    "ima = 1\n",
    "\n",
    "im_frame = Image.open(\"matlab/images/10.png\")\n",
    "np_frame = np.array(im_frame.getdata())\n",
    "    #tf_frame = tf.convert_to_tensor(np_frame)\n",
    "    #ts_frame = to_tensor(np_frame).unsqueeze(0)\n",
    "k_frame = transform_image_to_kspace(np_frame)\n",
    "result = to_tensor(k_frame).unsqueeze(0)\n",
    "  \n",
    "print(result)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd48432",
   "metadata": {},
   "source": [
    "# The initial model, based on the AlexNet architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd21176a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1), #320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64 ,64, kernel_size=3, padding=1),  # 320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1), #320/320\n",
    "            nn.Dropout2d(),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),  # 320/320\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 1, kernel_size=3, padding=1),  # 320/320\n",
    "            \n",
    "            \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b75232",
   "metadata": {},
   "source": [
    "# RestNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a018a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a75e07d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseBlock(torch.nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self,input_planes,planes,stride=1,dim_change=None):\n",
    "        super(baseBlock,self).__init__()\n",
    "        #declare convolutional layers with batch norms\n",
    "        self.conv1 = torch.nn.Conv2d(input_planes,planes,stride=stride,kernel_size=3,padding=1)\n",
    "        self.bn1   = torch.nn.BatchNorm2d(planes)\n",
    "        self.conv2 = torch.nn.Conv2d(planes,planes,stride=1,kernel_size=3,padding=1)\n",
    "        self.bn2   = torch.nn.BatchNorm2d(planes)\n",
    "        self.dim_change = dim_change\n",
    "    def forward(self,x):\n",
    "        #Save the residue\n",
    "        res = x\n",
    "        output = F.relu(self.bn1(self.conv1(x)))\n",
    "        output = self.bn2(self.conv2(output))\n",
    "        if self.dim_change is not None:\n",
    "            res = self.dim_change(res)\n",
    "        \n",
    "        output += res\n",
    "        output = F.relu(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "class ResNet(torch.nn.Module):\n",
    "    def __init__(self,block,num_layers,classes=10):\n",
    "        super(ResNet,self).__init__()\n",
    "        #according to research paper:\n",
    "        self.input_planes = 64\n",
    "        self.conv1 = torch.nn.Conv2d(1,64,kernel_size=3,stride=1,padding=1)\n",
    "        self.layer1 = self._layer(block,64,num_layers[0],stride=1)\n",
    "        self.layer2 = self._layer(block,64,num_layers[1],stride=1)\n",
    "        self.layer3 = self._layer(block,64,num_layers[2],stride=1)\n",
    "        self.layer4 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer5 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer6 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer7 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer8 = self._layer(block,64,num_layers[2],stride=1)\n",
    "        self.layer9 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer10 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer11 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer12 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer13 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer14 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer15 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer16 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer17 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.layer18 = self._layer(block,64,num_layers[3],stride=1)\n",
    "        self.conv2 = torch.nn.Conv2d(64,1,kernel_size=3,stride=1, padding=1)\n",
    "        \n",
    "    \n",
    "    def _layer(self,block,planes,num_layers,stride=1):\n",
    "        dim_change = None\n",
    "        if stride!=1 or planes != self.input_planes*block.expansion:\n",
    "            dim_change = torch.nn.Sequential(torch.nn.Conv2d(self.input_planes,planes*block.expansion,kernel_size=1,stride=stride),\n",
    "                                             torch.nn.BatchNorm2d(planes*block.expansion))\n",
    "        netLayers =[]\n",
    "        netLayers.append(block(self.input_planes,planes,stride=stride,dim_change=dim_change))\n",
    "        self.input_planes = planes * block.expansion\n",
    "        for i in range(1,num_layers):\n",
    "            netLayers.append(block(self.input_planes,planes))\n",
    "            self.input_planes = planes * block.expansion\n",
    "        \n",
    "        return torch.nn.Sequential(*netLayers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.layer5(x)\n",
    "        x = self.layer6(x)\n",
    "        x = self.layer7(x)\n",
    "        x = self.layer8(x)\n",
    "        x = self.layer9(x)\n",
    "        x = self.layer10(x)\n",
    "        x = self.layer11(x)\n",
    "        x = self.layer12(x)\n",
    "        x = self.layer13(x)\n",
    "        x = self.layer14(x)\n",
    "        x = self.layer15(x)\n",
    "        x = self.layer16(x)\n",
    "        x = self.layer17(x)\n",
    "        x = self.layer18(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ed409850",
   "metadata": {},
   "source": [
    "Changed in version 0.16: This function was renamed from skimage.measure.compare_ssim to skimage.metrics.structural_similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d66fe25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as cmp_ssim \n",
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return cmp_ssim(\n",
    "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2df39090",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (file signature not found)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yr/qwqgmggj2_12p38pnv1hf6380000gp/T/ipykernel_3079/1202113998.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mdata_path_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'matlab/images'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mdata_path_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'matlab/images'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mdata_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path_val\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# first load all file names, paths and slices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/yr/qwqgmggj2_12p38pnv1hf6380000gp/T/ipykernel_3079/2195987256.py\u001b[0m in \u001b[0;36mload_data_path\u001b[0;34m(train_data_path, val_data_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_data_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_data_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                 \u001b[0mnum_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kspace'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                                swmr=swmr)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[0;34m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r+'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Unable to open file (file signature not found)"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    data_path_train = 'matlab/images'\n",
    "    data_path_val = 'matlab/images'\n",
    "    data_list = load_data_path(data_path_train, data_path_val) # first load all file names, paths and slices.\n",
    "    \n",
    "    acc = 4\n",
    "    cen_fract = 0.08\n",
    "    seed = False # random masks for each slice \n",
    "    num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "    lr = 1e-3\n",
    "    \n",
    "    network_4fold = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "    network_4fold.to('cuda:0') #move the model on the GPU\n",
    "    mae_loss = nn.L1Loss().to('cuda:0')\n",
    "    \n",
    "    optimizer = optim.Adam(network_4fold.parameters(), lr=lr)\n",
    "    fixed_train_set = []\n",
    "    for val in data_list['train']:\n",
    "        if val[2] > 5:\n",
    "            fixed_train_set.append(val)\n",
    "    #create data loader for training set. It applies same to validation set as well\n",
    "    train_dataset = MRIDataset(fixed_train_set, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "    \n",
    "    losses=[]\n",
    "    mean_loss_list = []\n",
    "    img_nr = 0\n",
    "    for epoch in range(1):\n",
    "        for iteration, sample in enumerate(train_loader):\n",
    "            img_nr += 1\n",
    "            img_gt, img_und = sample\n",
    "        \n",
    "            img_gt = img_gt.unsqueeze(1).to('cuda:0')\n",
    "            img_und = img_und.unsqueeze(1).to('cuda:0')\n",
    "\n",
    "            output = network_4fold(img_und)       #feedforward\n",
    "\n",
    "\n",
    "            loss = mae_loss(output, img_gt)\n",
    "\n",
    "            optimizer.zero_grad()       #set current gradients to 0\n",
    "            loss.backward()      #backpropagate\n",
    "            optimizer.step()     #update the weights\n",
    "            mean_loss_list.append(loss.item())\n",
    "            #compute and print the mean L1 lossscore for the last 20 training images.\n",
    "            if img_nr%20 == 0:\n",
    "                print(\"L1 Loss score: \", np.round(np.mean(mean_loss_list), decimals = 5), \"  Image number: \", img_nr, \"  Epoch: \", epoch+1)\n",
    "                mean_loss_list = []\n",
    "            losses.append(loss.item() * img_gt.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22904887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#L1 loss plot ofr the 4-fold data\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6a8c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_improvement = []\n",
    "SSIM_score = []\n",
    "for i in range(0,len(train_dataset)):\n",
    "    gt, image = train_dataset[i]\n",
    "    image = image.unsqueeze(0).to('cuda:0')\n",
    "    image = image.unsqueeze(0)\n",
    "    gt = gt.unsqueeze(0).numpy()\n",
    "    output = network_4fold(image)\n",
    "    output = output.squeeze(1).cpu().detach().numpy()\n",
    "    output_loss = torch.tensor(ssim(gt, output))\n",
    "    image_loss = torch.tensor(ssim(gt, image.squeeze(1).cpu().numpy()))\n",
    "    SSIM_improvement.append(output_loss.item()-image_loss.item())\n",
    "    SSIM_score.append(output_loss.item())\n",
    "\n",
    "print(np.nanmean(SSIM_improvement))\n",
    "print(np.nanmean(SSIM_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9240cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_improvement.sort()\n",
    "plt.plot(SSIM_improvement)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "36d7b12e",
   "metadata": {},
   "source": [
    "All the code from here on is almost the same as before, only for the 8fold data. The main difference is the learning rate, which is 3 times higher for the 8fold data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4465b107",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-19-f510181ece07>\", line 14, in __getitem__\n    return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)\n  File \"<ipython-input-20-a66fef96421b>\", line 14, in get_epoch_batch\n    noise_k_frame = add_noise(k_frame, 0.3) #noise np array of image\n  File \"<ipython-input-5-6f0348ac40d7>\", line 17, in add_noise\n    channels = array.shape[1]\nIndexError: tuple index out of range\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-07329be63312>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mimg_nr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mimg_nr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mimg_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_und\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    857\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;31m# (https://bugs.python.org/issue2651), so we work around it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyErrorMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Caught IndexError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/worker.py\", line 178, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-19-f510181ece07>\", line 14, in __getitem__\n    return get_epoch_batch(subject_id, self.acceleration, self.center_fraction, self.use_seed)\n  File \"<ipython-input-20-a66fef96421b>\", line 14, in get_epoch_batch\n    noise_k_frame = add_noise(k_frame, 0.3) #noise np array of image\n  File \"<ipython-input-5-6f0348ac40d7>\", line 17, in add_noise\n    channels = array.shape[1]\nIndexError: tuple index out of range\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    data_path_train = 'data/train'\n",
    "    data_path_val = 'data/val'\n",
    "    data_list = load_data_path(data_path_train, data_path_val)\n",
    "    \n",
    "    acc = 8\n",
    "    cen_fract = 0.04\n",
    "    seed = False # random masks for each slice \n",
    "    num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "    mae_loss = nn.L1Loss().to('cuda:0')\n",
    "    #mae_loss = nn.L1Loss()\n",
    "    lr = 3e-3\n",
    "    #acc =8 , network_8fold\n",
    "    network_8fold = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "    network_8fold.to('cuda:0') #move the model on the GPU\n",
    "\n",
    "    \n",
    "    optimizer2 = optim.Adam(network_8fold.parameters(), lr=lr)\n",
    "    fixed_train_set = []\n",
    "    #for val in data_list['train']:\n",
    "      #  print(f'val shape {val}')\n",
    "      #  if val[2] > 5:\n",
    "       #     fixed_train_set.append(val)\n",
    "    #create data loader for training set. It applies same to validation set as well\n",
    "    #train_dataset = MRIDataset(fixed_train_set, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "    \n",
    "    losses2=[]\n",
    "    mean_loss_list = []\n",
    "    img_nr = 0\n",
    "    for epoch in range(3):\n",
    "        for iteration, sample in enumerate(train_loader):\n",
    "            img_nr += 1\n",
    "            img_gt, img_und = sample\n",
    "        \n",
    "            img_gt = img_gt.unsqueeze(1).to('cuda:0')\n",
    "            img_und = img_und.unsqueeze(1).to('cuda:0')\n",
    "\n",
    "            output = network_8fold(img_und)      #feedforward\n",
    "\n",
    "            loss = mae_loss(output, img_gt)\n",
    "\n",
    "            optimizer2.zero_grad()       #set current gradients to 0\n",
    "            loss.backward()      #backpropagate\n",
    "            optimizer2.step()     #update the weights\n",
    "            mean_loss_list.append(loss.item())\n",
    "            #compute and print the mean L1 lossscore for the last 20 training images.\n",
    "            if img_nr%20 == 0:\n",
    "                print(\"L1 Loss score: \", np.round(np.mean(mean_loss_list), decimals = 5), \"  Image number: \", img_nr, \"  Epoch: \", epoch+1)\n",
    "                mean_loss_list = []\n",
    "            losses2.append(loss.item() * img_gt.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a029008c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    data_path_train = 'data/train'\n",
    "    data_path_val = 'data/val'\n",
    "    data_list = load_data_path(data_path_train, data_path_val)\n",
    "    \n",
    "    acc = 8\n",
    "    cen_fract = 0.04\n",
    "    seed = False # random masks for each slice \n",
    "    num_workers = 12 # data loading is faster using a bigger number for num_workers. 0 means using one cpu to load data\n",
    "    \n",
    "    mae_loss = nn.L1Loss().to('cuda:0')\n",
    "    #mae_loss = nn.L1Loss()\n",
    "    lr = 3e-3\n",
    "    #acc =8 , network_8fold\n",
    "    network_8fold = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "    network_8fold.to('cuda:0') #move the model on the GPU\n",
    "\n",
    "    \n",
    "    optimizer2 = optim.Adam(network_8fold.parameters(), lr=lr)\n",
    "    fixed_train_set = []\n",
    "    #for val in data_list['train']:\n",
    "      #  print(f'val shape {val}')\n",
    "      #  if val[2] > 5:\n",
    "       #     fixed_train_set.append(val)\n",
    "    #create data loader for training set. It applies same to validation set as well\n",
    "    #train_dataset = MRIDataset(fixed_train_set, acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_dataset = MRIDataset(data_list['train'], acceleration=acc, center_fraction=cen_fract, use_seed=seed)\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=1, num_workers=num_workers) \n",
    "    print(len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8126f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead95078",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the SSIM score for every image after a feedforward propagation through \n",
    "#the network.\n",
    "#Subtract the image SSIM score before the feedforward prop to obtain the net improvement for every image.\n",
    "#Print the average improvement and the average SSIM score after the reconstruction.\n",
    "SSIM_improvement = []\n",
    "SSIM_score = []\n",
    "for i in range(0,len(train_dataset)):\n",
    "    gt, image = train_dataset[i]\n",
    "    image = image.unsqueeze(0).to('cuda:0')\n",
    "    #image = image.unsqueeze(0)\n",
    "    gt = gt.unsqueeze(0).numpy()\n",
    "    output = network_8fold(image)\n",
    "    output = output.squeeze(1).cpu().detach().numpy()\n",
    "    output_loss = torch.tensor(ssim(gt, output))\n",
    "    image_loss = torch.tensor(ssim(gt, image.squeeze(1).cpu().numpy()))\n",
    "    SSIM_improvement.append(output_loss.item()-image_loss.item())\n",
    "    SSIM_score.append(output_loss.item())\n",
    "\n",
    "print(np.nanmean(SSIM_improvement))\n",
    "print(np.nanmean(SSIM_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f540973",
   "metadata": {},
   "outputs": [],
   "source": [
    "SSIM_improvement.sort()\n",
    "plt.plot(SSIM_improvement)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a323f",
   "metadata": {},
   "source": [
    "## save Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "139b0d99",
   "metadata": {},
   "source": [
    "https://learn-pytorch.oneoffcoder.com/model-persistence.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e9ee87",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e914ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"s3://savemodels/network_8fold/restnet-model{index}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b240d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model to S3 bucket or data\n",
    "torch.save(network_8fold.state_dict(), output_dir)\n",
    "#torch.save(network_8fold.state_dict(), './models/resnet18-model.pt')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a7534e4",
   "metadata": {},
   "source": [
    "#save the whole model\n",
    "torch.save(network_8fold, output_dir)\n",
    "#torch.save(network_8fold, './models/resnet18-model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d494ec85",
   "metadata": {},
   "source": [
    "## Load Model from saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1d80d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"s3://savemodels/network_8fold/restnet-model{index}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f00b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model on GPU\n",
    "device = torch.device(\"cuda\")\n",
    "model = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "\n",
    "#model = model.to('cuda:0')\n",
    "model.load_state_dict(torch.load(output_dir, map_location='cuda:0'))\n",
    "#model.load_state_dict(torch.load('./models/resnet18-model.pt', map_location='cuda:0'))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6cabd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13502a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model on CPU: laptop\n",
    "device = torch.device('cpu')\n",
    "#model = TheModelClass(*args, **kwargs)\n",
    "model = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "#model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "model.load_state_dict(torch.load(output_dir, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "79e038f0",
   "metadata": {},
   "source": [
    "model = torch.load(PATH)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047160e8",
   "metadata": {},
   "source": [
    "## Predict a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b95fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = f\"s3://savemodels/network_8fold/restnet-model{index}.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d93592",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model on CPU: laptop\n",
    "device = torch.device('cpu')\n",
    "#model = TheModelClass(*args, **kwargs)\n",
    "model = ResNet(baseBlock,[2, 2, 2, 2, 3, 3, 3, 3, 2, 2, 2, 2])\n",
    "#model.load_state_dict(torch.load(PATH, map_location=device))\n",
    "model.load_state_dict(torch.load(model_dir, map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19480abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4578bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"/data/test/images/101.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1df154",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_frame = Image.open(file_dir)\n",
    "np_frame = np.array(im_frame.getdata())\n",
    "   \n",
    "k_frame = transform_image_to_kspace(np_frame)\n",
    "    \n",
    "     #add noise\n",
    "noise_k_frame = add_noise(k_frame, 0.3) #noise np array of image\n",
    "                                        # noise_k_frame + revert k_space = np_noise\n",
    "    \n",
    "slice_kspace = to_tensor(k_frame).unsqueeze(0)\n",
    "masked_kspace = to_tensor(noise_k_frame).unsqueeze(0)\n",
    "\n",
    "img_gt, noise_image = ifft2(slice_kspace), ifft2(masked_kspace)\n",
    "    \n",
    "    \n",
    "# perform data normalization which is important for network to learn useful features\n",
    "# during inference there is no ground truth image so use the zero-filled recon to normalize\n",
    "norm = complex_abs(img_und).max()\n",
    "if norm < 1e-6: norm = 1e-6\n",
    "    \n",
    "img_gt, noise_image, rawdata_und = img_gt/norm, noise_image/norm, masked_kspace/norm\n",
    "      \n",
    "\n",
    "img_gt = center_crop(complex_abs(img_gt), [320, 320])\n",
    "noise_image = center_crop(complex_abs(noise_image), [320, 320]) #use for testing\n",
    "####################\n",
    "#image = noise_image.unsqueeze(0) , no need to unsqueeze because it was not squeeze\n",
    "k_noise_image = noise_k_frame # k_noise numpy array\n",
    "np_noise_image = transform_kspace_to_image(k_noise_image)# image noise numpy array\n",
    "im_noise = Image.fromarray(np_noise_image)\n",
    "im_noise.save(\"noise.png\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f76ce163",
   "metadata": {},
   "source": [
    "https://www.kite.com/python/examples/4887/PIL-convert-between-a-pil-%60image%60-and-a-numpy-%60array%60\n",
    "\n",
    "https://stackoverflow.com/questions/2659312/how-do-i-convert-a-numpy-array-to-and-display-an-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "display(im_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f3d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as cmp_ssim \n",
    "def ssim(gt, pred):\n",
    "    \"\"\" Compute Structural Similarity Index Metric (SSIM). \"\"\"\n",
    "    return cmp_ssim(\n",
    "        gt.transpose(1, 2, 0), pred.transpose(1, 2, 0), multichannel=True, data_range=gt.max()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123237bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #noise_image = noise_image.to('cuda:0')\n",
    "    \n",
    "img_gt = img_gt.numpy()\n",
    "output = model(noise_image)\n",
    "   # output = output.squeeze(1).cpu().detach().numpy()\n",
    "output = output.squeeze(1).detach().numpy()   #image under numpy form\n",
    "output_loss = torch.tensor(ssim(img_gt, output))  \n",
    "image_loss = torch.tensor(ssim(img_gt, output.squeeze(1).numpy()))\n",
    "SSIM_improvement = (output_loss.item()-image_loss.item())\n",
    "SSIM_score = output_loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ee2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_rescontruct_image = transform_kspace_to_image(output)# image noise numpy array\n",
    "im_reconstruct = Image.fromarray(np_rescontruct_image)\n",
    "im_reconstruct.save(\"testing/test.png\") #for prediction values\n",
    "im_reconstruct.save(\"pred1.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
