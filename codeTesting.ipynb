{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b348c738-5676-44cb-ae94-c6f9a2810265",
   "metadata": {},
   "source": [
    "# Machine Learning- reconstruct Image from partial/incomplete image"
   ]
  },
  {
   "cell_type": "raw",
   "id": "19a9d273-16b9-41c9-a95f-1c15a875adf5",
   "metadata": {},
   "source": [
    "https://github.com/simontomaskarlsson/GAN-MRI\n",
    "\n",
    "https://github.com/puneesh00/cs-mri-gan\n",
    "\n",
    "https://medium.com/howtoai/gan-for-medical-imaging-generating-images-and-annotations-8ad7c778809c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0654ab55-34ac-4552-b769-6d985ad316e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995621a2-ef6a-4d94-98da-15f7f13240f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Layer, Input, Conv2D, Activation, add, BatchNormalization, UpSampling2D, ZeroPadding2D, Conv2DTranspose, Flatten, MaxPooling2D, AveragePooling2D\n",
    "from keras_contrib.layers.normalization import InstanceNormalization, InputSpec\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.core import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.backend import mean\n",
    "from keras.models import Model, model_from_json\n",
    "from keras.utils import plot_model\n",
    "from keras.engine.topology import Container\n",
    "\n",
    "from collections import OrderedDict\n",
    "from scipy.misc import imsave, toimage  # has depricated\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import csv\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24fcbca-4fc9-4b25-889a-4ae2bb85d39d",
   "metadata": {},
   "source": [
    "## 1/ Extract information from incomplete image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6c0427-a9ca-4b77-913f-b786eda1c925",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from keras.utils import Sequence\n",
    "#from skimage.io import imread\n",
    "\n",
    "\n",
    "def load_data(nr_of_channels, batch_size=1, nr_A_train_imgs=None, nr_B_train_imgs=None,\n",
    "              nr_A_test_imgs=None, nr_B_test_imgs=None, subfolder='',\n",
    "              generator=False, D_model=None, use_multiscale_discriminator=False, use_supervised_learning=False, REAL_LABEL=1.0):\n",
    "\n",
    "    trainA_path = os.path.join('data', subfolder, 'trainA')\n",
    "    trainB_path = os.path.join('data', subfolder, 'trainB')\n",
    "    testA_path = os.path.join('data', subfolder, 'testA')\n",
    "    testB_path = os.path.join('data', subfolder, 'testB')\n",
    "\n",
    "    trainA_image_names = os.listdir(trainA_path)\n",
    "    if nr_A_train_imgs != None:\n",
    "        trainA_image_names = trainA_image_names[:nr_A_train_imgs]\n",
    "\n",
    "    trainB_image_names = os.listdir(trainB_path)\n",
    "    if nr_B_train_imgs != None:\n",
    "        trainB_image_names = trainB_image_names[:nr_B_train_imgs]\n",
    "\n",
    "    testA_image_names = os.listdir(testA_path)\n",
    "    if nr_A_test_imgs != None:\n",
    "        testA_image_names = testA_image_names[:nr_A_test_imgs]\n",
    "\n",
    "    testB_image_names = os.listdir(testB_path)\n",
    "    if nr_B_test_imgs != None:\n",
    "        testB_image_names = testB_image_names[:nr_B_test_imgs]\n",
    "\n",
    "    if generator:\n",
    "        return data_sequence(trainA_path, trainB_path, trainA_image_names, trainB_image_names, batch_size=batch_size)  # D_model, use_multiscale_discriminator, use_supervised_learning, REAL_LABEL)\n",
    "    else:\n",
    "        trainA_images = create_image_array(trainA_image_names, trainA_path, nr_of_channels)\n",
    "        trainB_images = create_image_array(trainB_image_names, trainB_path, nr_of_channels)\n",
    "        testA_images = create_image_array(testA_image_names, testA_path, nr_of_channels)\n",
    "        testB_images = create_image_array(testB_image_names, testB_path, nr_of_channels)\n",
    "        return {\"trainA_images\": trainA_images, \"trainB_images\": trainB_images,\n",
    "                \"testA_images\": testA_images, \"testB_images\": testB_images,\n",
    "                \"trainA_image_names\": trainA_image_names,\n",
    "                \"trainB_image_names\": trainB_image_names,\n",
    "                \"testA_image_names\": testA_image_names,\n",
    "                \"testB_image_names\": testB_image_names}\n",
    "\n",
    "\n",
    "def create_image_array(image_list, image_path, nr_of_channels):\n",
    "    image_array = []\n",
    "    for image_name in image_list:\n",
    "        if image_name[-1].lower() == 'g':  # to avoid e.g. thumbs.db files\n",
    "            if nr_of_channels == 1:  # Gray scale image -> MR image\n",
    "                image = np.array(Image.open(os.path.join(image_path, image_name)))\n",
    "                image = image[:, :, np.newaxis]\n",
    "            else:                   # RGB image -> 3 channels\n",
    "                image = np.array(Image.open(os.path.join(image_path, image_name)))\n",
    "            image = normalize_array(image)\n",
    "            image_array.append(image)\n",
    "\n",
    "    return np.array(image_array)\n",
    "  \n",
    "  \n",
    "  # If using 16 bit depth images, use the formula 'array = array / 32767.5 - 1' instead\n",
    "def normalize_array(array):\n",
    "    array = array / 127.5 - 1\n",
    "    return array\n",
    "\n",
    "\n",
    "class data_sequence(Sequence):\n",
    "\n",
    "    def __init__(self, trainA_path, trainB_path, image_list_A, image_list_B, batch_size=1):  # , D_model, use_multiscale_discriminator, use_supervised_learning, REAL_LABEL):\n",
    "        self.batch_size = batch_size\n",
    "        self.train_A = []\n",
    "        self.train_B = []\n",
    "        for image_name in image_list_A:\n",
    "            if image_name[-1].lower() == 'g':  # to avoid e.g. thumbs.db files\n",
    "                self.train_A.append(os.path.join(trainA_path, image_name))\n",
    "        for image_name in image_list_B:\n",
    "            if image_name[-1].lower() == 'g':  # to avoid e.g. thumbs.db files\n",
    "                self.train_B.append(os.path.join(trainB_path, image_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(max(len(self.train_A), len(self.train_B)) / float(self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):  # , use_multiscale_discriminator, use_supervised_learning):if loop_index + batch_size >= min_nr_imgs:\n",
    "        if idx >= min(len(self.train_A), len(self.train_B)):\n",
    "            # If all images soon are used for one domain,\n",
    "            # randomly pick from this domain\n",
    "            if len(self.train_A) <= len(self.train_B):\n",
    "                indexes_A = np.random.randint(len(self.train_A), size=self.batch_size)\n",
    "                batch_A = []\n",
    "                for i in indexes_A:\n",
    "                    batch_A.append(self.train_A[i])\n",
    "                batch_B = self.train_B[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            else:\n",
    "                indexes_B = np.random.randint(len(self.train_B), size=self.batch_size)\n",
    "                batch_B = []\n",
    "                for i in indexes_B:\n",
    "                    batch_B.append(self.train_B[i])\n",
    "                batch_A = self.train_A[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        else:\n",
    "            batch_A = self.train_A[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "            batch_B = self.train_B[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        real_images_A = create_image_array(batch_A, '', 3)\n",
    "        real_images_B = create_image_array(batch_B, '', 3)\n",
    "\n",
    "        return real_images_A, real_images_B  # input_data, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbccb94-3aea-4a8d-a84f-50d6e4a7c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractInformation(image):\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107206fa-235d-4d78-968f-c8bdcfff7c66",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6cbf42c-54f7-4bf0-94fa-0af9c2503449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0152563a-4232-46b2-bc6e-15e8a2376be0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1cf42-8373-4ea4-a25a-4510203b87c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f543c15-d1ab-4f30-a27f-8c7735eb16c3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd8fc22-d258-4721-8365-d5a5cd73e7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
