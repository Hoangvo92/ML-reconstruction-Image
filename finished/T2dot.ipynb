{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2b25b451-e7e8-476a-8910-cf745809ef08",
   "metadata": {},
   "source": [
    "get T2* from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8f951d4e-51fb-4521-81cd-48eb1c5e2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\"\"\"Import from keras_preprocessing not from keras.preprocessing, because Keras may or maynot contain the features discussed here depending upon when you read this article, until the keras_preprocessed library is updated in Keras use the github version.\"\"\"\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "#from tensorflow.keras import optimizers #., optimizers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#Importing all the relevant library\n",
    "%matplotlib inline\n",
    "import h5py, os\n",
    "#from functions import transforms as T\n",
    "#from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "#from functions import transforms as T \n",
    "#from functions.subsample import MaskFunc\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cd8d08e5-d611-4b99-add9-02cc854a9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path, val_data_path, test_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_val_test = ['train', 'val', 'test']\n",
    "    data_path = [train_data_path, val_data_path, test_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)):\n",
    "\n",
    "        data_list[train_val_test[i]] = []\n",
    "        \n",
    "        which_data_path = data_path[i]\n",
    "        t1 = 0\n",
    "        t2 = 0\n",
    "        t2dot = 0\n",
    "        tr = 0\n",
    "        te = 0\n",
    "        alfa = 0\n",
    "        for fname in sorted(os.listdir(which_data_path + '/images')):\n",
    "            if fname != \".DS_Store\":\n",
    "\n",
    "            \n",
    "                subject_data_path = os.path.join(which_data_path + '/images', fname)\n",
    "                     \n",
    "                if not os.path.isfile(subject_data_path): continue \n",
    "            \n",
    "          #  im_frame = Image.open(subject_data_path)\n",
    "\n",
    "            #get information from text file\n",
    "            # this will return a tuple of root and extension\n",
    "                split_tup = os.path.splitext(fname)\n",
    "\n",
    "  \n",
    "            # extract the file name and extension\n",
    "                file_name = split_tup[0]\n",
    "                file_path = os.path.join(which_data_path + '/texts', file_name) + '.txt'\n",
    "                f = open(os.path.join(which_data_path + '/texts', file_name) + '.txt', 'r')\n",
    "                line = f.readlines()[1]\n",
    "            \n",
    "                fields = line.split(',')\n",
    "                t1 = float(fields[0])\n",
    "                t2 = float(fields[1])\n",
    "                t2dot = float(fields[2])\n",
    "                tr = int(fields[3])\n",
    "                te = int(fields[4])\n",
    "                alfa = int(fields[5])\n",
    "                f.close()\n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "                data_list[train_val_test[i]] += [(fname, t2dot)]\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e859bb9f-fa84-4609-bf9e-e2decd504740",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = load_data_path (\"data/train\", \"data/val\", \"data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "81e6528b-7dbc-44ca-86f3-810e789cba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_list['train']\n",
    "val_data = data_list['val']\n",
    "test_data = data_list['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08aa5111-5a82-4237-ba6c-72ee17a69ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_data, columns=['fnames', 'labels'])\n",
    "train_df['labels']= train_df['labels'].astype(float)\n",
    "train_df['labels'] = round(train_df['labels'], 3)\n",
    "train_df['labels']= train_df['labels'].astype(str)\n",
    "train_df['fnames']= train_df['fnames'].astype(str)\n",
    "val_df = pd.DataFrame(val_data, columns=['fnames', 'labels'])\n",
    "val_df['labels']= val_df['labels'].astype(float)\n",
    "val_df['labels'] = round(val_df['labels'], 3)\n",
    "val_df['labels']= val_df['labels'].astype(str)\n",
    "val_df['fnames']= val_df['fnames'].astype(str)\n",
    "test_df = pd.DataFrame(test_data, columns=['fnames', 'labels'])\n",
    "test_df['labels']= test_df['labels'].astype(float)\n",
    "test_df['labels'] = round(test_df['labels'], 3)\n",
    "test_df['labels']= test_df['labels'].astype(str)\n",
    "test_df['fnames']= test_df['fnames'].astype(str)\n",
    "labels = train_df.labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce8641ab-96af-4fea-8b4e-2b4e38f108a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(labels)\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "163727c7-6099-4c4b-9311-877115a251da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1.99', '0.062', '0.0', '-0.081', '-0.543', '-0.123', '-0.088',\n",
       "       '-0.314', '0.088', '-0.094', '-0.087', '-0.089', '-0.114',\n",
       "       '-0.113', '-0.115', '-0.122', '-0.091', '-0.116', '-0.112',\n",
       "       '-0.093', '-0.148', '-0.09', '-0.084', '-0.107', '-0.151',\n",
       "       '-0.136', '-0.105', '-0.095', '-0.127', '-0.145', '-0.097',\n",
       "       '-0.11', '-0.108', '-0.119', '-0.149', '-0.141', '-0.131',\n",
       "       '-0.137', '-0.147', '-0.143', '-0.12', '-0.374', '-0.117',\n",
       "       '-0.128', '-0.152', '-0.13', '-0.146', '-0.54', '-0.15', '-0.134',\n",
       "       '-0.124', '-0.083', '-0.109', '-0.135', '-0.153', '-0.129',\n",
       "       '-0.14', '-0.138', '-0.133', '-0.086', '-0.132', '-0.106',\n",
       "       '-0.125', '-0.102', '-0.103', '-0.144', '-0.092', '-0.085',\n",
       "       '-0.359', '-0.139', '-0.111', '-0.101', '-0.096', '-0.126',\n",
       "       '-0.099', '-0.46', '-0.082', '-0.142', '-0.571', '-0.098',\n",
       "       '-0.104', '-0.121', '-0.118', '-0.573', '-0.1', '-0.325', '-0.529',\n",
       "       '-0.315', '-0.494', '-0.566', '-0.395'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a3c7e74a-edb4-4d1a-b1f4-52addf03c259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of         fnames  labels\n",
       "0        1.png    1.99\n",
       "1       10.png    1.99\n",
       "2      100.png    1.99\n",
       "3     1000.png    1.99\n",
       "4     1001.png   0.062\n",
       "...        ...     ...\n",
       "3704   995.png    1.99\n",
       "3705   996.png     0.0\n",
       "3706   997.png    1.99\n",
       "3707   998.png  -0.395\n",
       "3708   999.png    1.99\n",
       "\n",
       "[3709 rows x 2 columns]>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "beca63cc-5587-4daf-b728-00942ebfa873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3709 validated image filenames belonging to 91 classes.\n",
      "Found 749 validated image filenames belonging to 63 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "      dataframe=train_df,\n",
    "      directory=\"./data/train/images/\",\n",
    "      x_col=\"fnames\",\n",
    "      y_col=\"labels\",\n",
    "      batch_size=64,\n",
    "      seed=42,\n",
    "      shuffle=True,\n",
    "      class_mode=\"categorical\",\n",
    "      target_size=(224,224))\n",
    "\n",
    "\n",
    "\n",
    "valid_generator=test_datagen.flow_from_dataframe(\n",
    "      dataframe=val_df,\n",
    "      directory=\"./data/val/images/\",\n",
    "      x_col=\"fnames\",\n",
    "      y_col=\"labels\",\n",
    "      batch_size=64,\n",
    "      seed=42,\n",
    "      shuffle=True,\n",
    "      class_mode=\"categorical\",\n",
    "     target_size=(224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9220a40c-673a-43da-bf1e-99206716e291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-0.081': 0,\n",
       " '-0.082': 1,\n",
       " '-0.083': 2,\n",
       " '-0.084': 3,\n",
       " '-0.085': 4,\n",
       " '-0.086': 5,\n",
       " '-0.087': 6,\n",
       " '-0.088': 7,\n",
       " '-0.089': 8,\n",
       " '-0.09': 9,\n",
       " '-0.091': 10,\n",
       " '-0.092': 11,\n",
       " '-0.093': 12,\n",
       " '-0.094': 13,\n",
       " '-0.095': 14,\n",
       " '-0.096': 15,\n",
       " '-0.097': 16,\n",
       " '-0.098': 17,\n",
       " '-0.099': 18,\n",
       " '-0.1': 19,\n",
       " '-0.101': 20,\n",
       " '-0.102': 21,\n",
       " '-0.103': 22,\n",
       " '-0.104': 23,\n",
       " '-0.105': 24,\n",
       " '-0.106': 25,\n",
       " '-0.107': 26,\n",
       " '-0.108': 27,\n",
       " '-0.109': 28,\n",
       " '-0.11': 29,\n",
       " '-0.111': 30,\n",
       " '-0.112': 31,\n",
       " '-0.113': 32,\n",
       " '-0.114': 33,\n",
       " '-0.115': 34,\n",
       " '-0.116': 35,\n",
       " '-0.117': 36,\n",
       " '-0.118': 37,\n",
       " '-0.119': 38,\n",
       " '-0.12': 39,\n",
       " '-0.121': 40,\n",
       " '-0.122': 41,\n",
       " '-0.123': 42,\n",
       " '-0.124': 43,\n",
       " '-0.125': 44,\n",
       " '-0.126': 45,\n",
       " '-0.127': 46,\n",
       " '-0.128': 47,\n",
       " '-0.129': 48,\n",
       " '-0.13': 49,\n",
       " '-0.131': 50,\n",
       " '-0.132': 51,\n",
       " '-0.133': 52,\n",
       " '-0.134': 53,\n",
       " '-0.135': 54,\n",
       " '-0.136': 55,\n",
       " '-0.137': 56,\n",
       " '-0.138': 57,\n",
       " '-0.139': 58,\n",
       " '-0.14': 59,\n",
       " '-0.141': 60,\n",
       " '-0.142': 61,\n",
       " '-0.143': 62,\n",
       " '-0.144': 63,\n",
       " '-0.145': 64,\n",
       " '-0.146': 65,\n",
       " '-0.147': 66,\n",
       " '-0.148': 67,\n",
       " '-0.149': 68,\n",
       " '-0.15': 69,\n",
       " '-0.151': 70,\n",
       " '-0.152': 71,\n",
       " '-0.153': 72,\n",
       " '-0.314': 73,\n",
       " '-0.315': 74,\n",
       " '-0.325': 75,\n",
       " '-0.359': 76,\n",
       " '-0.374': 77,\n",
       " '-0.395': 78,\n",
       " '-0.46': 79,\n",
       " '-0.494': 80,\n",
       " '-0.529': 81,\n",
       " '-0.54': 82,\n",
       " '-0.543': 83,\n",
       " '-0.566': 84,\n",
       " '-0.571': 85,\n",
       " '-0.573': 86,\n",
       " '0.0': 87,\n",
       " '0.062': 88,\n",
       " '0.088': 89,\n",
       " '1.99': 90}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5513bc77-6525-4757-a66a-ac3cad6800fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-0.082': 0,\n",
       " '-0.084': 1,\n",
       " '-0.085': 2,\n",
       " '-0.086': 3,\n",
       " '-0.089': 4,\n",
       " '-0.091': 5,\n",
       " '-0.093': 6,\n",
       " '-0.094': 7,\n",
       " '-0.095': 8,\n",
       " '-0.096': 9,\n",
       " '-0.097': 10,\n",
       " '-0.099': 11,\n",
       " '-0.1': 12,\n",
       " '-0.101': 13,\n",
       " '-0.102': 14,\n",
       " '-0.104': 15,\n",
       " '-0.107': 16,\n",
       " '-0.108': 17,\n",
       " '-0.11': 18,\n",
       " '-0.111': 19,\n",
       " '-0.112': 20,\n",
       " '-0.113': 21,\n",
       " '-0.114': 22,\n",
       " '-0.116': 23,\n",
       " '-0.117': 24,\n",
       " '-0.118': 25,\n",
       " '-0.119': 26,\n",
       " '-0.12': 27,\n",
       " '-0.122': 28,\n",
       " '-0.123': 29,\n",
       " '-0.124': 30,\n",
       " '-0.125': 31,\n",
       " '-0.126': 32,\n",
       " '-0.127': 33,\n",
       " '-0.128': 34,\n",
       " '-0.129': 35,\n",
       " '-0.13': 36,\n",
       " '-0.131': 37,\n",
       " '-0.132': 38,\n",
       " '-0.134': 39,\n",
       " '-0.136': 40,\n",
       " '-0.137': 41,\n",
       " '-0.138': 42,\n",
       " '-0.139': 43,\n",
       " '-0.14': 44,\n",
       " '-0.141': 45,\n",
       " '-0.143': 46,\n",
       " '-0.144': 47,\n",
       " '-0.145': 48,\n",
       " '-0.146': 49,\n",
       " '-0.147': 50,\n",
       " '-0.148': 51,\n",
       " '-0.15': 52,\n",
       " '-0.151': 53,\n",
       " '-0.152': 54,\n",
       " '-0.466': 55,\n",
       " '-0.488': 56,\n",
       " '-0.502': 57,\n",
       " '-0.526': 58,\n",
       " '0.0': 59,\n",
       " '0.062': 60,\n",
       " '0.088': 61,\n",
       " '1.99': 62}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "909ee323-c038-4c3e-aa80-cc5f2974c52b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-0.081': 0,\n",
       " '-0.082': 1,\n",
       " '-0.083': 2,\n",
       " '-0.084': 3,\n",
       " '-0.085': 4,\n",
       " '-0.086': 5,\n",
       " '-0.087': 6,\n",
       " '-0.088': 7,\n",
       " '-0.089': 8,\n",
       " '-0.09': 9,\n",
       " '-0.091': 10,\n",
       " '-0.092': 11,\n",
       " '-0.093': 12,\n",
       " '-0.094': 13,\n",
       " '-0.095': 14,\n",
       " '-0.096': 15,\n",
       " '-0.097': 16,\n",
       " '-0.098': 17,\n",
       " '-0.099': 18,\n",
       " '-0.1': 19,\n",
       " '-0.101': 20,\n",
       " '-0.102': 21,\n",
       " '-0.103': 22,\n",
       " '-0.104': 23,\n",
       " '-0.105': 24,\n",
       " '-0.106': 25,\n",
       " '-0.107': 26,\n",
       " '-0.108': 27,\n",
       " '-0.109': 28,\n",
       " '-0.11': 29,\n",
       " '-0.111': 30,\n",
       " '-0.112': 31,\n",
       " '-0.113': 32,\n",
       " '-0.114': 33,\n",
       " '-0.115': 34,\n",
       " '-0.116': 35,\n",
       " '-0.117': 36,\n",
       " '-0.118': 37,\n",
       " '-0.119': 38,\n",
       " '-0.12': 39,\n",
       " '-0.121': 40,\n",
       " '-0.122': 41,\n",
       " '-0.123': 42,\n",
       " '-0.124': 43,\n",
       " '-0.125': 44,\n",
       " '-0.126': 45,\n",
       " '-0.127': 46,\n",
       " '-0.128': 47,\n",
       " '-0.129': 48,\n",
       " '-0.13': 49,\n",
       " '-0.131': 50,\n",
       " '-0.132': 51,\n",
       " '-0.133': 52,\n",
       " '-0.134': 53,\n",
       " '-0.135': 54,\n",
       " '-0.136': 55,\n",
       " '-0.137': 56,\n",
       " '-0.138': 57,\n",
       " '-0.139': 58,\n",
       " '-0.14': 59,\n",
       " '-0.141': 60,\n",
       " '-0.142': 61,\n",
       " '-0.143': 62,\n",
       " '-0.144': 63,\n",
       " '-0.145': 64,\n",
       " '-0.146': 65,\n",
       " '-0.147': 66,\n",
       " '-0.148': 67,\n",
       " '-0.149': 68,\n",
       " '-0.15': 69,\n",
       " '-0.151': 70,\n",
       " '-0.152': 71,\n",
       " '-0.153': 72,\n",
       " '-0.314': 73,\n",
       " '-0.315': 74,\n",
       " '-0.325': 75,\n",
       " '-0.359': 76,\n",
       " '-0.374': 77,\n",
       " '-0.395': 78,\n",
       " '-0.46': 79,\n",
       " '-0.494': 80,\n",
       " '-0.529': 81,\n",
       " '-0.54': 82,\n",
       " '-0.543': 83,\n",
       " '-0.566': 84,\n",
       " '-0.571': 85,\n",
       " '-0.573': 86,\n",
       " '0.0': 87,\n",
       " '0.062': 88,\n",
       " '0.088': 89,\n",
       " '1.99': 90,\n",
       " '-0.466': 91,\n",
       " '-0.488': 92,\n",
       " '-0.502': 93,\n",
       " '-0.526': 94}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = train_generator.class_indices.copy()\n",
    "classes.update(valid_generator.class_indices)\n",
    "#classes = {**valid_generator.class_indices, ** train_generator.class_indices}\n",
    "index = 0\n",
    "for key, value in classes.items():\n",
    "    classes[key] = index\n",
    "    index += 1\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1884f9b1-1eb6-404d-ba7b-4f1e61a5366c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(classes)\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "eb70c6c5-46dd-47ce-a514-5e6c673c4183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3709 validated image filenames belonging to 95 classes.\n",
      "Found 749 validated image filenames belonging to 95 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "      dataframe=train_df,\n",
    "      directory=\"./data/train/images/\",\n",
    "      x_col=\"fnames\",\n",
    "      y_col=\"labels\",\n",
    "      batch_size=64,\n",
    "      seed=42,\n",
    "      shuffle=True,\n",
    "      class_mode=\"categorical\",\n",
    "      classes= classes,\n",
    "      target_size=(224,224))\n",
    "\n",
    "\n",
    "\n",
    "valid_generator=test_datagen.flow_from_dataframe(\n",
    "      dataframe=val_df,\n",
    "      directory=\"./data/val/images/\",\n",
    "      x_col=\"fnames\",\n",
    "      y_col=\"labels\",\n",
    "      batch_size=64,\n",
    "      seed=42,\n",
    "      shuffle=True,\n",
    "      class_mode=\"categorical\",\n",
    "     classes= classes,\n",
    "     target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49c790a0-c070-46fc-a219-c03f065e41d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 749 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "dataframe=test_df,\n",
    "      directory=\"./data/test/images\",\n",
    "      x_col=\"fnames\",\n",
    "      batch_size=1,\n",
    "      seed=42,\n",
    "      shuffle=False,\n",
    "      class_mode=None,\n",
    "      target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa73fe-6bed-4869-98a4-f74416348bab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a04b567-9c49-4fad-9bcd-000cb1a058a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d4403221-0e45-4cc5-9cc8-6e84aa7d76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(224,224,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_class, activation='sigmoid'))\n",
    "\n",
    "#opt = SGD(lr=0.001, momentum=0.9)\n",
    "opt =tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "model.compile(optimizer=opt,loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e5ee260d-ecc7-46a5-988f-9ca7b092e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 8 #100 #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f69b861c-1472-444a-a2ee-c158e0a96e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "57/57 [==============================] - 828s 14s/step - loss: 0.2065 - accuracy: 0.4457 - val_loss: 0.0148 - val_accuracy: 0.8466\n",
      "Epoch 2/8\n",
      "57/57 [==============================] - 859s 15s/step - loss: 0.0198 - accuracy: 0.8360 - val_loss: 0.0103 - val_accuracy: 0.8807\n",
      "Epoch 3/8\n",
      "57/57 [==============================] - 832s 15s/step - loss: 0.0136 - accuracy: 0.8685 - val_loss: 0.0089 - val_accuracy: 0.8835\n",
      "Epoch 4/8\n",
      "57/57 [==============================] - 681s 12s/step - loss: 0.0106 - accuracy: 0.8655 - val_loss: 0.0078 - val_accuracy: 0.8778\n",
      "Epoch 5/8\n",
      "57/57 [==============================] - 670s 12s/step - loss: 0.0087 - accuracy: 0.8680 - val_loss: 0.0072 - val_accuracy: 0.8864\n",
      "Epoch 6/8\n",
      "57/57 [==============================] - 746s 13s/step - loss: 0.0076 - accuracy: 0.8753 - val_loss: 0.0071 - val_accuracy: 0.8807\n",
      "Epoch 7/8\n",
      "57/57 [==============================] - 725s 13s/step - loss: 0.0075 - accuracy: 0.8731 - val_loss: 0.0073 - val_accuracy: 0.8807\n",
      "Epoch 8/8\n",
      "57/57 [==============================] - 610s 11s/step - loss: 0.0072 - accuracy: 0.8758 - val_loss: 0.0069 - val_accuracy: 0.8878\n"
     ]
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "history = model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "55d2eba7-f3ca-4e0f-8238-2d8b1c212b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9323567d-93a3-4500-b39a-a00318629079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "749/749 [==============================] - 66s 87ms/step\n"
     ]
    }
   ],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict_generator(test_generator,\n",
    "steps=STEP_SIZE_TEST,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "55f169b3-9e88-43d7-9480-c17a80cb9e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.35479808e-10, 5.26551913e-09, 1.81937110e-09, ...,\n",
       "        5.54137569e-10, 6.03112404e-09, 1.00000000e+00],\n",
       "       [3.25811101e-13, 2.94069456e-11, 1.01467125e-11, ...,\n",
       "        3.46230301e-11, 2.23226090e-11, 1.00000000e+00],\n",
       "       [9.63877511e-09, 1.51252067e-07, 1.01021058e-07, ...,\n",
       "        9.91281865e-08, 5.25281223e-08, 9.99999523e-01],\n",
       "       ...,\n",
       "       [6.98009133e-03, 1.17859244e-02, 6.05478883e-03, ...,\n",
       "        5.02824783e-04, 1.73807144e-04, 3.95043935e-06],\n",
       "       [5.23570727e-12, 5.28433075e-10, 1.82525467e-10, ...,\n",
       "        2.39080089e-10, 1.17863319e-09, 1.00000000e+00],\n",
       "       [9.00044372e-10, 2.32330475e-08, 1.48386077e-08, ...,\n",
       "        1.84939459e-08, 5.29453281e-09, 1.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8d306d73-b624-46e9-aaa3-dbcca93dd9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bool = (pred >0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "392a65ff-7ab7-4ec6-a745-bbf13c9c73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "labels = train_generator.class_indices\n",
    "\n",
    "textfile = open(\"T2dot_labels.txt\", \"w\")\n",
    "for element in labels:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "for row in pred_bool:\n",
    "    l=[]\n",
    "    for index,cls in enumerate(row):\n",
    "        if cls:\n",
    "            l.append(labels[index])\n",
    "    predictions.append(\",\".join(l))\n",
    "filenames=test_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})\n",
    "results.to_csv(\"T2dotresults.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7385e1fa-c041-40a3-ad3a-c47482ed3e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.png</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.png</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.png</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101.png</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102.png</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>95.png</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>96.png</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>97.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>98.png</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>99.png</td>\n",
       "      <td>1.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>749 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Filename Predictions\n",
       "0      1.png        1.99\n",
       "1     10.png        1.99\n",
       "2    100.png        1.99\n",
       "3    101.png        1.99\n",
       "4    102.png        1.99\n",
       "..       ...         ...\n",
       "744   95.png         0.0\n",
       "745   96.png        1.99\n",
       "746   97.png            \n",
       "747   98.png        1.99\n",
       "748   99.png        1.99\n",
       "\n",
       "[749 rows x 2 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "476521a0-bfa6-411f-9793-2fd3f3d8e161",
   "metadata": {},
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "#load the image\n",
    "my_image = load_img('data/test/images/127.png', target_size=(224, 224))\n",
    "\n",
    "#preprocess the image\n",
    "my_image = img_to_array(my_image)\n",
    "my_image = my_image.reshape((1, my_image.shape[0], my_image.shape[1], my_image.shape[2]))\n",
    "my_image = preprocess_input(my_image)\n",
    "\n",
    "#make the prediction\n",
    "prediction = model.predict(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a1c30c92-b894-4711-a67e-0bf044228268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "original = PIL.Image.open(\"data/test/images/100.png\")\n",
    "file_type = original.format\n",
    "\n",
    "original.save(\"testing/test.png\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0ed903af-3ed6-445b-ad90-5300d4304062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 validated image filenames.\n",
      "1/1 [==============================] - 0s 135ms/step\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "original = PIL.Image.open(\"data/test/images/100.png\") # replace later with image input\n",
    "file_type = original.format\n",
    "\n",
    "original.save(\"testing/test.png\", format=\"png\")\n",
    "##########################\n",
    "testdata = []\n",
    "for fname in sorted(os.listdir('testing')):\n",
    "    if fname == \".DS_Store\": continue\n",
    "            \n",
    "    subject_data_path = os.path.join('testing', fname)                   \n",
    "    if not os.path.isfile(subject_data_path): continue          \n",
    "    testdata.append(fname)\n",
    "    \n",
    "df = pd.DataFrame(testdata, columns=['fnames'])\n",
    "df['fnames']= df['fnames'].astype(str)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "test_g=test_datagen.flow_from_dataframe(\n",
    "     dataframe= df,\n",
    "      directory=\"./testing\",\n",
    "      x_col=\"fnames\",\n",
    "      batch_size=1,\n",
    "      seed=42,\n",
    "      shuffle=False,\n",
    "      class_mode=None,\n",
    "      target_size=(224,224))\n",
    "STEP_SIZE_TEST=test_g.n//test_g.batch_size\n",
    "pred = model.predict_generator(test_g,\n",
    "                               steps=STEP_SIZE_TEST,\n",
    "                               verbose=1)\n",
    "pred_bool = (pred >0.5)\n",
    "predictions=[]\n",
    "#labels = train_generator.class_indices\n",
    "#labels = dict((v,k) for k,v in labels.items())\n",
    "labels = {}\n",
    "file1 = open('T2dot_labels.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    " \n",
    "count = 0\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    labels[count] = line.strip()\n",
    "    count += 1\n",
    "    \n",
    "for row in pred_bool:\n",
    "    l=[]\n",
    "    for index,cls in enumerate(row):\n",
    "        if cls:\n",
    "            l.append(labels[index])\n",
    "    predictions.append(\",\".join(l))\n",
    "    \n",
    "    \n",
    "if predictions[0] == '':\n",
    "    result = 0\n",
    "else:\n",
    "    result = float (predictions[0] )\n",
    "#result = float (predictions[0] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "18fcfdb4-3241-4161-85cb-9d2a6cec1b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.99'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "86e81fe7-eaf1-4b6c-bb83-60163f2051ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.99"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "06603d7a-be6f-4b00-a566-274256c1fbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '-0.081',\n",
       " 1: '-0.082',\n",
       " 2: '-0.083',\n",
       " 3: '-0.084',\n",
       " 4: '-0.085',\n",
       " 5: '-0.086',\n",
       " 6: '-0.087',\n",
       " 7: '-0.088',\n",
       " 8: '-0.089',\n",
       " 9: '-0.09',\n",
       " 10: '-0.091',\n",
       " 11: '-0.092',\n",
       " 12: '-0.093',\n",
       " 13: '-0.094',\n",
       " 14: '-0.095',\n",
       " 15: '-0.096',\n",
       " 16: '-0.097',\n",
       " 17: '-0.098',\n",
       " 18: '-0.099',\n",
       " 19: '-0.1',\n",
       " 20: '-0.101',\n",
       " 21: '-0.102',\n",
       " 22: '-0.103',\n",
       " 23: '-0.104',\n",
       " 24: '-0.105',\n",
       " 25: '-0.106',\n",
       " 26: '-0.107',\n",
       " 27: '-0.108',\n",
       " 28: '-0.109',\n",
       " 29: '-0.11',\n",
       " 30: '-0.111',\n",
       " 31: '-0.112',\n",
       " 32: '-0.113',\n",
       " 33: '-0.114',\n",
       " 34: '-0.115',\n",
       " 35: '-0.116',\n",
       " 36: '-0.117',\n",
       " 37: '-0.118',\n",
       " 38: '-0.119',\n",
       " 39: '-0.12',\n",
       " 40: '-0.121',\n",
       " 41: '-0.122',\n",
       " 42: '-0.123',\n",
       " 43: '-0.124',\n",
       " 44: '-0.125',\n",
       " 45: '-0.126',\n",
       " 46: '-0.127',\n",
       " 47: '-0.128',\n",
       " 48: '-0.129',\n",
       " 49: '-0.13',\n",
       " 50: '-0.131',\n",
       " 51: '-0.132',\n",
       " 52: '-0.133',\n",
       " 53: '-0.134',\n",
       " 54: '-0.135',\n",
       " 55: '-0.136',\n",
       " 56: '-0.137',\n",
       " 57: '-0.138',\n",
       " 58: '-0.139',\n",
       " 59: '-0.14',\n",
       " 60: '-0.141',\n",
       " 61: '-0.142',\n",
       " 62: '-0.143',\n",
       " 63: '-0.144',\n",
       " 64: '-0.145',\n",
       " 65: '-0.146',\n",
       " 66: '-0.147',\n",
       " 67: '-0.148',\n",
       " 68: '-0.149',\n",
       " 69: '-0.15',\n",
       " 70: '-0.151',\n",
       " 71: '-0.152',\n",
       " 72: '-0.153',\n",
       " 73: '-0.314',\n",
       " 74: '-0.315',\n",
       " 75: '-0.325',\n",
       " 76: '-0.359',\n",
       " 77: '-0.374',\n",
       " 78: '-0.395',\n",
       " 79: '-0.46',\n",
       " 80: '-0.466',\n",
       " 81: '-0.488',\n",
       " 82: '-0.494',\n",
       " 83: '-0.502',\n",
       " 84: '-0.526',\n",
       " 85: '-0.529',\n",
       " 86: '-0.54',\n",
       " 87: '-0.543',\n",
       " 88: '-0.566',\n",
       " 89: '-0.571',\n",
       " 90: '-0.573',\n",
       " 91: '0.0',\n",
       " 92: '0.062',\n",
       " 93: '0.088',\n",
       " 94: '1.99'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f98c159d-64be-4d98-b12d-98ff511626a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7fda98765110>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2964a2d6-0d92-4019-b93e-f908bd728ac7",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('data/test/images/127.png', target_size =(224,224))\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "result = model.predict(test_image)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd6718b4-dc5f-4115-b886-424f6fcfe55d",
   "metadata": {},
   "source": [
    "test_image = image.load_img('data/test/images/228.png', target_size =(224,224))\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "be769dfd-b348-4067-bb46-276897728b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "filename2 = 'model_t2dot.h5' \n",
    "model.save(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "607599c9-9b5d-44b2-a74a-95516d370e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# load model\n",
    "model = load_model('model_t2dot.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "652a1b62-1de5-4853-b3c4-d36882a6a62b",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2020/10/create-image-classification-model-python-keras/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23e021e2-aaed-440a-bda5-a8b1b4e313fd",
   "metadata": {},
   "source": [
    "    def flow_from_dataframe(self,\n",
    "                            dataframe,\n",
    "                            directory=None,\n",
    "                            x_col=\"filename\",\n",
    "                            y_col=\"class\",\n",
    "                            weight_col=None,\n",
    "                            target_size=(256, 256),\n",
    "                            color_mode='rgb',\n",
    "                            classes=None,\n",
    "                            class_mode='categorical',\n",
    "                            batch_size=32,\n",
    "                            shuffle=True,\n",
    "                            seed=None,\n",
    "                            save_to_dir=None,\n",
    "                            save_prefix='',\n",
    "                            save_format='png',\n",
    "                            subset=None,\n",
    "                            interpolation='nearest',\n",
    "                            validate_filenames=True,\n",
    "                            **kwargs):\n",
    "        \"\"\"Takes the dataframe and the path to a directory\n",
    "         and generates batches of augmented/normalized data.\n",
    "        **A simple tutorial can be found **[here](\n",
    "                                    http://bit.ly/keras_flow_from_dataframe).\n",
    "        # Arguments\n",
    "            dataframe: Pandas dataframe containing the filepaths relative to\n",
    "                `directory` (or absolute paths if `directory` is None) of the\n",
    "                images in a string column. It should include other column/s\n",
    "                depending on the `class_mode`:\n",
    "                - if `class_mode` is `\"categorical\"` (default value) it must\n",
    "                    include the `y_col` column with the class/es of each image.\n",
    "                    Values in column can be string/list/tuple if a single class\n",
    "                    or list/tuple if multiple classes.\n",
    "                - if `class_mode` is `\"binary\"` or `\"sparse\"` it must include\n",
    "                    the given `y_col` column with class values as strings.\n",
    "                - if `class_mode` is `\"raw\"` or `\"multi_output\"` it should contain\n",
    "                the columns specified in `y_col`.\n",
    "                - if `class_mode` is `\"input\"` or `None` no extra column is needed.\n",
    "            directory: string, path to the directory to read images from. If `None`,\n",
    "                data in `x_col` column should be absolute paths.\n",
    "            x_col: string, column in `dataframe` that contains the filenames (or\n",
    "                absolute paths if `directory` is `None`).\n",
    "            y_col: string or list, column/s in `dataframe` that has the target data.\n",
    "            weight_col: string, column in `dataframe` that contains the sample\n",
    "                weights. Default: `None`.\n",
    "            target_size: tuple of integers `(height, width)`, default: `(256, 256)`.\n",
    "                The dimensions to which all images found will be resized.\n",
    "            color_mode: one of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
    "                Whether the images will be converted to have 1 or 3 color channels.\n",
    "            classes: optional list of classes (e.g. `['dogs', 'cats']`).\n",
    "                Default: None. If not provided, the list of classes will be\n",
    "                automatically inferred from the `y_col`,\n",
    "                which will map to the label indices, will be alphanumeric).\n",
    "                The dictionary containing the mapping from class names to class\n",
    "                indices can be obtained via the attribute `class_indices`.\n",
    "            class_mode: one of \"binary\", \"categorical\", \"input\", \"multi_output\",\n",
    "                \"raw\", sparse\" or None. Default: \"categorical\".\n",
    "                Mode for yielding the targets:\n",
    "                - `\"binary\"`: 1D NumPy array of binary labels,\n",
    "                - `\"categorical\"`: 2D NumPy array of one-hot encoded labels.\n",
    "                    Supports multi-label output.\n",
    "                - `\"input\"`: images identical to input images (mainly used to\n",
    "                    work with autoencoders),\n",
    "                - `\"multi_output\"`: list with the values of the different columns,\n",
    "                - `\"raw\"`: NumPy array of values in `y_col` column(s),\n",
    "                - `\"sparse\"`: 1D NumPy array of integer labels,\n",
    "                - `None`, no targets are returned (the generator will only yield\n",
    "                    batches of image data, which is useful to use in\n",
    "                    `model.predict_generator()`).\n",
    "            batch_size: size of the batches of data (default: 32).\n",
    "            shuffle: whether to shuffle the data (default: True)\n",
    "            seed: optional random seed for shuffling and transformations.\n",
    "            save_to_dir: None or str (default: None).\n",
    "                This allows you to optionally specify a directory\n",
    "                to which to save the augmented pictures being generated\n",
    "                (useful for visualizing what you are doing).\n",
    "            save_prefix: str. Prefix to use for filenames of saved pictures\n",
    "                (only relevant if `save_to_dir` is set).\n",
    "            save_format: one of \"png\", \"jpeg\"\n",
    "                (only relevant if `save_to_dir` is set). Default: \"png\".\n",
    "            follow_links: whether to follow symlinks inside class subdirectories\n",
    "                (default: False).\n",
    "            subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
    "                `validation_split` is set in `ImageDataGenerator`.\n",
    "            interpolation: Interpolation method used to resample the image if the\n",
    "                target size is different from that of the loaded image.\n",
    "                Supported methods are `\"nearest\"`, `\"bilinear\"`, and `\"bicubic\"`.\n",
    "                If PIL version 1.1.3 or newer is installed, `\"lanczos\"` is also\n",
    "                supported. If PIL version 3.4.0 or newer is installed, `\"box\"` and\n",
    "                `\"hamming\"` are also supported. By default, `\"nearest\"` is used.\n",
    "            validate_filenames: Boolean, whether to validate image filenames in\n",
    "                `x_col`. If `True`, invalid images will be ignored. Disabling this\n",
    "                option can lead to speed-up in the execution of this function.\n",
    "                Default: `True`.\n",
    "        # Returns\n",
    "            A `DataFrameIterator` yielding tuples of `(x, y)`\n",
    "            where `x` is a NumPy array containing a batch\n",
    "            of images with shape `(batch_size, *target_size, channels)`\n",
    "            and `y` is a NumPy array of corresponding labels.\n",
    "        \"\"\"\n",
    "        if 'has_ext' in kwargs:\n",
    "            warnings.warn('has_ext is deprecated, filenames in the dataframe have '\n",
    "                          'to match the exact filenames in disk.',\n",
    "                          DeprecationWarning)\n",
    "        if 'sort' in kwargs:\n",
    "            warnings.warn('sort is deprecated, batches will be created in the'\n",
    "                          'same order than the filenames provided if shuffle'\n",
    "                          'is set to False.', DeprecationWarning)\n",
    "        if class_mode == 'other':\n",
    "            warnings.warn('`class_mode` \"other\" is deprecated, please use '\n",
    "                          '`class_mode` \"raw\".', DeprecationWarning)\n",
    "            class_mode = 'raw'\n",
    "        if 'drop_duplicates' in kwargs:\n",
    "            warnings.warn('drop_duplicates is deprecated, you can drop duplicates '\n",
    "                          'by using the pandas.DataFrame.drop_duplicates method.',\n",
    "                          DeprecationWarning)\n",
    "\n",
    "        return DataFrameIterator(\n",
    "            dataframe,\n",
    "            directory,\n",
    "            self,\n",
    "            x_col=x_col,\n",
    "            y_col=y_col,\n",
    "            weight_col=weight_col,\n",
    "            target_size=target_size,\n",
    "            color_mode=color_mode,\n",
    "            classes=classes,\n",
    "            class_mode=class_mode,\n",
    "            data_format=self.data_format,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format,\n",
    "            subset=subset,\n",
    "            interpolation=interpolation,\n",
    "            validate_filenames=validate_filenames,\n",
    "            dtype=self.dtype\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0415ad83-e8ea-42f0-af3f-7c72baacf2fa",
   "metadata": {},
   "source": [
    "\"\"\"Utilities for real-time data augmentation on image data.\n",
    "\"\"\"\n",
    "import os\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from .iterator import BatchFromFilesMixin, Iterator\n",
    "from .utils import validate_filename\n",
    "\n",
    "\n",
    "class DataFrameIterator(BatchFromFilesMixin, Iterator):\n",
    "    \"\"\"Iterator capable of reading images from a directory on disk\n",
    "        through a dataframe.\n",
    "    # Arguments\n",
    "        dataframe: Pandas dataframe containing the filepaths relative to\n",
    "            `directory` (or absolute paths if `directory` is None) of the\n",
    "            images in a string column. It should include other column/s\n",
    "            depending on the `class_mode`:\n",
    "            - if `class_mode` is `\"categorical\"` (default value) it must\n",
    "                include the `y_col` column with the class/es of each image.\n",
    "                Values in column can be string/list/tuple if a single class\n",
    "                or list/tuple if multiple classes.\n",
    "            - if `class_mode` is `\"binary\"` or `\"sparse\"` it must include\n",
    "                the given `y_col` column with class values as strings.\n",
    "            - if `class_mode` is `\"raw\"` or `\"multi_output\"` it should contain\n",
    "                the columns specified in `y_col`.\n",
    "            - if `class_mode` is `\"input\"` or `None` no extra column is needed.\n",
    "        directory: string, path to the directory to read images from. If `None`,\n",
    "            data in `x_col` column should be absolute paths.\n",
    "        image_data_generator: Instance of `ImageDataGenerator` to use for\n",
    "            random transformations and normalization. If None, no transformations\n",
    "            and normalizations are made.\n",
    "        x_col: string, column in `dataframe` that contains the filenames (or\n",
    "            absolute paths if `directory` is `None`).\n",
    "        y_col: string or list, column/s in `dataframe` that has the target data.\n",
    "        weight_col: string, column in `dataframe` that contains the sample\n",
    "            weights. Default: `None`.\n",
    "        target_size: tuple of integers, dimensions to resize input images to.\n",
    "        color_mode: One of `\"rgb\"`, `\"rgba\"`, `\"grayscale\"`.\n",
    "            Color mode to read images.\n",
    "        classes: Optional list of strings, classes to use (e.g. `[\"dogs\", \"cats\"]`).\n",
    "            If None, all classes in `y_col` will be used.\n",
    "        class_mode: one of \"binary\", \"categorical\", \"input\", \"multi_output\",\n",
    "            \"raw\", \"sparse\" or None. Default: \"categorical\".\n",
    "            Mode for yielding the targets:\n",
    "            - `\"binary\"`: 1D numpy array of binary labels,\n",
    "            - `\"categorical\"`: 2D numpy array of one-hot encoded labels.\n",
    "                Supports multi-label output.\n",
    "            - `\"input\"`: images identical to input images (mainly used to\n",
    "                work with autoencoders),\n",
    "            - `\"multi_output\"`: list with the values of the different columns,\n",
    "            - `\"raw\"`: numpy array of values in `y_col` column(s),\n",
    "            - `\"sparse\"`: 1D numpy array of integer labels,\n",
    "            - `None`, no targets are returned (the generator will only yield\n",
    "                batches of image data, which is useful to use in\n",
    "                `model.predict_generator()`).\n",
    "        batch_size: Integer, size of a batch.\n",
    "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
    "        seed: Random seed for data shuffling.\n",
    "        data_format: String, one of `channels_first`, `channels_last`.\n",
    "        save_to_dir: Optional directory where to save the pictures\n",
    "            being yielded, in a viewable format. This is useful\n",
    "            for visualizing the random transformations being\n",
    "            applied, for debugging purposes.\n",
    "        save_prefix: String prefix to use for saving sample\n",
    "            images (if `save_to_dir` is set).\n",
    "        save_format: Format to use for saving sample images\n",
    "            (if `save_to_dir` is set).\n",
    "        subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
    "            validation_split is set in ImageDataGenerator.\n",
    "        interpolation: Interpolation method used to resample the image if the\n",
    "            target size is different from that of the loaded image.\n",
    "            Supported methods are \"nearest\", \"bilinear\", and \"bicubic\".\n",
    "            If PIL version 1.1.3 or newer is installed, \"lanczos\" is also\n",
    "            supported. If PIL version 3.4.0 or newer is installed, \"box\" and\n",
    "            \"hamming\" are also supported. By default, \"nearest\" is used.\n",
    "        keep_aspect_ratio: Boolean, whether to resize images to a target size\n",
    "            without aspect ratio distortion. The image is cropped in the center\n",
    "            with target aspect ratio before resizing.\n",
    "        dtype: Dtype to use for the generated arrays.\n",
    "        validate_filenames: Boolean, whether to validate image filenames in\n",
    "        `x_col`. If `True`, invalid images will be ignored. Disabling this option\n",
    "        can lead to speed-up in the instantiation of this class. Default: `True`.\n",
    "    \"\"\"\n",
    "    allowed_class_modes = {\n",
    "        'binary', 'categorical', 'input', 'multi_output', 'raw', 'sparse', None\n",
    "    }\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        try:\n",
    "            from tensorflow.keras.utils import Sequence as TFSequence\n",
    "            if TFSequence not in cls.__bases__:\n",
    "                cls.__bases__ = cls.__bases__ + (TFSequence,)\n",
    "        except ImportError:\n",
    "            pass\n",
    "        return super(DataFrameIterator, cls).__new__(cls)\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataframe,\n",
    "                 directory=None,\n",
    "                 image_data_generator=None,\n",
    "                 x_col=\"filename\",\n",
    "                 y_col=\"class\",\n",
    "                 weight_col=None,\n",
    "                 target_size=(256, 256),\n",
    "                 color_mode='rgb',\n",
    "                 classes=None,\n",
    "                 class_mode='categorical',\n",
    "                 batch_size=32,\n",
    "                 shuffle=True,\n",
    "                 seed=None,\n",
    "                 data_format='channels_last',\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='png',\n",
    "                 subset=None,\n",
    "                 interpolation='nearest',\n",
    "                 keep_aspect_ratio=False,\n",
    "                 dtype='float32',\n",
    "                 validate_filenames=True):\n",
    "\n",
    "        super(DataFrameIterator, self).set_processing_attrs(image_data_generator,\n",
    "                                                            target_size,\n",
    "                                                            color_mode,\n",
    "                                                            data_format,\n",
    "                                                            save_to_dir,\n",
    "                                                            save_prefix,\n",
    "                                                            save_format,\n",
    "                                                            subset,\n",
    "                                                            interpolation,\n",
    "                                                            keep_aspect_ratio)\n",
    "        df = dataframe.copy()\n",
    "        self.directory = directory or ''\n",
    "        self.class_mode = class_mode\n",
    "        self.dtype = dtype\n",
    "        # check that inputs match the required class_mode\n",
    "        self._check_params(df, x_col, y_col, weight_col, classes)\n",
    "        if validate_filenames:  # check which image files are valid and keep them\n",
    "            df = self._filter_valid_filepaths(df, x_col)\n",
    "        if class_mode not in [\"input\", \"multi_output\", \"raw\", None]:\n",
    "            df, classes = self._filter_classes(df, y_col, classes)\n",
    "            num_classes = len(classes)\n",
    "            # build an index of all the unique classes\n",
    "            self.class_indices = dict(zip(classes, range(len(classes))))\n",
    "        # retrieve only training or validation set\n",
    "        if self.split:\n",
    "            num_files = len(df)\n",
    "            start = int(self.split[0] * num_files)\n",
    "            stop = int(self.split[1] * num_files)\n",
    "            df = df.iloc[start: stop, :]\n",
    "        # get labels for each observation\n",
    "        if class_mode not in [\"input\", \"multi_output\", \"raw\", None]:\n",
    "            self.classes = self.get_classes(df, y_col)\n",
    "        self.filenames = df[x_col].tolist()\n",
    "        self._sample_weight = df[weight_col].values if weight_col else None\n",
    "\n",
    "        if class_mode == \"multi_output\":\n",
    "            self._targets = [np.array(df[col].tolist()) for col in y_col]\n",
    "        if class_mode == \"raw\":\n",
    "            self._targets = df[y_col].values\n",
    "        self.samples = len(self.filenames)\n",
    "        validated_string = 'validated' if validate_filenames else 'non-validated'\n",
    "        if class_mode in [\"input\", \"multi_output\", \"raw\", None]:\n",
    "            print('Found {} {} image filenames.'\n",
    "                  .format(self.samples, validated_string))\n",
    "        else:\n",
    "            print('Found {} {} image filenames belonging to {} classes.'\n",
    "                  .format(self.samples, validated_string, num_classes))\n",
    "        self._filepaths = [\n",
    "            os.path.join(self.directory, fname) for fname in self.filenames\n",
    "        ]\n",
    "        super(DataFrameIterator, self).__init__(self.samples,\n",
    "                                                batch_size,\n",
    "                                                shuffle,\n",
    "                                                seed)\n",
    "\n",
    "    def _check_params(self, df, x_col, y_col, weight_col, classes):\n",
    "        # check class mode is one of the currently supported\n",
    "        if self.class_mode not in self.allowed_class_modes:\n",
    "            raise ValueError('Invalid class_mode: {}; expected one of: {}'\n",
    "                             .format(self.class_mode, self.allowed_class_modes))\n",
    "        # check that y_col has several column names if class_mode is multi_output\n",
    "        if (self.class_mode == 'multi_output') and not isinstance(y_col, list):\n",
    "            raise TypeError(\n",
    "                'If class_mode=\"{}\", y_col must be a list. Received {}.'\n",
    "                .format(self.class_mode, type(y_col).__name__)\n",
    "            )\n",
    "        # check that filenames/filepaths column values are all strings\n",
    "        if not all(df[x_col].apply(lambda x: isinstance(x, str))):\n",
    "            raise TypeError('All values in column x_col={} must be strings.'\n",
    "                            .format(x_col))\n",
    "        # check labels are string if class_mode is binary or sparse\n",
    "        if self.class_mode in {'binary', 'sparse'}:\n",
    "            if not all(df[y_col].apply(lambda x: isinstance(x, str))):\n",
    "                raise TypeError('If class_mode=\"{}\", y_col=\"{}\" column '\n",
    "                                'values must be strings.'\n",
    "                                .format(self.class_mode, y_col))\n",
    "        # check that if binary there are only 2 different classes\n",
    "        if self.class_mode == 'binary':\n",
    "            if classes:\n",
    "                classes = set(classes)\n",
    "                if len(classes) != 2:\n",
    "                    raise ValueError('If class_mode=\"binary\" there must be 2 '\n",
    "                                     'classes. {} class/es were given.'\n",
    "                                     .format(len(classes)))\n",
    "            elif df[y_col].nunique() != 2:\n",
    "                raise ValueError('If class_mode=\"binary\" there must be 2 classes. '\n",
    "                                 'Found {} classes.'.format(df[y_col].nunique()))\n",
    "        # check values are string, list or tuple if class_mode is categorical\n",
    "        if self.class_mode == 'categorical':\n",
    "            types = (str, list, tuple)\n",
    "            if not all(df[y_col].apply(lambda x: isinstance(x, types))):\n",
    "                raise TypeError('If class_mode=\"{}\", y_col=\"{}\" column '\n",
    "                                'values must be type string, list or tuple.'\n",
    "                                .format(self.class_mode, y_col))\n",
    "        # raise warning if classes are given but will be unused\n",
    "        if classes and self.class_mode in {\"input\", \"multi_output\", \"raw\", None}:\n",
    "            warnings.warn('`classes` will be ignored given the class_mode=\"{}\"'\n",
    "                          .format(self.class_mode))\n",
    "        # check that if weight column that the values are numerical\n",
    "        if weight_col and not issubclass(df[weight_col].dtype.type, np.number):\n",
    "            raise TypeError('Column weight_col={} must be numeric.'\n",
    "                            .format(weight_col))\n",
    "\n",
    "    def get_classes(self, df, y_col):\n",
    "        labels = []\n",
    "        for label in df[y_col]:\n",
    "            if isinstance(label, (list, tuple)):\n",
    "                labels.append([self.class_indices[lbl] for lbl in label])\n",
    "            else:\n",
    "                labels.append(self.class_indices[label])\n",
    "        return labels\n",
    "\n",
    "    @staticmethod\n",
    "    def _filter_classes(df, y_col, classes):\n",
    "        df = df.copy()\n",
    "\n",
    "        def remove_classes(labels, classes):\n",
    "            if isinstance(labels, (list, tuple)):\n",
    "                labels = [cls for cls in labels if cls in classes]\n",
    "                return labels or None\n",
    "            elif isinstance(labels, str):\n",
    "                return labels if labels in classes else None\n",
    "            else:\n",
    "                raise TypeError(\n",
    "                    \"Expect string, list or tuple but found {} in {} column \"\n",
    "                    .format(type(labels), y_col)\n",
    "                )\n",
    "\n",
    "        if classes:\n",
    "            # prepare for membership lookup\n",
    "            classes = list(OrderedDict.fromkeys(classes).keys())\n",
    "            df[y_col] = df[y_col].apply(lambda x: remove_classes(x, classes))\n",
    "        else:\n",
    "            classes = set()\n",
    "            for v in df[y_col]:\n",
    "                if isinstance(v, (list, tuple)):\n",
    "                    classes.update(v)\n",
    "                else:\n",
    "                    classes.add(v)\n",
    "            classes = sorted(classes)\n",
    "        return df.dropna(subset=[y_col]), classes\n",
    "\n",
    "    def _filter_valid_filepaths(self, df, x_col):\n",
    "        \"\"\"Keep only dataframe rows with valid filenames\n",
    "        # Arguments\n",
    "            df: Pandas dataframe containing filenames in a column\n",
    "            x_col: string, column in `df` that contains the filenames or filepaths\n",
    "        # Returns\n",
    "            absolute paths to image files\n",
    "        \"\"\"\n",
    "        filepaths = df[x_col].map(\n",
    "            lambda fname: os.path.join(self.directory, fname)\n",
    "        )\n",
    "        mask = filepaths.apply(validate_filename, args=(self.white_list_formats,))\n",
    "        n_invalid = (~mask).sum()\n",
    "        if n_invalid:\n",
    "            warnings.warn(\n",
    "                'Found {} invalid image filename(s) in x_col=\"{}\". '\n",
    "                'These filename(s) will be ignored.'\n",
    "                .format(n_invalid, x_col)\n",
    "            )\n",
    "        return df[mask]\n",
    "\n",
    "    @property\n",
    "    def filepaths(self):\n",
    "        return self._filepaths\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        if self.class_mode in {\"multi_output\", \"raw\"}:\n",
    "            return self._targets\n",
    "        else:\n",
    "            return self.classes\n",
    "\n",
    "    @property\n",
    "    def sample_weight(self):\n",
    "        return self._sample_weight"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f606ee1-be74-4bad-8ac7-7f71faf4010b",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/image/iterator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72d1523-4ca2-4bfa-9cf5-1a2a5dc652ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = test_df['labels'].astype(str)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "19a8004a-07d0-47b6-8b87-2d70c9c5b0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 749 validated image filenames.\n",
      "749/749 [==============================] - 91s 115ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test = test_df['labels'].astype(str)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "test_g=test_datagen.flow_from_dataframe(\n",
    "     dataframe= test_df,\n",
    "      directory=\"./data/test/images\",\n",
    "      x_col=\"fnames\",\n",
    "      batch_size=1,\n",
    "      seed=42,\n",
    "      shuffle=False,\n",
    "      class_mode=None,\n",
    "      target_size=(224,224)) # to avoid loss of information\n",
    "STEP_SIZE_TEST=test_g.n//test_g.batch_size\n",
    "pred = model.predict_generator(test_g,\n",
    "                               steps=STEP_SIZE_TEST,\n",
    "                               verbose=1)\n",
    "pred_bool = (pred >0.5)\n",
    "predictions=[]\n",
    "#labels = train_generator.class_indices\n",
    "#labels = dict((v,k) for k,v in labels.items())\n",
    "labels = {}\n",
    "file1 = open('T2dot_labels.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    " \n",
    "count = 0\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    labels[count] = line.strip()\n",
    "    count += 1\n",
    "    \n",
    "for row in pred_bool:\n",
    "    l=[]\n",
    "    for index,cls in enumerate(row):\n",
    "        if cls:\n",
    "            l.append(labels[index])\n",
    "      \n",
    "    predictions.append(\",\".join(l))\n",
    "y_pred =[]\n",
    "for i in range(0, len(predictions)):\n",
    "    if predictions[i] == '':\n",
    "        y_pred.append(0)\n",
    "    else:\n",
    "        listValue  = list(map(float, predictions[i].split(',')))\n",
    "\n",
    "        y_pred.append(str(round(max(listValue), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9ad7c55a-c4f7-4cd3-af03-cf211b9ff8e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "749"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e050f-8dff-4525-afb8-fd27e125284a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aee961c0-0365-415e-919f-e1a38ffe7d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the classifier is: 0.6475300400534045\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Importing all necessary libraries\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Calculating the accuracy of classifier\n",
    "print(f\"Accuracy of the classifier is: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0e89f057-c665-4817-8826-e445e6095dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0 ...   0   0   1]\n",
      " [  0   0   0 ...   0   0   2]\n",
      " [  0   0   0 ...   0   0   1]\n",
      " ...\n",
      " [  0   0   0 ...   0   0  12]\n",
      " [  0   0   0 ...   0   0   0]\n",
      " [  0   0   0 ...   0  11 483]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# confusion_matrix funnction a matrix containing the summary of predictions\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "56df3dab-ec0f-4d8a-95ef-00f2d6641a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2916114579439253"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "  # Calculation of Mean Squared Error (MSE)\n",
    "mean_squared_error([float(i) for i in y_test], [float(i) for i in y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ac7c1-c7f8-49b2-857f-4b8b9bff395e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03517834-ec28-4da6-a4bc-ce818f1048bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da5cef1-02a2-4899-bed6-e27502a2c281",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca948645-e626-4f72-947c-8b05fd3c045f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
