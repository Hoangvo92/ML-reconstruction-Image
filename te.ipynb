{
 "cells": [
  {
   "cell_type": "raw",
   "id": "2b25b451-e7e8-476a-8910-cf745809ef08",
   "metadata": {},
   "source": [
    "get KE from image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f951d4e-51fb-4521-81cd-48eb1c5e2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "\"\"\"Import from keras_preprocessing not from keras.preprocessing, because Keras may or maynot contain the features discussed here depending upon when you read this article, until the keras_preprocessed library is updated in Keras use the github version.\"\"\"\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import regularizers, optimizers\n",
    "#from tensorflow.keras import optimizers #., optimizers\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#Importing all the relevant library\n",
    "%matplotlib inline\n",
    "import h5py, os\n",
    "#from functions import transforms as T\n",
    "#from functions.subsample import MaskFunc\n",
    "from scipy.io import loadmat\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import glob\n",
    "#from functions import transforms as T \n",
    "#from functions.subsample import MaskFunc\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd8d08e5-d611-4b99-add9-02cc854a9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_path(train_data_path, val_data_path, test_data_path):\n",
    "    \"\"\" Go through each subset (training, validation) and list all \n",
    "    the file names, the file paths and the slices of subjects in the training and validation sets \n",
    "    \"\"\"\n",
    "\n",
    "    data_list = {}\n",
    "    train_val_test = ['train', 'val', 'test']\n",
    "    data_path = [train_data_path, val_data_path, test_data_path]\n",
    "      \n",
    "    for i in range(len(data_path)):\n",
    "\n",
    "        data_list[train_val_test[i]] = []\n",
    "        \n",
    "        which_data_path = data_path[i]\n",
    "        tr = 0\n",
    "        te = 0\n",
    "        alfa = 0\n",
    "        for fname in sorted(os.listdir(which_data_path + '/images')):\n",
    "            if fname != \".DS_Store\":\n",
    "\n",
    "            \n",
    "                subject_data_path = os.path.join(which_data_path + '/images', fname)\n",
    "                     \n",
    "                if not os.path.isfile(subject_data_path): continue \n",
    "            \n",
    "          #  im_frame = Image.open(subject_data_path)\n",
    "\n",
    "            #get information from text file\n",
    "            # this will return a tuple of root and extension\n",
    "                split_tup = os.path.splitext(fname)\n",
    "\n",
    "  \n",
    "            # extract the file name and extension\n",
    "                file_name = split_tup[0]\n",
    "                file_path = os.path.join(which_data_path + '/texts', file_name) + '.txt'\n",
    "                f = open(os.path.join(which_data_path + '/texts', file_name) + '.txt', 'r')\n",
    "                line = f.readlines()[1]\n",
    "            \n",
    "                fields = line.split(',')\n",
    "                tr = int(fields[3])\n",
    "                te = int(fields[4])\n",
    "                alfa = int(fields[5])\n",
    "                f.close()\n",
    "                \n",
    "            # the first 5 slices are mostly noise so it is better to exlude them\n",
    "                data_list[train_val_test[i]] += [(fname, te)]\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e859bb9f-fa84-4609-bf9e-e2decd504740",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = load_data_path (\"data/train\", \"data/val\", \"data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e6528b-7dbc-44ca-86f3-810e789cba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_list['train']\n",
    "val_data = data_list['val']\n",
    "test_data = data_list['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08aa5111-5a82-4237-ba6c-72ee17a69ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(train_data, columns=['fnames', 'labels'])\n",
    "train_df['labels']= train_df['labels'].astype(str)\n",
    "train_df['fnames']= train_df['fnames'].astype(str)\n",
    "val_df = pd.DataFrame(val_data, columns=['fnames', 'labels'])\n",
    "val_df['labels']= val_df['labels'].astype(str)\n",
    "val_df['fnames']= val_df['fnames'].astype(str)\n",
    "test_df = pd.DataFrame(test_data, columns=['fnames', 'labels'])\n",
    "test_df['labels']= test_df['labels'].astype(str)\n",
    "test_df['fnames']= test_df['fnames'].astype(str)\n",
    "labels = train_df.labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce8641ab-96af-4fea-8b4e-2b4e38f108a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_class = len(labels)\n",
    "num_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3c7e74a-edb4-4d1a-b1f4-52addf03c259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of         fnames labels\n",
       "0        1.png     11\n",
       "1       10.png      3\n",
       "2      100.png     11\n",
       "3     1000.png      7\n",
       "4     1001.png     13\n",
       "...        ...    ...\n",
       "3060   995.png      2\n",
       "3061   996.png      3\n",
       "3062   997.png     10\n",
       "3063   998.png      3\n",
       "3064   999.png      3\n",
       "\n",
       "[3065 rows x 2 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "beca63cc-5587-4daf-b728-00942ebfa873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3065 validated image filenames belonging to 18 classes.\n",
      "Found 655 validated image filenames belonging to 18 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen=ImageDataGenerator(rescale=1./255.)\n",
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "train_generator=datagen.flow_from_dataframe(\n",
    "      dataframe=train_df,\n",
    "      directory=\"./data/train/images/\",\n",
    "      x_col=\"fnames\",\n",
    "      y_col=\"labels\",\n",
    "      batch_size=64,\n",
    "      seed=42,\n",
    "      shuffle=True,\n",
    "      class_mode=\"categorical\",\n",
    "      target_size=(224,224))\n",
    "\n",
    "\n",
    "\n",
    "valid_generator=test_datagen.flow_from_dataframe(\n",
    "      dataframe=val_df,\n",
    "      directory=\"./data/val/images/\",\n",
    "      x_col=\"fnames\",\n",
    "      y_col=\"labels\",\n",
    "      batch_size=64,\n",
    "      seed=42,\n",
    "      shuffle=True,\n",
    "      class_mode=\"categorical\",\n",
    "     target_size=(224,224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49c790a0-c070-46fc-a219-c03f065e41d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 646 validated image filenames.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/keras_preprocessing/image/dataframe_iterator.py:282: UserWarning: Found 9 invalid image filename(s) in x_col=\"fnames\". These filename(s) will be ignored.\n",
      "  .format(n_invalid, x_col)\n"
     ]
    }
   ],
   "source": [
    "test_generator=test_datagen.flow_from_dataframe(\n",
    "dataframe=test_df,\n",
    "      directory=\"./data/test/images\",\n",
    "      x_col=\"fnames\",\n",
    "      batch_size=1,\n",
    "      seed=42,\n",
    "      shuffle=False,\n",
    "      class_mode=None,\n",
    "      target_size=(224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4403221-0e45-4cc5-9cc8-6e84aa7d76e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-30 16:59:38.325782: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=(224,224,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_class, activation='sigmoid'))\n",
    "model.compile(tf.keras.optimizers.RMSprop(lr=0.001, decay=1e-6),loss=\"binary_crossentropy\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5ee260d-ecc7-46a5-988f-9ca7b092e3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 300 #100 #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f69b861c-1472-444a-a2ee-c158e0a96e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
      "2021-11-30 16:59:39.574789: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "47/47 [==============================] - 252s 5s/step - loss: 0.3885 - accuracy: 0.1387 - val_loss: 0.2063 - val_accuracy: 0.2734\n",
      "Epoch 2/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.1985 - accuracy: 0.1966 - val_loss: 0.2016 - val_accuracy: 0.2641\n",
      "Epoch 3/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.1807 - accuracy: 0.2542 - val_loss: 0.1612 - val_accuracy: 0.2953\n",
      "Epoch 4/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.1613 - accuracy: 0.2959 - val_loss: 0.1507 - val_accuracy: 0.3078\n",
      "Epoch 5/300\n",
      "47/47 [==============================] - 243s 5s/step - loss: 0.1469 - accuracy: 0.3478 - val_loss: 0.1441 - val_accuracy: 0.3297\n",
      "Epoch 6/300\n",
      "47/47 [==============================] - 242s 5s/step - loss: 0.1403 - accuracy: 0.3707 - val_loss: 0.1386 - val_accuracy: 0.3500\n",
      "Epoch 7/300\n",
      "47/47 [==============================] - 243s 5s/step - loss: 0.1336 - accuracy: 0.3756 - val_loss: 0.1255 - val_accuracy: 0.3953\n",
      "Epoch 8/300\n",
      "47/47 [==============================] - 245s 5s/step - loss: 0.1464 - accuracy: 0.4116 - val_loss: 0.1249 - val_accuracy: 0.4125\n",
      "Epoch 9/300\n",
      "47/47 [==============================] - 242s 5s/step - loss: 0.1231 - accuracy: 0.4355 - val_loss: 0.1198 - val_accuracy: 0.4469\n",
      "Epoch 10/300\n",
      "47/47 [==============================] - 241s 5s/step - loss: 0.1205 - accuracy: 0.4442 - val_loss: 0.1222 - val_accuracy: 0.4484\n",
      "Epoch 11/300\n",
      "47/47 [==============================] - 258s 6s/step - loss: 0.1211 - accuracy: 0.4439 - val_loss: 0.1177 - val_accuracy: 0.4109\n",
      "Epoch 12/300\n",
      "47/47 [==============================] - 247s 5s/step - loss: 0.1163 - accuracy: 0.4523 - val_loss: 0.1223 - val_accuracy: 0.4109\n",
      "Epoch 13/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.1120 - accuracy: 0.4867 - val_loss: 0.1189 - val_accuracy: 0.4281\n",
      "Epoch 14/300\n",
      "47/47 [==============================] - 241s 5s/step - loss: 0.1045 - accuracy: 0.5323 - val_loss: 0.1214 - val_accuracy: 0.4047\n",
      "Epoch 15/300\n",
      "47/47 [==============================] - 247s 5s/step - loss: 0.1091 - accuracy: 0.4965 - val_loss: 0.1086 - val_accuracy: 0.4891\n",
      "Epoch 16/300\n",
      "47/47 [==============================] - 243s 5s/step - loss: 0.1016 - accuracy: 0.5284 - val_loss: 0.1274 - val_accuracy: 0.4406\n",
      "Epoch 17/300\n",
      "47/47 [==============================] - 266s 6s/step - loss: 0.1008 - accuracy: 0.5265 - val_loss: 0.1105 - val_accuracy: 0.4594\n",
      "Epoch 18/300\n",
      "47/47 [==============================] - 271s 6s/step - loss: 0.1292 - accuracy: 0.5220 - val_loss: 0.1113 - val_accuracy: 0.4531\n",
      "Epoch 19/300\n",
      "47/47 [==============================] - 279s 6s/step - loss: 0.0975 - accuracy: 0.5504 - val_loss: 0.1113 - val_accuracy: 0.4734\n",
      "Epoch 20/300\n",
      "47/47 [==============================] - 256s 5s/step - loss: 0.0973 - accuracy: 0.5350 - val_loss: 0.1163 - val_accuracy: 0.4500\n",
      "Epoch 21/300\n",
      "47/47 [==============================] - 260s 6s/step - loss: 0.0932 - accuracy: 0.5527 - val_loss: 0.1137 - val_accuracy: 0.4703\n",
      "Epoch 22/300\n",
      "47/47 [==============================] - 249s 5s/step - loss: 0.0939 - accuracy: 0.5488 - val_loss: 0.1087 - val_accuracy: 0.4953\n",
      "Epoch 23/300\n",
      "47/47 [==============================] - 248s 5s/step - loss: 0.0925 - accuracy: 0.5580 - val_loss: 0.1145 - val_accuracy: 0.4531\n",
      "Epoch 24/300\n",
      "47/47 [==============================] - 250s 5s/step - loss: 0.0931 - accuracy: 0.5655 - val_loss: 0.1128 - val_accuracy: 0.4656\n",
      "Epoch 25/300\n",
      "47/47 [==============================] - 269s 6s/step - loss: 0.0903 - accuracy: 0.5675 - val_loss: 0.1112 - val_accuracy: 0.4563\n",
      "Epoch 26/300\n",
      "47/47 [==============================] - 272s 6s/step - loss: 0.0889 - accuracy: 0.5770 - val_loss: 0.1103 - val_accuracy: 0.4891\n",
      "Epoch 27/300\n",
      "47/47 [==============================] - 243s 5s/step - loss: 0.0959 - accuracy: 0.5541 - val_loss: 0.1118 - val_accuracy: 0.4906\n",
      "Epoch 28/300\n",
      "47/47 [==============================] - 242s 5s/step - loss: 0.0880 - accuracy: 0.5823 - val_loss: 0.1195 - val_accuracy: 0.4672\n",
      "Epoch 29/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.0895 - accuracy: 0.5739 - val_loss: 0.1188 - val_accuracy: 0.4500\n",
      "Epoch 30/300\n",
      "47/47 [==============================] - 241s 5s/step - loss: 0.0891 - accuracy: 0.5786 - val_loss: 0.1229 - val_accuracy: 0.4578\n",
      "Epoch 31/300\n",
      "47/47 [==============================] - 241s 5s/step - loss: 0.0867 - accuracy: 0.5839 - val_loss: 0.1206 - val_accuracy: 0.4625\n",
      "Epoch 32/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.0876 - accuracy: 0.5838 - val_loss: 0.1190 - val_accuracy: 0.4609\n",
      "Epoch 33/300\n",
      "47/47 [==============================] - 241s 5s/step - loss: 0.0871 - accuracy: 0.5931 - val_loss: 0.1253 - val_accuracy: 0.4578\n",
      "Epoch 34/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.0849 - accuracy: 0.5875 - val_loss: 0.1281 - val_accuracy: 0.4672\n",
      "Epoch 35/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.0810 - accuracy: 0.6068 - val_loss: 0.1147 - val_accuracy: 0.4719\n",
      "Epoch 36/300\n",
      "47/47 [==============================] - 241s 5s/step - loss: 0.0830 - accuracy: 0.6081 - val_loss: 0.1317 - val_accuracy: 0.4516\n",
      "Epoch 37/300\n",
      "47/47 [==============================] - 241s 5s/step - loss: 0.0839 - accuracy: 0.5990 - val_loss: 0.1269 - val_accuracy: 0.4469\n",
      "Epoch 38/300\n",
      "47/47 [==============================] - 241s 5s/step - loss: 0.0825 - accuracy: 0.6117 - val_loss: 0.1300 - val_accuracy: 0.4594\n",
      "Epoch 39/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.0810 - accuracy: 0.6207 - val_loss: 0.1251 - val_accuracy: 0.4828\n",
      "Epoch 40/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.0764 - accuracy: 0.6388 - val_loss: 0.1366 - val_accuracy: 0.4641\n",
      "Epoch 41/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.0829 - accuracy: 0.6067 - val_loss: 0.1505 - val_accuracy: 0.4328\n",
      "Epoch 42/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0800 - accuracy: 0.6244 - val_loss: 0.1329 - val_accuracy: 0.4625\n",
      "Epoch 43/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0801 - accuracy: 0.6207 - val_loss: 0.1328 - val_accuracy: 0.4875\n",
      "Epoch 44/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0765 - accuracy: 0.6416 - val_loss: 0.1295 - val_accuracy: 0.4594\n",
      "Epoch 45/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0770 - accuracy: 0.6373 - val_loss: 0.1267 - val_accuracy: 0.4422\n",
      "Epoch 46/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0751 - accuracy: 0.6404 - val_loss: 0.1314 - val_accuracy: 0.4672\n",
      "Epoch 47/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0761 - accuracy: 0.6389 - val_loss: 0.1251 - val_accuracy: 0.4750\n",
      "Epoch 48/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0761 - accuracy: 0.6406 - val_loss: 0.1426 - val_accuracy: 0.4688\n",
      "Epoch 49/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0746 - accuracy: 0.6437 - val_loss: 0.1285 - val_accuracy: 0.4734\n",
      "Epoch 50/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0768 - accuracy: 0.6399 - val_loss: 0.1338 - val_accuracy: 0.4875\n",
      "Epoch 51/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0755 - accuracy: 0.6454 - val_loss: 0.1357 - val_accuracy: 0.4656\n",
      "Epoch 52/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0751 - accuracy: 0.6430 - val_loss: 0.1458 - val_accuracy: 0.4672\n",
      "Epoch 53/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0776 - accuracy: 0.6364 - val_loss: 0.1589 - val_accuracy: 0.4594\n",
      "Epoch 54/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0739 - accuracy: 0.6510 - val_loss: 0.1372 - val_accuracy: 0.4719\n",
      "Epoch 55/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0763 - accuracy: 0.6622 - val_loss: 0.1674 - val_accuracy: 0.4750\n",
      "Epoch 56/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0720 - accuracy: 0.6568 - val_loss: 0.1507 - val_accuracy: 0.4625\n",
      "Epoch 57/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0726 - accuracy: 0.6575 - val_loss: 0.1535 - val_accuracy: 0.4844\n",
      "Epoch 58/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0743 - accuracy: 0.6374 - val_loss: 0.1567 - val_accuracy: 0.4734\n",
      "Epoch 59/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0714 - accuracy: 0.6627 - val_loss: 0.1489 - val_accuracy: 0.4672\n",
      "Epoch 60/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0722 - accuracy: 0.6511 - val_loss: 0.1409 - val_accuracy: 0.4703\n",
      "Epoch 61/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0724 - accuracy: 0.6599 - val_loss: 0.1689 - val_accuracy: 0.4297\n",
      "Epoch 62/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0734 - accuracy: 0.6597 - val_loss: 0.1427 - val_accuracy: 0.4859\n",
      "Epoch 63/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0711 - accuracy: 0.6633 - val_loss: 0.1364 - val_accuracy: 0.4594\n",
      "Epoch 64/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0694 - accuracy: 0.6731 - val_loss: 0.1512 - val_accuracy: 0.4453\n",
      "Epoch 65/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0874 - accuracy: 0.6486 - val_loss: 0.1478 - val_accuracy: 0.4859\n",
      "Epoch 66/300\n",
      "47/47 [==============================] - 252s 5s/step - loss: 0.0697 - accuracy: 0.6716 - val_loss: 0.1532 - val_accuracy: 0.4563\n",
      "Epoch 67/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0696 - accuracy: 0.6726 - val_loss: 0.1475 - val_accuracy: 0.4703\n",
      "Epoch 68/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0684 - accuracy: 0.6775 - val_loss: 0.1542 - val_accuracy: 0.4625\n",
      "Epoch 69/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0680 - accuracy: 0.6793 - val_loss: 0.1548 - val_accuracy: 0.4641\n",
      "Epoch 70/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0735 - accuracy: 0.6644 - val_loss: 0.1522 - val_accuracy: 0.4656\n",
      "Epoch 71/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0679 - accuracy: 0.6726 - val_loss: 0.1717 - val_accuracy: 0.4641\n",
      "Epoch 72/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0657 - accuracy: 0.6912 - val_loss: 0.1453 - val_accuracy: 0.4641\n",
      "Epoch 73/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0678 - accuracy: 0.6722 - val_loss: 0.1656 - val_accuracy: 0.4734\n",
      "Epoch 74/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0675 - accuracy: 0.6812 - val_loss: 0.1756 - val_accuracy: 0.4688\n",
      "Epoch 75/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0662 - accuracy: 0.6923 - val_loss: 0.1660 - val_accuracy: 0.4625\n",
      "Epoch 76/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0685 - accuracy: 0.6771 - val_loss: 0.1647 - val_accuracy: 0.4781\n",
      "Epoch 77/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0687 - accuracy: 0.6772 - val_loss: 0.1532 - val_accuracy: 0.4641\n",
      "Epoch 78/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0652 - accuracy: 0.6908 - val_loss: 0.1882 - val_accuracy: 0.4656\n",
      "Epoch 79/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0686 - accuracy: 0.6773 - val_loss: 0.1800 - val_accuracy: 0.4656\n",
      "Epoch 80/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0665 - accuracy: 0.6818 - val_loss: 0.1772 - val_accuracy: 0.4719\n",
      "Epoch 81/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0656 - accuracy: 0.6965 - val_loss: 0.1832 - val_accuracy: 0.4734\n",
      "Epoch 82/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0669 - accuracy: 0.6931 - val_loss: 0.1792 - val_accuracy: 0.4688\n",
      "Epoch 83/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0661 - accuracy: 0.6764 - val_loss: 0.1542 - val_accuracy: 0.4531\n",
      "Epoch 84/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0673 - accuracy: 0.6774 - val_loss: 0.1668 - val_accuracy: 0.4672\n",
      "Epoch 85/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0699 - accuracy: 0.6794 - val_loss: 0.1665 - val_accuracy: 0.4609\n",
      "Epoch 86/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0640 - accuracy: 0.7063 - val_loss: 0.1735 - val_accuracy: 0.4656\n",
      "Epoch 87/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0663 - accuracy: 0.6878 - val_loss: 0.1524 - val_accuracy: 0.4656\n",
      "Epoch 88/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0671 - accuracy: 0.6643 - val_loss: 0.2164 - val_accuracy: 0.4625\n",
      "Epoch 89/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0669 - accuracy: 0.6870 - val_loss: 0.1720 - val_accuracy: 0.4563\n",
      "Epoch 90/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0657 - accuracy: 0.6874 - val_loss: 0.1537 - val_accuracy: 0.4344\n",
      "Epoch 91/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0646 - accuracy: 0.7034 - val_loss: 0.1951 - val_accuracy: 0.4437\n",
      "Epoch 92/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0693 - accuracy: 0.6859 - val_loss: 0.2279 - val_accuracy: 0.4500\n",
      "Epoch 93/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0662 - accuracy: 0.6945 - val_loss: 0.1540 - val_accuracy: 0.4734\n",
      "Epoch 94/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0671 - accuracy: 0.6912 - val_loss: 0.1670 - val_accuracy: 0.4359\n",
      "Epoch 95/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0652 - accuracy: 0.6915 - val_loss: 0.1842 - val_accuracy: 0.4359\n",
      "Epoch 96/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0634 - accuracy: 0.6922 - val_loss: 0.2267 - val_accuracy: 0.4766\n",
      "Epoch 97/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0645 - accuracy: 0.6980 - val_loss: 0.2076 - val_accuracy: 0.4656\n",
      "Epoch 98/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0652 - accuracy: 0.7010 - val_loss: 0.1749 - val_accuracy: 0.4625\n",
      "Epoch 99/300\n",
      "47/47 [==============================] - 239s 5s/step - loss: 0.0648 - accuracy: 0.6953 - val_loss: 0.1707 - val_accuracy: 0.4594\n",
      "Epoch 100/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0657 - accuracy: 0.6868 - val_loss: 0.1915 - val_accuracy: 0.4656\n",
      "Epoch 101/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0641 - accuracy: 0.6917 - val_loss: 0.1642 - val_accuracy: 0.4672\n",
      "Epoch 102/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0632 - accuracy: 0.6972 - val_loss: 0.2073 - val_accuracy: 0.4641\n",
      "Epoch 103/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0682 - accuracy: 0.6849 - val_loss: 0.2027 - val_accuracy: 0.4437\n",
      "Epoch 104/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0650 - accuracy: 0.6969 - val_loss: 0.1757 - val_accuracy: 0.4453\n",
      "Epoch 105/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0636 - accuracy: 0.6933 - val_loss: 0.1493 - val_accuracy: 0.4641\n",
      "Epoch 106/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0634 - accuracy: 0.6994 - val_loss: 0.1684 - val_accuracy: 0.4766\n",
      "Epoch 107/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0640 - accuracy: 0.7062 - val_loss: 0.1720 - val_accuracy: 0.4500\n",
      "Epoch 108/300\n",
      "47/47 [==============================] - 243s 5s/step - loss: 0.0657 - accuracy: 0.6999 - val_loss: 0.2037 - val_accuracy: 0.4578\n",
      "Epoch 109/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0653 - accuracy: 0.6902 - val_loss: 0.2158 - val_accuracy: 0.4609\n",
      "Epoch 110/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0661 - accuracy: 0.6871 - val_loss: 0.1736 - val_accuracy: 0.4500\n",
      "Epoch 111/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0658 - accuracy: 0.6913 - val_loss: 0.1776 - val_accuracy: 0.4641\n",
      "Epoch 112/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0636 - accuracy: 0.6957 - val_loss: 0.2043 - val_accuracy: 0.4672\n",
      "Epoch 113/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0636 - accuracy: 0.6918 - val_loss: 0.1693 - val_accuracy: 0.4547\n",
      "Epoch 114/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0601 - accuracy: 0.7129 - val_loss: 0.2291 - val_accuracy: 0.4453\n",
      "Epoch 115/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0619 - accuracy: 0.6993 - val_loss: 0.1750 - val_accuracy: 0.4672\n",
      "Epoch 116/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0618 - accuracy: 0.7101 - val_loss: 0.2103 - val_accuracy: 0.4594\n",
      "Epoch 117/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0592 - accuracy: 0.7202 - val_loss: 0.1849 - val_accuracy: 0.4656\n",
      "Epoch 118/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0617 - accuracy: 0.7036 - val_loss: 0.1730 - val_accuracy: 0.4781\n",
      "Epoch 119/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0659 - accuracy: 0.7096 - val_loss: 0.1977 - val_accuracy: 0.4797\n",
      "Epoch 120/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0607 - accuracy: 0.7182 - val_loss: 0.2348 - val_accuracy: 0.4469\n",
      "Epoch 121/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0618 - accuracy: 0.7037 - val_loss: 0.1658 - val_accuracy: 0.4594\n",
      "Epoch 122/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0607 - accuracy: 0.7163 - val_loss: 0.2659 - val_accuracy: 0.4391\n",
      "Epoch 123/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0617 - accuracy: 0.7125 - val_loss: 0.2135 - val_accuracy: 0.4828\n",
      "Epoch 124/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0649 - accuracy: 0.6942 - val_loss: 0.2025 - val_accuracy: 0.4469\n",
      "Epoch 125/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0620 - accuracy: 0.7102 - val_loss: 0.2594 - val_accuracy: 0.4359\n",
      "Epoch 126/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0610 - accuracy: 0.7182 - val_loss: 0.1945 - val_accuracy: 0.4688\n",
      "Epoch 127/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0631 - accuracy: 0.7024 - val_loss: 0.2014 - val_accuracy: 0.4703\n",
      "Epoch 128/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0725 - accuracy: 0.7033 - val_loss: 0.1837 - val_accuracy: 0.4594\n",
      "Epoch 129/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0613 - accuracy: 0.7160 - val_loss: 0.2130 - val_accuracy: 0.4656\n",
      "Epoch 130/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0624 - accuracy: 0.6979 - val_loss: 0.1784 - val_accuracy: 0.4641\n",
      "Epoch 131/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0600 - accuracy: 0.7105 - val_loss: 0.2503 - val_accuracy: 0.4500\n",
      "Epoch 132/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0631 - accuracy: 0.7052 - val_loss: 0.2563 - val_accuracy: 0.4328\n",
      "Epoch 133/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0613 - accuracy: 0.7052 - val_loss: 0.2300 - val_accuracy: 0.4359\n",
      "Epoch 134/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0609 - accuracy: 0.7177 - val_loss: 0.1681 - val_accuracy: 0.4391\n",
      "Epoch 135/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0639 - accuracy: 0.6984 - val_loss: 0.2066 - val_accuracy: 0.4547\n",
      "Epoch 136/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0604 - accuracy: 0.7167 - val_loss: 0.2332 - val_accuracy: 0.4547\n",
      "Epoch 137/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0642 - accuracy: 0.7078 - val_loss: 0.2045 - val_accuracy: 0.4406\n",
      "Epoch 138/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0589 - accuracy: 0.7181 - val_loss: 0.2188 - val_accuracy: 0.4313\n",
      "Epoch 139/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0587 - accuracy: 0.7301 - val_loss: 0.1820 - val_accuracy: 0.4469\n",
      "Epoch 140/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0595 - accuracy: 0.7218 - val_loss: 0.2080 - val_accuracy: 0.4594\n",
      "Epoch 141/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0565 - accuracy: 0.7328 - val_loss: 0.2191 - val_accuracy: 0.4703\n",
      "Epoch 142/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0590 - accuracy: 0.7125 - val_loss: 0.2357 - val_accuracy: 0.4531\n",
      "Epoch 143/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0607 - accuracy: 0.6931 - val_loss: 0.2193 - val_accuracy: 0.4641\n",
      "Epoch 144/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0586 - accuracy: 0.7303 - val_loss: 0.2461 - val_accuracy: 0.4578\n",
      "Epoch 145/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0604 - accuracy: 0.7134 - val_loss: 0.2299 - val_accuracy: 0.4437\n",
      "Epoch 146/300\n",
      "47/47 [==============================] - 231s 5s/step - loss: 0.0600 - accuracy: 0.7112 - val_loss: 0.2056 - val_accuracy: 0.4703\n",
      "Epoch 147/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0600 - accuracy: 0.7220 - val_loss: 0.2186 - val_accuracy: 0.4406\n",
      "Epoch 148/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0578 - accuracy: 0.7312 - val_loss: 0.2323 - val_accuracy: 0.4500\n",
      "Epoch 149/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0595 - accuracy: 0.7172 - val_loss: 0.2255 - val_accuracy: 0.4594\n",
      "Epoch 150/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0580 - accuracy: 0.7176 - val_loss: 0.2225 - val_accuracy: 0.4547\n",
      "Epoch 151/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0584 - accuracy: 0.7193 - val_loss: 0.1753 - val_accuracy: 0.4484\n",
      "Epoch 152/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0597 - accuracy: 0.7189 - val_loss: 0.1867 - val_accuracy: 0.4547\n",
      "Epoch 153/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0574 - accuracy: 0.7291 - val_loss: 0.2179 - val_accuracy: 0.4391\n",
      "Epoch 154/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0565 - accuracy: 0.7280 - val_loss: 0.1777 - val_accuracy: 0.4688\n",
      "Epoch 155/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0640 - accuracy: 0.7232 - val_loss: 0.2210 - val_accuracy: 0.4437\n",
      "Epoch 156/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0579 - accuracy: 0.7256 - val_loss: 0.2313 - val_accuracy: 0.4547\n",
      "Epoch 157/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0683 - accuracy: 0.7312 - val_loss: 0.2816 - val_accuracy: 0.4547\n",
      "Epoch 158/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0585 - accuracy: 0.7175 - val_loss: 0.2032 - val_accuracy: 0.4641\n",
      "Epoch 159/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0662 - accuracy: 0.7096 - val_loss: 0.2447 - val_accuracy: 0.4375\n",
      "Epoch 160/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0568 - accuracy: 0.7295 - val_loss: 0.2523 - val_accuracy: 0.4609\n",
      "Epoch 161/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0580 - accuracy: 0.7174 - val_loss: 0.2238 - val_accuracy: 0.4531\n",
      "Epoch 162/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0575 - accuracy: 0.7257 - val_loss: 0.2329 - val_accuracy: 0.4844\n",
      "Epoch 163/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0586 - accuracy: 0.7198 - val_loss: 0.2016 - val_accuracy: 0.4516\n",
      "Epoch 164/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0591 - accuracy: 0.7170 - val_loss: 0.1993 - val_accuracy: 0.4422\n",
      "Epoch 165/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0566 - accuracy: 0.7379 - val_loss: 0.2468 - val_accuracy: 0.4688\n",
      "Epoch 166/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0559 - accuracy: 0.7365 - val_loss: 0.2283 - val_accuracy: 0.4375\n",
      "Epoch 167/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0767 - accuracy: 0.7065 - val_loss: 0.2110 - val_accuracy: 0.4672\n",
      "Epoch 168/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0574 - accuracy: 0.7269 - val_loss: 0.1985 - val_accuracy: 0.4484\n",
      "Epoch 169/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0623 - accuracy: 0.7366 - val_loss: 0.2005 - val_accuracy: 0.4781\n",
      "Epoch 170/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0585 - accuracy: 0.7077 - val_loss: 0.2181 - val_accuracy: 0.4719\n",
      "Epoch 171/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0566 - accuracy: 0.7230 - val_loss: 0.2656 - val_accuracy: 0.4703\n",
      "Epoch 172/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0594 - accuracy: 0.7258 - val_loss: 0.2480 - val_accuracy: 0.4594\n",
      "Epoch 173/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0769 - accuracy: 0.7196 - val_loss: 0.2130 - val_accuracy: 0.4672\n",
      "Epoch 174/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0741 - accuracy: 0.6626 - val_loss: 0.2976 - val_accuracy: 0.4297\n",
      "Epoch 175/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0635 - accuracy: 0.7114 - val_loss: 0.2263 - val_accuracy: 0.4578\n",
      "Epoch 176/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0595 - accuracy: 0.7298 - val_loss: 0.2576 - val_accuracy: 0.4734\n",
      "Epoch 177/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0584 - accuracy: 0.7094 - val_loss: 0.1950 - val_accuracy: 0.4531\n",
      "Epoch 178/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0565 - accuracy: 0.7453 - val_loss: 0.2763 - val_accuracy: 0.4703\n",
      "Epoch 179/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0558 - accuracy: 0.7351 - val_loss: 0.3213 - val_accuracy: 0.4578\n",
      "Epoch 180/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0567 - accuracy: 0.7311 - val_loss: 0.1729 - val_accuracy: 0.4500\n",
      "Epoch 181/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0564 - accuracy: 0.7338 - val_loss: 0.2496 - val_accuracy: 0.4609\n",
      "Epoch 182/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0572 - accuracy: 0.7436 - val_loss: 0.2289 - val_accuracy: 0.4766\n",
      "Epoch 183/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0577 - accuracy: 0.7326 - val_loss: 0.2212 - val_accuracy: 0.4578\n",
      "Epoch 184/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0567 - accuracy: 0.7333 - val_loss: 0.2660 - val_accuracy: 0.4672\n",
      "Epoch 185/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0577 - accuracy: 0.7190 - val_loss: 0.1913 - val_accuracy: 0.4469\n",
      "Epoch 186/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0573 - accuracy: 0.7225 - val_loss: 0.2578 - val_accuracy: 0.4688\n",
      "Epoch 187/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0573 - accuracy: 0.7369 - val_loss: 0.2322 - val_accuracy: 0.4672\n",
      "Epoch 188/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0551 - accuracy: 0.7419 - val_loss: 0.2490 - val_accuracy: 0.4625\n",
      "Epoch 189/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0545 - accuracy: 0.7423 - val_loss: 0.1867 - val_accuracy: 0.4656\n",
      "Epoch 190/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0579 - accuracy: 0.7343 - val_loss: 0.2316 - val_accuracy: 0.4453\n",
      "Epoch 191/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0612 - accuracy: 0.7289 - val_loss: 0.2861 - val_accuracy: 0.4453\n",
      "Epoch 192/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0560 - accuracy: 0.7404 - val_loss: 0.2296 - val_accuracy: 0.4703\n",
      "Epoch 193/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0544 - accuracy: 0.7479 - val_loss: 0.2465 - val_accuracy: 0.4656\n",
      "Epoch 194/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0615 - accuracy: 0.7356 - val_loss: 0.2534 - val_accuracy: 0.4609\n",
      "Epoch 195/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0531 - accuracy: 0.7452 - val_loss: 0.2674 - val_accuracy: 0.4656\n",
      "Epoch 196/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0563 - accuracy: 0.7188 - val_loss: 0.2831 - val_accuracy: 0.4563\n",
      "Epoch 197/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0560 - accuracy: 0.7311 - val_loss: 0.2200 - val_accuracy: 0.4625\n",
      "Epoch 198/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0537 - accuracy: 0.7388 - val_loss: 0.2156 - val_accuracy: 0.4469\n",
      "Epoch 199/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0604 - accuracy: 0.7230 - val_loss: 0.2558 - val_accuracy: 0.4516\n",
      "Epoch 200/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0549 - accuracy: 0.7417 - val_loss: 0.2548 - val_accuracy: 0.4344\n",
      "Epoch 201/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0539 - accuracy: 0.7407 - val_loss: 0.2536 - val_accuracy: 0.4437\n",
      "Epoch 202/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0558 - accuracy: 0.7356 - val_loss: 0.2473 - val_accuracy: 0.4500\n",
      "Epoch 203/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0528 - accuracy: 0.7451 - val_loss: 0.2151 - val_accuracy: 0.4500\n",
      "Epoch 204/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0540 - accuracy: 0.7377 - val_loss: 0.2340 - val_accuracy: 0.4484\n",
      "Epoch 205/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0557 - accuracy: 0.7395 - val_loss: 0.2335 - val_accuracy: 0.4594\n",
      "Epoch 206/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0529 - accuracy: 0.7436 - val_loss: 0.2713 - val_accuracy: 0.4437\n",
      "Epoch 207/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0579 - accuracy: 0.7324 - val_loss: 0.2595 - val_accuracy: 0.4703\n",
      "Epoch 208/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0529 - accuracy: 0.7486 - val_loss: 0.2814 - val_accuracy: 0.4500\n",
      "Epoch 209/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0550 - accuracy: 0.7458 - val_loss: 0.2184 - val_accuracy: 0.4797\n",
      "Epoch 210/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0555 - accuracy: 0.7461 - val_loss: 0.2228 - val_accuracy: 0.4672\n",
      "Epoch 211/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0529 - accuracy: 0.7503 - val_loss: 0.2157 - val_accuracy: 0.4453\n",
      "Epoch 212/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0550 - accuracy: 0.7344 - val_loss: 0.2136 - val_accuracy: 0.4531\n",
      "Epoch 213/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0554 - accuracy: 0.7348 - val_loss: 0.2274 - val_accuracy: 0.4531\n",
      "Epoch 214/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0522 - accuracy: 0.7561 - val_loss: 0.2537 - val_accuracy: 0.4437\n",
      "Epoch 215/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0547 - accuracy: 0.7305 - val_loss: 0.2416 - val_accuracy: 0.4531\n",
      "Epoch 216/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0549 - accuracy: 0.7520 - val_loss: 0.2283 - val_accuracy: 0.4547\n",
      "Epoch 217/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0556 - accuracy: 0.7263 - val_loss: 0.3381 - val_accuracy: 0.4391\n",
      "Epoch 218/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0537 - accuracy: 0.7501 - val_loss: 0.2798 - val_accuracy: 0.4641\n",
      "Epoch 219/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0521 - accuracy: 0.7520 - val_loss: 0.2745 - val_accuracy: 0.4359\n",
      "Epoch 220/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0543 - accuracy: 0.7361 - val_loss: 0.2173 - val_accuracy: 0.4422\n",
      "Epoch 221/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0554 - accuracy: 0.7384 - val_loss: 0.3315 - val_accuracy: 0.4437\n",
      "Epoch 222/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0547 - accuracy: 0.7376 - val_loss: 0.1915 - val_accuracy: 0.4625\n",
      "Epoch 223/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0537 - accuracy: 0.7506 - val_loss: 0.2737 - val_accuracy: 0.4500\n",
      "Epoch 224/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0517 - accuracy: 0.7542 - val_loss: 0.2694 - val_accuracy: 0.4625\n",
      "Epoch 225/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0551 - accuracy: 0.7404 - val_loss: 0.2476 - val_accuracy: 0.4531\n",
      "Epoch 226/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0540 - accuracy: 0.7447 - val_loss: 0.3307 - val_accuracy: 0.4516\n",
      "Epoch 227/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0535 - accuracy: 0.7396 - val_loss: 0.2729 - val_accuracy: 0.4594\n",
      "Epoch 228/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0514 - accuracy: 0.7692 - val_loss: 0.3335 - val_accuracy: 0.4609\n",
      "Epoch 229/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0524 - accuracy: 0.7402 - val_loss: 0.2695 - val_accuracy: 0.4437\n",
      "Epoch 230/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0545 - accuracy: 0.7476 - val_loss: 0.2356 - val_accuracy: 0.4609\n",
      "Epoch 231/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0532 - accuracy: 0.7395 - val_loss: 0.2811 - val_accuracy: 0.4531\n",
      "Epoch 232/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0497 - accuracy: 0.7601 - val_loss: 0.3229 - val_accuracy: 0.4234\n",
      "Epoch 233/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0537 - accuracy: 0.7358 - val_loss: 0.2892 - val_accuracy: 0.4563\n",
      "Epoch 234/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0533 - accuracy: 0.7459 - val_loss: 0.2687 - val_accuracy: 0.4453\n",
      "Epoch 235/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0513 - accuracy: 0.7556 - val_loss: 0.3107 - val_accuracy: 0.4313\n",
      "Epoch 236/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0542 - accuracy: 0.7474 - val_loss: 0.3690 - val_accuracy: 0.4406\n",
      "Epoch 237/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0550 - accuracy: 0.7471 - val_loss: 0.3089 - val_accuracy: 0.4563\n",
      "Epoch 238/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0530 - accuracy: 0.7376 - val_loss: 0.2590 - val_accuracy: 0.4641\n",
      "Epoch 239/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0548 - accuracy: 0.7421 - val_loss: 0.2304 - val_accuracy: 0.4719\n",
      "Epoch 240/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0525 - accuracy: 0.7470 - val_loss: 0.2759 - val_accuracy: 0.4594\n",
      "Epoch 241/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0556 - accuracy: 0.7338 - val_loss: 0.2672 - val_accuracy: 0.4703\n",
      "Epoch 242/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0551 - accuracy: 0.7324 - val_loss: 0.3201 - val_accuracy: 0.4688\n",
      "Epoch 243/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0519 - accuracy: 0.7576 - val_loss: 0.3597 - val_accuracy: 0.4516\n",
      "Epoch 244/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0529 - accuracy: 0.7474 - val_loss: 0.3418 - val_accuracy: 0.4422\n",
      "Epoch 245/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0513 - accuracy: 0.7574 - val_loss: 0.2339 - val_accuracy: 0.4781\n",
      "Epoch 246/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0518 - accuracy: 0.7496 - val_loss: 0.3420 - val_accuracy: 0.4406\n",
      "Epoch 247/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0533 - accuracy: 0.7487 - val_loss: 0.3581 - val_accuracy: 0.4469\n",
      "Epoch 248/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0549 - accuracy: 0.7382 - val_loss: 0.1611 - val_accuracy: 0.4609\n",
      "Epoch 249/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0552 - accuracy: 0.7416 - val_loss: 0.2664 - val_accuracy: 0.4437\n",
      "Epoch 250/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0537 - accuracy: 0.7378 - val_loss: 0.2388 - val_accuracy: 0.4297\n",
      "Epoch 251/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0538 - accuracy: 0.7436 - val_loss: 0.2705 - val_accuracy: 0.4594\n",
      "Epoch 252/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0524 - accuracy: 0.7560 - val_loss: 0.2648 - val_accuracy: 0.4594\n",
      "Epoch 253/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0520 - accuracy: 0.7539 - val_loss: 0.2642 - val_accuracy: 0.4578\n",
      "Epoch 254/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0510 - accuracy: 0.7577 - val_loss: 0.3243 - val_accuracy: 0.4484\n",
      "Epoch 255/300\n",
      "47/47 [==============================] - 231s 5s/step - loss: 0.0511 - accuracy: 0.7560 - val_loss: 0.2981 - val_accuracy: 0.4250\n",
      "Epoch 256/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0529 - accuracy: 0.7397 - val_loss: 0.2880 - val_accuracy: 0.4516\n",
      "Epoch 257/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0532 - accuracy: 0.7483 - val_loss: 0.2827 - val_accuracy: 0.4516\n",
      "Epoch 258/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0517 - accuracy: 0.7540 - val_loss: 0.2306 - val_accuracy: 0.4250\n",
      "Epoch 259/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0523 - accuracy: 0.7515 - val_loss: 0.3636 - val_accuracy: 0.4578\n",
      "Epoch 260/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0520 - accuracy: 0.7564 - val_loss: 0.3178 - val_accuracy: 0.4531\n",
      "Epoch 261/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0509 - accuracy: 0.7570 - val_loss: 0.2723 - val_accuracy: 0.4375\n",
      "Epoch 262/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0504 - accuracy: 0.7652 - val_loss: 0.2936 - val_accuracy: 0.4484\n",
      "Epoch 263/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0531 - accuracy: 0.7549 - val_loss: 0.2252 - val_accuracy: 0.4391\n",
      "Epoch 264/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0527 - accuracy: 0.7462 - val_loss: 0.3054 - val_accuracy: 0.4578\n",
      "Epoch 265/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0506 - accuracy: 0.7520 - val_loss: 0.3078 - val_accuracy: 0.4578\n",
      "Epoch 266/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0528 - accuracy: 0.7445 - val_loss: 0.3533 - val_accuracy: 0.4375\n",
      "Epoch 267/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0539 - accuracy: 0.7447 - val_loss: 0.2443 - val_accuracy: 0.4609\n",
      "Epoch 268/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0528 - accuracy: 0.7552 - val_loss: 0.2862 - val_accuracy: 0.4703\n",
      "Epoch 269/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0503 - accuracy: 0.7541 - val_loss: 0.2801 - val_accuracy: 0.4844\n",
      "Epoch 270/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0523 - accuracy: 0.7562 - val_loss: 0.3305 - val_accuracy: 0.4609\n",
      "Epoch 271/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0511 - accuracy: 0.7503 - val_loss: 0.2744 - val_accuracy: 0.4500\n",
      "Epoch 272/300\n",
      "47/47 [==============================] - 232s 5s/step - loss: 0.0515 - accuracy: 0.7576 - val_loss: 0.4400 - val_accuracy: 0.4531\n",
      "Epoch 273/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0556 - accuracy: 0.7588 - val_loss: 0.3029 - val_accuracy: 0.4578\n",
      "Epoch 274/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0577 - accuracy: 0.7494 - val_loss: 0.3156 - val_accuracy: 0.4484\n",
      "Epoch 275/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0513 - accuracy: 0.7532 - val_loss: 0.2969 - val_accuracy: 0.4734\n",
      "Epoch 276/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0522 - accuracy: 0.7532 - val_loss: 0.3635 - val_accuracy: 0.4547\n",
      "Epoch 277/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0496 - accuracy: 0.7629 - val_loss: 0.2735 - val_accuracy: 0.4563\n",
      "Epoch 278/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0502 - accuracy: 0.7636 - val_loss: 0.3463 - val_accuracy: 0.4672\n",
      "Epoch 279/300\n",
      "47/47 [==============================] - 233s 5s/step - loss: 0.0525 - accuracy: 0.7492 - val_loss: 0.2637 - val_accuracy: 0.4734\n",
      "Epoch 280/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0562 - accuracy: 0.7448 - val_loss: 0.2211 - val_accuracy: 0.4703\n",
      "Epoch 281/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0546 - accuracy: 0.7511 - val_loss: 0.3672 - val_accuracy: 0.4563\n",
      "Epoch 282/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0525 - accuracy: 0.7615 - val_loss: 0.3024 - val_accuracy: 0.4453\n",
      "Epoch 283/300\n",
      "47/47 [==============================] - 234s 5s/step - loss: 0.0495 - accuracy: 0.7710 - val_loss: 0.2369 - val_accuracy: 0.4172\n",
      "Epoch 284/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0512 - accuracy: 0.7464 - val_loss: 0.2845 - val_accuracy: 0.4531\n",
      "Epoch 285/300\n",
      "47/47 [==============================] - 244s 5s/step - loss: 0.0504 - accuracy: 0.7643 - val_loss: 0.2878 - val_accuracy: 0.4672\n",
      "Epoch 286/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0522 - accuracy: 0.7586 - val_loss: 0.3561 - val_accuracy: 0.4437\n",
      "Epoch 287/300\n",
      "47/47 [==============================] - 242s 5s/step - loss: 0.0504 - accuracy: 0.7642 - val_loss: 0.3049 - val_accuracy: 0.4328\n",
      "Epoch 288/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0519 - accuracy: 0.7452 - val_loss: 0.4595 - val_accuracy: 0.4391\n",
      "Epoch 289/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0517 - accuracy: 0.7780 - val_loss: 0.3573 - val_accuracy: 0.4656\n",
      "Epoch 290/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0533 - accuracy: 0.7442 - val_loss: 0.2823 - val_accuracy: 0.4688\n",
      "Epoch 291/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0525 - accuracy: 0.7602 - val_loss: 0.2325 - val_accuracy: 0.4531\n",
      "Epoch 292/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0528 - accuracy: 0.7489 - val_loss: 0.2487 - val_accuracy: 0.4531\n",
      "Epoch 293/300\n",
      "47/47 [==============================] - 235s 5s/step - loss: 0.0640 - accuracy: 0.7655 - val_loss: 0.2327 - val_accuracy: 0.4609\n",
      "Epoch 294/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0505 - accuracy: 0.7492 - val_loss: 0.2729 - val_accuracy: 0.4609\n",
      "Epoch 295/300\n",
      "47/47 [==============================] - 240s 5s/step - loss: 0.0515 - accuracy: 0.7504 - val_loss: 0.2870 - val_accuracy: 0.4484\n",
      "Epoch 296/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0503 - accuracy: 0.7554 - val_loss: 0.3016 - val_accuracy: 0.4688\n",
      "Epoch 297/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0513 - accuracy: 0.7548 - val_loss: 0.3016 - val_accuracy: 0.4469\n",
      "Epoch 298/300\n",
      "47/47 [==============================] - 237s 5s/step - loss: 0.0515 - accuracy: 0.7568 - val_loss: 0.2379 - val_accuracy: 0.4719\n",
      "Epoch 299/300\n",
      "47/47 [==============================] - 238s 5s/step - loss: 0.0526 - accuracy: 0.7604 - val_loss: 0.3384 - val_accuracy: 0.4641\n",
      "Epoch 300/300\n",
      "47/47 [==============================] - 236s 5s/step - loss: 0.0528 - accuracy: 0.7506 - val_loss: 0.3284 - val_accuracy: 0.4641\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f92b1dd4d90>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
    "STEP_SIZE_VALID=valid_generator.n//valid_generator.batch_size\n",
    "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    steps_per_epoch=STEP_SIZE_TRAIN,\n",
    "                    validation_data=valid_generator,\n",
    "                    validation_steps=STEP_SIZE_VALID,\n",
    "                    epochs=n_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9323567d-93a3-4500-b39a-a00318629079",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "646/646 [==============================] - 38s 59ms/step\n"
     ]
    }
   ],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict_generator(test_generator,\n",
    "steps=STEP_SIZE_TEST,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55f169b3-9e88-43d7-9480-c17a80cb9e4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.8603887e-06, 7.1591976e-06, 2.2128458e-07, ..., 1.3970106e-07,\n",
       "        1.7018463e-07, 2.6283331e-07],\n",
       "       [8.0961401e-25, 1.3346599e-24, 9.9745268e-01, ..., 1.1962352e-13,\n",
       "        5.5591663e-08, 2.3077607e-02],\n",
       "       [2.7306040e-14, 1.0710371e-13, 1.8770956e-09, ..., 1.1225283e-02,\n",
       "        4.0474743e-02, 8.2398246e-06],\n",
       "       ...,\n",
       "       [1.0000000e+00, 3.6453353e-17, 1.9277035e-24, ..., 1.3829804e-24,\n",
       "        9.1412462e-27, 2.0635383e-23],\n",
       "       [5.8603887e-06, 7.1591976e-06, 2.2128458e-07, ..., 1.3970106e-07,\n",
       "        1.7018463e-07, 2.6283331e-07],\n",
       "       [2.1589161e-25, 7.6994143e-17, 0.0000000e+00, ..., 0.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d306d73-b624-46e9-aaa3-dbcca93dd9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_bool = (pred >0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "392a65ff-7ab7-4ec6-a745-bbf13c9c73de",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "labels = train_generator.class_indices\n",
    "\n",
    "textfile = open(\"TE_labels.txt\", \"w\")\n",
    "for element in labels:\n",
    "    textfile.write(element + \"\\n\")\n",
    "textfile.close()\n",
    "labels = dict((v,k) for k,v in labels.items())\n",
    "for row in pred_bool:\n",
    "    l=[]\n",
    "    for index,cls in enumerate(row):\n",
    "        if cls:\n",
    "            l.append(labels[index])\n",
    "    predictions.append(\",\".join(l))\n",
    "filenames=test_generator.filenames\n",
    "results=pd.DataFrame({\"Filename\":filenames,\n",
    "                      \"Predictions\":predictions})\n",
    "results.to_csv(\"TEresults.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7385e1fa-c041-40a3-ad3a-c47482ed3e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Filename</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10.png</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.png</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>102.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>641</th>\n",
       "      <td>95.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>642</th>\n",
       "      <td>96.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>643</th>\n",
       "      <td>97.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>644</th>\n",
       "      <td>98.png</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>645</th>\n",
       "      <td>99.png</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>646 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Filename Predictions\n",
       "0      1.png            \n",
       "1     10.png          10\n",
       "2    100.png           6\n",
       "3    101.png           2\n",
       "4    102.png           0\n",
       "..       ...         ...\n",
       "641   95.png           1\n",
       "642   96.png           2\n",
       "643   97.png           0\n",
       "644   98.png            \n",
       "645   99.png           2\n",
       "\n",
       "[646 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "raw",
   "id": "476521a0-bfa6-411f-9793-2fd3f3d8e161",
   "metadata": {},
   "source": [
    "from keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "#load the image\n",
    "my_image = load_img('data/test/images/127.png', target_size=(224, 224))\n",
    "\n",
    "#preprocess the image\n",
    "my_image = img_to_array(my_image)\n",
    "my_image = my_image.reshape((1, my_image.shape[0], my_image.shape[1], my_image.shape[2]))\n",
    "my_image = preprocess_input(my_image)\n",
    "\n",
    "#make the prediction\n",
    "prediction = model.predict(my_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1c30c92-b894-4711-a67e-0bf044228268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "original = PIL.Image.open(\"data/test/images/100.png\")\n",
    "file_type = original.format\n",
    "\n",
    "original.save(\"testing/test.png\", format=\"png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ed903af-3ed6-445b-ad90-5300d4304062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 validated image filenames.\n",
      "1/1 [==============================] - 0s 94ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hoangvo/opt/anaconda3/envs/coursework/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1905: UserWarning: `Model.predict_generator` is deprecated and will be removed in a future version. Please use `Model.predict`, which supports generators.\n",
      "  warnings.warn('`Model.predict_generator` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "original = PIL.Image.open(\"data/test/images/100.png\") # replace later with image input\n",
    "file_type = original.format\n",
    "\n",
    "original.save(\"testing/test.png\", format=\"png\")\n",
    "##########################\n",
    "testdata = []\n",
    "for fname in sorted(os.listdir('testing')):\n",
    "    if fname == \".DS_Store\": continue\n",
    "            \n",
    "    subject_data_path = os.path.join('testing', fname)                   \n",
    "    if not os.path.isfile(subject_data_path): continue          \n",
    "    testdata.append(fname)\n",
    "    \n",
    "df = pd.DataFrame(testdata, columns=['fnames'])\n",
    "df['fnames']= df['fnames'].astype(str)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "test_g=test_datagen.flow_from_dataframe(\n",
    "     dataframe= df,\n",
    "      directory=\"./testing\",\n",
    "      x_col=\"fnames\",\n",
    "      batch_size=1,\n",
    "      seed=42,\n",
    "      shuffle=False,\n",
    "      class_mode=None,\n",
    "      target_size=(224,224))\n",
    "STEP_SIZE_TEST=test_g.n//test_g.batch_size\n",
    "pred = model.predict_generator(test_g,\n",
    "                               steps=STEP_SIZE_TEST,\n",
    "                               verbose=1)\n",
    "pred_bool = (pred >0.5)\n",
    "predictions=[]\n",
    "#labels = train_generator.class_indices\n",
    "#labels = dict((v,k) for k,v in labels.items())\n",
    "labels = {}\n",
    "file1 = open('TE_labels.txt', 'r')\n",
    "Lines = file1.readlines()\n",
    " \n",
    "count = 0\n",
    "# Strips the newline character\n",
    "for line in Lines:\n",
    "    labels[count] = line.strip()\n",
    "    count += 1\n",
    "    \n",
    "for row in pred_bool:\n",
    "    l=[]\n",
    "    for index,cls in enumerate(row):\n",
    "        if cls:\n",
    "            l.append(labels[index])\n",
    "    predictions.append(\",\".join(l))\n",
    "    \n",
    "    \n",
    "if predictions[0] == '':\n",
    "    result = 0\n",
    "else:\n",
    "    result = float (predictions[0] )\n",
    "#result = float (predictions[0] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18fcfdb4-3241-4161-85cb-9d2a6cec1b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86e81fe7-eaf1-4b6c-bb83-60163f2051ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06603d7a-be6f-4b00-a566-274256c1fbda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '0',\n",
       " 1: '1',\n",
       " 2: '10',\n",
       " 3: '11',\n",
       " 4: '12',\n",
       " 5: '13',\n",
       " 6: '14',\n",
       " 7: '15',\n",
       " 8: '16',\n",
       " 9: '17',\n",
       " 10: '2',\n",
       " 11: '3',\n",
       " 12: '4',\n",
       " 13: '5',\n",
       " 14: '6',\n",
       " 15: '7',\n",
       " 16: '8',\n",
       " 17: '9'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f98c159d-64be-4d98-b12d-98ff511626a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f9307bfaf10>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2964a2d6-0d92-4019-b93e-f908bd728ac7",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "test_image = image.load_img('data/test/images/127.png', target_size =(224,224))\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "result = model.predict(test_image)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fd6718b4-dc5f-4115-b886-424f6fcfe55d",
   "metadata": {},
   "source": [
    "test_image = image.load_img('data/test/images/228.png', target_size =(224,224))\n",
    "test_image = np.expand_dims(test_image,axis=0)\n",
    "test_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be769dfd-b348-4067-bb46-276897728b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "filename2 = 'model_te.h5' \n",
    "model.save(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "607599c9-9b5d-44b2-a74a-95516d370e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "# load model\n",
    "model = load_model('model_te.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "652a1b62-1de5-4853-b3c4-d36882a6a62b",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2020/10/create-image-classification-model-python-keras/"
   ]
  },
  {
   "cell_type": "raw",
   "id": "23e021e2-aaed-440a-bda5-a8b1b4e313fd",
   "metadata": {},
   "source": [
    "    def flow_from_dataframe(self,\n",
    "                            dataframe,\n",
    "                            directory=None,\n",
    "                            x_col=\"filename\",\n",
    "                            y_col=\"class\",\n",
    "                            weight_col=None,\n",
    "                            target_size=(256, 256),\n",
    "                            color_mode='rgb',\n",
    "                            classes=None,\n",
    "                            class_mode='categorical',\n",
    "                            batch_size=32,\n",
    "                            shuffle=True,\n",
    "                            seed=None,\n",
    "                            save_to_dir=None,\n",
    "                            save_prefix='',\n",
    "                            save_format='png',\n",
    "                            subset=None,\n",
    "                            interpolation='nearest',\n",
    "                            validate_filenames=True,\n",
    "                            **kwargs):\n",
    "        \"\"\"Takes the dataframe and the path to a directory\n",
    "         and generates batches of augmented/normalized data.\n",
    "        **A simple tutorial can be found **[here](\n",
    "                                    http://bit.ly/keras_flow_from_dataframe).\n",
    "        # Arguments\n",
    "            dataframe: Pandas dataframe containing the filepaths relative to\n",
    "                `directory` (or absolute paths if `directory` is None) of the\n",
    "                images in a string column. It should include other column/s\n",
    "                depending on the `class_mode`:\n",
    "                - if `class_mode` is `\"categorical\"` (default value) it must\n",
    "                    include the `y_col` column with the class/es of each image.\n",
    "                    Values in column can be string/list/tuple if a single class\n",
    "                    or list/tuple if multiple classes.\n",
    "                - if `class_mode` is `\"binary\"` or `\"sparse\"` it must include\n",
    "                    the given `y_col` column with class values as strings.\n",
    "                - if `class_mode` is `\"raw\"` or `\"multi_output\"` it should contain\n",
    "                the columns specified in `y_col`.\n",
    "                - if `class_mode` is `\"input\"` or `None` no extra column is needed.\n",
    "            directory: string, path to the directory to read images from. If `None`,\n",
    "                data in `x_col` column should be absolute paths.\n",
    "            x_col: string, column in `dataframe` that contains the filenames (or\n",
    "                absolute paths if `directory` is `None`).\n",
    "            y_col: string or list, column/s in `dataframe` that has the target data.\n",
    "            weight_col: string, column in `dataframe` that contains the sample\n",
    "                weights. Default: `None`.\n",
    "            target_size: tuple of integers `(height, width)`, default: `(256, 256)`.\n",
    "                The dimensions to which all images found will be resized.\n",
    "            color_mode: one of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
    "                Whether the images will be converted to have 1 or 3 color channels.\n",
    "            classes: optional list of classes (e.g. `['dogs', 'cats']`).\n",
    "                Default: None. If not provided, the list of classes will be\n",
    "                automatically inferred from the `y_col`,\n",
    "                which will map to the label indices, will be alphanumeric).\n",
    "                The dictionary containing the mapping from class names to class\n",
    "                indices can be obtained via the attribute `class_indices`.\n",
    "            class_mode: one of \"binary\", \"categorical\", \"input\", \"multi_output\",\n",
    "                \"raw\", sparse\" or None. Default: \"categorical\".\n",
    "                Mode for yielding the targets:\n",
    "                - `\"binary\"`: 1D NumPy array of binary labels,\n",
    "                - `\"categorical\"`: 2D NumPy array of one-hot encoded labels.\n",
    "                    Supports multi-label output.\n",
    "                - `\"input\"`: images identical to input images (mainly used to\n",
    "                    work with autoencoders),\n",
    "                - `\"multi_output\"`: list with the values of the different columns,\n",
    "                - `\"raw\"`: NumPy array of values in `y_col` column(s),\n",
    "                - `\"sparse\"`: 1D NumPy array of integer labels,\n",
    "                - `None`, no targets are returned (the generator will only yield\n",
    "                    batches of image data, which is useful to use in\n",
    "                    `model.predict_generator()`).\n",
    "            batch_size: size of the batches of data (default: 32).\n",
    "            shuffle: whether to shuffle the data (default: True)\n",
    "            seed: optional random seed for shuffling and transformations.\n",
    "            save_to_dir: None or str (default: None).\n",
    "                This allows you to optionally specify a directory\n",
    "                to which to save the augmented pictures being generated\n",
    "                (useful for visualizing what you are doing).\n",
    "            save_prefix: str. Prefix to use for filenames of saved pictures\n",
    "                (only relevant if `save_to_dir` is set).\n",
    "            save_format: one of \"png\", \"jpeg\"\n",
    "                (only relevant if `save_to_dir` is set). Default: \"png\".\n",
    "            follow_links: whether to follow symlinks inside class subdirectories\n",
    "                (default: False).\n",
    "            subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
    "                `validation_split` is set in `ImageDataGenerator`.\n",
    "            interpolation: Interpolation method used to resample the image if the\n",
    "                target size is different from that of the loaded image.\n",
    "                Supported methods are `\"nearest\"`, `\"bilinear\"`, and `\"bicubic\"`.\n",
    "                If PIL version 1.1.3 or newer is installed, `\"lanczos\"` is also\n",
    "                supported. If PIL version 3.4.0 or newer is installed, `\"box\"` and\n",
    "                `\"hamming\"` are also supported. By default, `\"nearest\"` is used.\n",
    "            validate_filenames: Boolean, whether to validate image filenames in\n",
    "                `x_col`. If `True`, invalid images will be ignored. Disabling this\n",
    "                option can lead to speed-up in the execution of this function.\n",
    "                Default: `True`.\n",
    "        # Returns\n",
    "            A `DataFrameIterator` yielding tuples of `(x, y)`\n",
    "            where `x` is a NumPy array containing a batch\n",
    "            of images with shape `(batch_size, *target_size, channels)`\n",
    "            and `y` is a NumPy array of corresponding labels.\n",
    "        \"\"\"\n",
    "        if 'has_ext' in kwargs:\n",
    "            warnings.warn('has_ext is deprecated, filenames in the dataframe have '\n",
    "                          'to match the exact filenames in disk.',\n",
    "                          DeprecationWarning)\n",
    "        if 'sort' in kwargs:\n",
    "            warnings.warn('sort is deprecated, batches will be created in the'\n",
    "                          'same order than the filenames provided if shuffle'\n",
    "                          'is set to False.', DeprecationWarning)\n",
    "        if class_mode == 'other':\n",
    "            warnings.warn('`class_mode` \"other\" is deprecated, please use '\n",
    "                          '`class_mode` \"raw\".', DeprecationWarning)\n",
    "            class_mode = 'raw'\n",
    "        if 'drop_duplicates' in kwargs:\n",
    "            warnings.warn('drop_duplicates is deprecated, you can drop duplicates '\n",
    "                          'by using the pandas.DataFrame.drop_duplicates method.',\n",
    "                          DeprecationWarning)\n",
    "\n",
    "        return DataFrameIterator(\n",
    "            dataframe,\n",
    "            directory,\n",
    "            self,\n",
    "            x_col=x_col,\n",
    "            y_col=y_col,\n",
    "            weight_col=weight_col,\n",
    "            target_size=target_size,\n",
    "            color_mode=color_mode,\n",
    "            classes=classes,\n",
    "            class_mode=class_mode,\n",
    "            data_format=self.data_format,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format,\n",
    "            subset=subset,\n",
    "            interpolation=interpolation,\n",
    "            validate_filenames=validate_filenames,\n",
    "            dtype=self.dtype\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0415ad83-e8ea-42f0-af3f-7c72baacf2fa",
   "metadata": {},
   "source": [
    "\"\"\"Utilities for real-time data augmentation on image data.\n",
    "\"\"\"\n",
    "import os\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from .iterator import BatchFromFilesMixin, Iterator\n",
    "from .utils import validate_filename\n",
    "\n",
    "\n",
    "class DataFrameIterator(BatchFromFilesMixin, Iterator):\n",
    "    \"\"\"Iterator capable of reading images from a directory on disk\n",
    "        through a dataframe.\n",
    "    # Arguments\n",
    "        dataframe: Pandas dataframe containing the filepaths relative to\n",
    "            `directory` (or absolute paths if `directory` is None) of the\n",
    "            images in a string column. It should include other column/s\n",
    "            depending on the `class_mode`:\n",
    "            - if `class_mode` is `\"categorical\"` (default value) it must\n",
    "                include the `y_col` column with the class/es of each image.\n",
    "                Values in column can be string/list/tuple if a single class\n",
    "                or list/tuple if multiple classes.\n",
    "            - if `class_mode` is `\"binary\"` or `\"sparse\"` it must include\n",
    "                the given `y_col` column with class values as strings.\n",
    "            - if `class_mode` is `\"raw\"` or `\"multi_output\"` it should contain\n",
    "                the columns specified in `y_col`.\n",
    "            - if `class_mode` is `\"input\"` or `None` no extra column is needed.\n",
    "        directory: string, path to the directory to read images from. If `None`,\n",
    "            data in `x_col` column should be absolute paths.\n",
    "        image_data_generator: Instance of `ImageDataGenerator` to use for\n",
    "            random transformations and normalization. If None, no transformations\n",
    "            and normalizations are made.\n",
    "        x_col: string, column in `dataframe` that contains the filenames (or\n",
    "            absolute paths if `directory` is `None`).\n",
    "        y_col: string or list, column/s in `dataframe` that has the target data.\n",
    "        weight_col: string, column in `dataframe` that contains the sample\n",
    "            weights. Default: `None`.\n",
    "        target_size: tuple of integers, dimensions to resize input images to.\n",
    "        color_mode: One of `\"rgb\"`, `\"rgba\"`, `\"grayscale\"`.\n",
    "            Color mode to read images.\n",
    "        classes: Optional list of strings, classes to use (e.g. `[\"dogs\", \"cats\"]`).\n",
    "            If None, all classes in `y_col` will be used.\n",
    "        class_mode: one of \"binary\", \"categorical\", \"input\", \"multi_output\",\n",
    "            \"raw\", \"sparse\" or None. Default: \"categorical\".\n",
    "            Mode for yielding the targets:\n",
    "            - `\"binary\"`: 1D numpy array of binary labels,\n",
    "            - `\"categorical\"`: 2D numpy array of one-hot encoded labels.\n",
    "                Supports multi-label output.\n",
    "            - `\"input\"`: images identical to input images (mainly used to\n",
    "                work with autoencoders),\n",
    "            - `\"multi_output\"`: list with the values of the different columns,\n",
    "            - `\"raw\"`: numpy array of values in `y_col` column(s),\n",
    "            - `\"sparse\"`: 1D numpy array of integer labels,\n",
    "            - `None`, no targets are returned (the generator will only yield\n",
    "                batches of image data, which is useful to use in\n",
    "                `model.predict_generator()`).\n",
    "        batch_size: Integer, size of a batch.\n",
    "        shuffle: Boolean, whether to shuffle the data between epochs.\n",
    "        seed: Random seed for data shuffling.\n",
    "        data_format: String, one of `channels_first`, `channels_last`.\n",
    "        save_to_dir: Optional directory where to save the pictures\n",
    "            being yielded, in a viewable format. This is useful\n",
    "            for visualizing the random transformations being\n",
    "            applied, for debugging purposes.\n",
    "        save_prefix: String prefix to use for saving sample\n",
    "            images (if `save_to_dir` is set).\n",
    "        save_format: Format to use for saving sample images\n",
    "            (if `save_to_dir` is set).\n",
    "        subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
    "            validation_split is set in ImageDataGenerator.\n",
    "        interpolation: Interpolation method used to resample the image if the\n",
    "            target size is different from that of the loaded image.\n",
    "            Supported methods are \"nearest\", \"bilinear\", and \"bicubic\".\n",
    "            If PIL version 1.1.3 or newer is installed, \"lanczos\" is also\n",
    "            supported. If PIL version 3.4.0 or newer is installed, \"box\" and\n",
    "            \"hamming\" are also supported. By default, \"nearest\" is used.\n",
    "        keep_aspect_ratio: Boolean, whether to resize images to a target size\n",
    "            without aspect ratio distortion. The image is cropped in the center\n",
    "            with target aspect ratio before resizing.\n",
    "        dtype: Dtype to use for the generated arrays.\n",
    "        validate_filenames: Boolean, whether to validate image filenames in\n",
    "        `x_col`. If `True`, invalid images will be ignored. Disabling this option\n",
    "        can lead to speed-up in the instantiation of this class. Default: `True`.\n",
    "    \"\"\"\n",
    "    allowed_class_modes = {\n",
    "        'binary', 'categorical', 'input', 'multi_output', 'raw', 'sparse', None\n",
    "    }\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        try:\n",
    "            from tensorflow.keras.utils import Sequence as TFSequence\n",
    "            if TFSequence not in cls.__bases__:\n",
    "                cls.__bases__ = cls.__bases__ + (TFSequence,)\n",
    "        except ImportError:\n",
    "            pass\n",
    "        return super(DataFrameIterator, cls).__new__(cls)\n",
    "\n",
    "    def __init__(self,\n",
    "                 dataframe,\n",
    "                 directory=None,\n",
    "                 image_data_generator=None,\n",
    "                 x_col=\"filename\",\n",
    "                 y_col=\"class\",\n",
    "                 weight_col=None,\n",
    "                 target_size=(256, 256),\n",
    "                 color_mode='rgb',\n",
    "                 classes=None,\n",
    "                 class_mode='categorical',\n",
    "                 batch_size=32,\n",
    "                 shuffle=True,\n",
    "                 seed=None,\n",
    "                 data_format='channels_last',\n",
    "                 save_to_dir=None,\n",
    "                 save_prefix='',\n",
    "                 save_format='png',\n",
    "                 subset=None,\n",
    "                 interpolation='nearest',\n",
    "                 keep_aspect_ratio=False,\n",
    "                 dtype='float32',\n",
    "                 validate_filenames=True):\n",
    "\n",
    "        super(DataFrameIterator, self).set_processing_attrs(image_data_generator,\n",
    "                                                            target_size,\n",
    "                                                            color_mode,\n",
    "                                                            data_format,\n",
    "                                                            save_to_dir,\n",
    "                                                            save_prefix,\n",
    "                                                            save_format,\n",
    "                                                            subset,\n",
    "                                                            interpolation,\n",
    "                                                            keep_aspect_ratio)\n",
    "        df = dataframe.copy()\n",
    "        self.directory = directory or ''\n",
    "        self.class_mode = class_mode\n",
    "        self.dtype = dtype\n",
    "        # check that inputs match the required class_mode\n",
    "        self._check_params(df, x_col, y_col, weight_col, classes)\n",
    "        if validate_filenames:  # check which image files are valid and keep them\n",
    "            df = self._filter_valid_filepaths(df, x_col)\n",
    "        if class_mode not in [\"input\", \"multi_output\", \"raw\", None]:\n",
    "            df, classes = self._filter_classes(df, y_col, classes)\n",
    "            num_classes = len(classes)\n",
    "            # build an index of all the unique classes\n",
    "            self.class_indices = dict(zip(classes, range(len(classes))))\n",
    "        # retrieve only training or validation set\n",
    "        if self.split:\n",
    "            num_files = len(df)\n",
    "            start = int(self.split[0] * num_files)\n",
    "            stop = int(self.split[1] * num_files)\n",
    "            df = df.iloc[start: stop, :]\n",
    "        # get labels for each observation\n",
    "        if class_mode not in [\"input\", \"multi_output\", \"raw\", None]:\n",
    "            self.classes = self.get_classes(df, y_col)\n",
    "        self.filenames = df[x_col].tolist()\n",
    "        self._sample_weight = df[weight_col].values if weight_col else None\n",
    "\n",
    "        if class_mode == \"multi_output\":\n",
    "            self._targets = [np.array(df[col].tolist()) for col in y_col]\n",
    "        if class_mode == \"raw\":\n",
    "            self._targets = df[y_col].values\n",
    "        self.samples = len(self.filenames)\n",
    "        validated_string = 'validated' if validate_filenames else 'non-validated'\n",
    "        if class_mode in [\"input\", \"multi_output\", \"raw\", None]:\n",
    "            print('Found {} {} image filenames.'\n",
    "                  .format(self.samples, validated_string))\n",
    "        else:\n",
    "            print('Found {} {} image filenames belonging to {} classes.'\n",
    "                  .format(self.samples, validated_string, num_classes))\n",
    "        self._filepaths = [\n",
    "            os.path.join(self.directory, fname) for fname in self.filenames\n",
    "        ]\n",
    "        super(DataFrameIterator, self).__init__(self.samples,\n",
    "                                                batch_size,\n",
    "                                                shuffle,\n",
    "                                                seed)\n",
    "\n",
    "    def _check_params(self, df, x_col, y_col, weight_col, classes):\n",
    "        # check class mode is one of the currently supported\n",
    "        if self.class_mode not in self.allowed_class_modes:\n",
    "            raise ValueError('Invalid class_mode: {}; expected one of: {}'\n",
    "                             .format(self.class_mode, self.allowed_class_modes))\n",
    "        # check that y_col has several column names if class_mode is multi_output\n",
    "        if (self.class_mode == 'multi_output') and not isinstance(y_col, list):\n",
    "            raise TypeError(\n",
    "                'If class_mode=\"{}\", y_col must be a list. Received {}.'\n",
    "                .format(self.class_mode, type(y_col).__name__)\n",
    "            )\n",
    "        # check that filenames/filepaths column values are all strings\n",
    "        if not all(df[x_col].apply(lambda x: isinstance(x, str))):\n",
    "            raise TypeError('All values in column x_col={} must be strings.'\n",
    "                            .format(x_col))\n",
    "        # check labels are string if class_mode is binary or sparse\n",
    "        if self.class_mode in {'binary', 'sparse'}:\n",
    "            if not all(df[y_col].apply(lambda x: isinstance(x, str))):\n",
    "                raise TypeError('If class_mode=\"{}\", y_col=\"{}\" column '\n",
    "                                'values must be strings.'\n",
    "                                .format(self.class_mode, y_col))\n",
    "        # check that if binary there are only 2 different classes\n",
    "        if self.class_mode == 'binary':\n",
    "            if classes:\n",
    "                classes = set(classes)\n",
    "                if len(classes) != 2:\n",
    "                    raise ValueError('If class_mode=\"binary\" there must be 2 '\n",
    "                                     'classes. {} class/es were given.'\n",
    "                                     .format(len(classes)))\n",
    "            elif df[y_col].nunique() != 2:\n",
    "                raise ValueError('If class_mode=\"binary\" there must be 2 classes. '\n",
    "                                 'Found {} classes.'.format(df[y_col].nunique()))\n",
    "        # check values are string, list or tuple if class_mode is categorical\n",
    "        if self.class_mode == 'categorical':\n",
    "            types = (str, list, tuple)\n",
    "            if not all(df[y_col].apply(lambda x: isinstance(x, types))):\n",
    "                raise TypeError('If class_mode=\"{}\", y_col=\"{}\" column '\n",
    "                                'values must be type string, list or tuple.'\n",
    "                                .format(self.class_mode, y_col))\n",
    "        # raise warning if classes are given but will be unused\n",
    "        if classes and self.class_mode in {\"input\", \"multi_output\", \"raw\", None}:\n",
    "            warnings.warn('`classes` will be ignored given the class_mode=\"{}\"'\n",
    "                          .format(self.class_mode))\n",
    "        # check that if weight column that the values are numerical\n",
    "        if weight_col and not issubclass(df[weight_col].dtype.type, np.number):\n",
    "            raise TypeError('Column weight_col={} must be numeric.'\n",
    "                            .format(weight_col))\n",
    "\n",
    "    def get_classes(self, df, y_col):\n",
    "        labels = []\n",
    "        for label in df[y_col]:\n",
    "            if isinstance(label, (list, tuple)):\n",
    "                labels.append([self.class_indices[lbl] for lbl in label])\n",
    "            else:\n",
    "                labels.append(self.class_indices[label])\n",
    "        return labels\n",
    "\n",
    "    @staticmethod\n",
    "    def _filter_classes(df, y_col, classes):\n",
    "        df = df.copy()\n",
    "\n",
    "        def remove_classes(labels, classes):\n",
    "            if isinstance(labels, (list, tuple)):\n",
    "                labels = [cls for cls in labels if cls in classes]\n",
    "                return labels or None\n",
    "            elif isinstance(labels, str):\n",
    "                return labels if labels in classes else None\n",
    "            else:\n",
    "                raise TypeError(\n",
    "                    \"Expect string, list or tuple but found {} in {} column \"\n",
    "                    .format(type(labels), y_col)\n",
    "                )\n",
    "\n",
    "        if classes:\n",
    "            # prepare for membership lookup\n",
    "            classes = list(OrderedDict.fromkeys(classes).keys())\n",
    "            df[y_col] = df[y_col].apply(lambda x: remove_classes(x, classes))\n",
    "        else:\n",
    "            classes = set()\n",
    "            for v in df[y_col]:\n",
    "                if isinstance(v, (list, tuple)):\n",
    "                    classes.update(v)\n",
    "                else:\n",
    "                    classes.add(v)\n",
    "            classes = sorted(classes)\n",
    "        return df.dropna(subset=[y_col]), classes\n",
    "\n",
    "    def _filter_valid_filepaths(self, df, x_col):\n",
    "        \"\"\"Keep only dataframe rows with valid filenames\n",
    "        # Arguments\n",
    "            df: Pandas dataframe containing filenames in a column\n",
    "            x_col: string, column in `df` that contains the filenames or filepaths\n",
    "        # Returns\n",
    "            absolute paths to image files\n",
    "        \"\"\"\n",
    "        filepaths = df[x_col].map(\n",
    "            lambda fname: os.path.join(self.directory, fname)\n",
    "        )\n",
    "        mask = filepaths.apply(validate_filename, args=(self.white_list_formats,))\n",
    "        n_invalid = (~mask).sum()\n",
    "        if n_invalid:\n",
    "            warnings.warn(\n",
    "                'Found {} invalid image filename(s) in x_col=\"{}\". '\n",
    "                'These filename(s) will be ignored.'\n",
    "                .format(n_invalid, x_col)\n",
    "            )\n",
    "        return df[mask]\n",
    "\n",
    "    @property\n",
    "    def filepaths(self):\n",
    "        return self._filepaths\n",
    "\n",
    "    @property\n",
    "    def labels(self):\n",
    "        if self.class_mode in {\"multi_output\", \"raw\"}:\n",
    "            return self._targets\n",
    "        else:\n",
    "            return self.classes\n",
    "\n",
    "    @property\n",
    "    def sample_weight(self):\n",
    "        return self._sample_weight"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1f606ee1-be74-4bad-8ac7-7f71faf4010b",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras-preprocessing/blob/master/keras_preprocessing/image/iterator.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "19a8004a-07d0-47b6-8b87-2d70c9c5b0b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matlab.engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/yr/qwqgmggj2_12p38pnv1hf6380000gp/T/ipykernel_5075/1620819283.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmatlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matlab.engine'"
     ]
    }
   ],
   "source": [
    "import matlab.engine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee961c0-0365-415e-919f-e1a38ffe7d85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
